{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Importing the models\n",
    "from models.mlp import MLP\n",
    "from models.transformer import Transformer\n",
    "from models.ft_transformer_wrapper import FTTransformerWrapper\n",
    "from models.tab_transformer_wrapper import TabTransformerWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model_state(model_name, X, y, input_dim, num_classes, stratified_fold, batch_size=64, num_epochs=200, n_folds=10, device='cuda', metric='f1_score', cat_dims=[0, 12, 15, 29, 30], cat_idxs=[12, 3, 3, 3, 3], num_idxs=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]):\n",
    "    \"\"\"Get best model out of Cross-Validation, using the specified metric.\"\"\"\n",
    "    best_model_state = None\n",
    "    best_metric = -np.inf\n",
    "    fold_results_list = []\n",
    "    for fold, (train_ids, val_ids) in enumerate(stratified_fold.split(X, y)):\n",
    "        print(f'Fold {fold + 1}/{n_folds}')\n",
    "        # Prepare data loaders\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X[train_ids], y[train_ids]),\n",
    "            batch_size=batch_size, num_workers=0, shuffle=True\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X[val_ids], y[val_ids]),\n",
    "            batch_size=batch_size, num_workers=0, \n",
    "        )\n",
    "        # Initialize model, criterion, and optimizer\n",
    "        if model_name == 'MLP':\n",
    "            model = MLP(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "        elif model_name == 'TabTransformer':\n",
    "            model = TabTransformerWrapper(cat_dims=cat_dims,cat_idxs=cat_idxs,num_idxs=num_idxs,num_classes=num_classes,num_heads=4,num_layers=2,dim_model=64,dropout=0.1).to(device)\n",
    "        elif model_name == 'FTTransformer':\n",
    "            model = FTTransformerWrapper(cat_dims=cat_dims,cat_idxs=cat_idxs,num_idxs=num_idxs,num_classes=num_classes,num_heads=4,num_layers=2,dim_model=64,dim_ff=128,dropout=0.1).to(device)\n",
    "        elif model_name == 'Transformer':\n",
    "            model = Transformer(input_dim=input_dim, num_classes=num_classes, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # Train model\n",
    "        model_state = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=num_epochs)\n",
    "        model.load_state_dict(model_state)\n",
    "        # Evaluate model\n",
    "        result_df = evaluate_model(model, val_loader, device)\n",
    "        metric_result = result_df[metric].values[0]\n",
    "\n",
    "        fold_result = {\n",
    "            'Fold': fold + 1,\n",
    "            'Model Name': model_name,\n",
    "        }\n",
    "        fold_result.update(result_df.to_dict(orient='records')[0])\n",
    "        fold_results_list.append(fold_result)\n",
    "\n",
    "        if metric_result > best_metric:\n",
    "            best_metric = metric_result\n",
    "            best_model_state = model.state_dict()\n",
    "        print(f'Best {metric} for fold {fold + 1}: {best_metric:.4f}')\n",
    "        \n",
    "    return best_model_state, fold_results_list\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=500, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_attentions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "            # Get attention weights\n",
    "            if hasattr(model, 'get_attention_weights') and callable(model.get_attention_weights):\n",
    "                try:\n",
    "                    attention_weights = model.get_attention_weights(batch_x)\n",
    "                    if attention_weights is not None:  # Make sure it's not None\n",
    "                        all_attentions.append(attention_weights.cpu().numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting attention weights: {e}\")\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'attention_weights': None # np.concatenate(all_attentions, axis=0) if len(all_attentions) > 0 else None\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([results])\n",
    "\n",
    "def save_model_state(model_state, output_path):\n",
    "    torch.save(model_state, output_path)\n",
    "    print(f'Model state saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System settings\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# TabTransformer Model Settings\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('./data/3_train_processed.csv')\n",
    "train_ros_df = pd.read_csv('./data/3_train_ros_processed.csv')\n",
    "train_smote_df = pd.read_csv('./data/3_train_smote_processed.csv')\n",
    "test_df = pd.read_csv('./data/3_test_processed.csv')\n",
    "\n",
    "# Preprocess data\n",
    "train_features = train_df.drop(['credit_score'], axis=1)\n",
    "train_ros_features = train_ros_df.drop(['credit_score'], axis=1)\n",
    "train_smote_features = train_smote_df.drop(['credit_score'], axis=1)\n",
    "test_features = test_df.drop(['credit_score'], axis=1)\n",
    "\n",
    "train_labels = train_df['credit_score']\n",
    "train_ros_labels = train_ros_df['credit_score']\n",
    "train_smote_labels = train_smote_df['credit_score']\n",
    "test_labels = test_df['credit_score']\n",
    "\n",
    "X = torch.FloatTensor(train_features.values)\n",
    "X_ros = torch.FloatTensor(train_ros_features.values)\n",
    "X_smote = torch.FloatTensor(train_smote_features.values)\n",
    "X_test = torch.FloatTensor(test_features.values)\n",
    "y = torch.LongTensor(train_labels.values)\n",
    "y_ros = torch.LongTensor(train_ros_labels.values)\n",
    "y_smote = torch.LongTensor(train_smote_labels.values)\n",
    "y_test = torch.LongTensor(test_labels.values)\n",
    "\n",
    "# Transformer Model Settings\n",
    "# 1. Categorical features\n",
    "cat_columns = ['month', 'credit_mix', 'payment_of_min_amount', 'spending_level', 'payment_size']\n",
    "cat_idxs = [0, 12, 15, 29, 30]\n",
    "# 2. Continuous features - Month could have 12 unique values, so we treat it as a categorical feature\n",
    "cat_dims = [12, train_df['credit_mix'].nunique(), train_df['payment_of_min_amount'].nunique(), \n",
    "            train_df['spending_level'].nunique() + 1, train_df['payment_size'].nunique()]\n",
    "# 3. Other columns\n",
    "all_columns = list(train_df.columns)\n",
    "num_idxs = [i for i in range(len(all_columns)) if i not in cat_idxs and i != all_columns.index('credit_score')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - original data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7424, Val Loss: 0.6865\n",
      "Epoch [10/500], Train Loss: 0.6165, Val Loss: 0.6282\n",
      "Epoch [20/500], Train Loss: 0.5765, Val Loss: 0.6123\n",
      "Epoch [30/500], Train Loss: 0.5449, Val Loss: 0.5923\n",
      "Epoch [40/500], Train Loss: 0.5255, Val Loss: 0.5779\n",
      "Epoch [50/500], Train Loss: 0.5070, Val Loss: 0.5667\n",
      "Epoch [60/500], Train Loss: 0.4945, Val Loss: 0.5555\n",
      "Epoch [70/500], Train Loss: 0.4827, Val Loss: 0.5584\n",
      "Epoch [80/500], Train Loss: 0.4723, Val Loss: 0.5476\n",
      "Epoch [90/500], Train Loss: 0.4695, Val Loss: 0.5394\n",
      "Epoch [100/500], Train Loss: 0.4628, Val Loss: 0.5429\n",
      "Epoch [110/500], Train Loss: 0.4570, Val Loss: 0.5402\n",
      "Early stopping at epoch 111\n",
      "Best f1_score for fold 1: 0.7578\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7440, Val Loss: 0.6893\n",
      "Epoch [10/500], Train Loss: 0.6168, Val Loss: 0.6314\n",
      "Epoch [20/500], Train Loss: 0.5735, Val Loss: 0.6141\n",
      "Epoch [30/500], Train Loss: 0.5424, Val Loss: 0.5998\n",
      "Epoch [40/500], Train Loss: 0.5198, Val Loss: 0.5843\n",
      "Epoch [50/500], Train Loss: 0.5049, Val Loss: 0.5713\n",
      "Epoch [60/500], Train Loss: 0.4944, Val Loss: 0.5669\n",
      "Epoch [70/500], Train Loss: 0.4825, Val Loss: 0.5608\n",
      "Early stopping at epoch 76\n",
      "Best f1_score for fold 2: 0.7578\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7459, Val Loss: 0.7024\n",
      "Epoch [10/500], Train Loss: 0.6156, Val Loss: 0.6383\n",
      "Epoch [20/500], Train Loss: 0.5746, Val Loss: 0.6178\n",
      "Epoch [30/500], Train Loss: 0.5407, Val Loss: 0.5965\n",
      "Epoch [40/500], Train Loss: 0.5184, Val Loss: 0.5819\n",
      "Epoch [50/500], Train Loss: 0.5022, Val Loss: 0.5734\n",
      "Epoch [60/500], Train Loss: 0.4885, Val Loss: 0.5636\n",
      "Epoch [70/500], Train Loss: 0.4801, Val Loss: 0.5550\n",
      "Epoch [80/500], Train Loss: 0.4736, Val Loss: 0.5575\n",
      "Epoch [90/500], Train Loss: 0.4678, Val Loss: 0.5447\n",
      "Epoch [100/500], Train Loss: 0.4597, Val Loss: 0.5483\n",
      "Early stopping at epoch 100\n",
      "Best f1_score for fold 3: 0.7578\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7478, Val Loss: 0.6890\n",
      "Epoch [10/500], Train Loss: 0.6181, Val Loss: 0.6360\n",
      "Epoch [20/500], Train Loss: 0.5784, Val Loss: 0.6181\n",
      "Epoch [30/500], Train Loss: 0.5499, Val Loss: 0.5968\n",
      "Epoch [40/500], Train Loss: 0.5296, Val Loss: 0.5873\n",
      "Epoch [50/500], Train Loss: 0.5113, Val Loss: 0.5748\n",
      "Epoch [60/500], Train Loss: 0.4958, Val Loss: 0.5661\n",
      "Epoch [70/500], Train Loss: 0.4860, Val Loss: 0.5601\n",
      "Epoch [80/500], Train Loss: 0.4780, Val Loss: 0.5469\n",
      "Epoch [90/500], Train Loss: 0.4713, Val Loss: 0.5424\n",
      "Epoch [100/500], Train Loss: 0.4659, Val Loss: 0.5373\n",
      "Epoch [110/500], Train Loss: 0.4578, Val Loss: 0.5384\n",
      "Early stopping at epoch 115\n",
      "Best f1_score for fold 4: 0.7650\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7446, Val Loss: 0.6960\n",
      "Epoch [10/500], Train Loss: 0.6163, Val Loss: 0.6341\n",
      "Epoch [20/500], Train Loss: 0.5756, Val Loss: 0.6133\n",
      "Epoch [30/500], Train Loss: 0.5472, Val Loss: 0.6062\n",
      "Epoch [40/500], Train Loss: 0.5231, Val Loss: 0.5961\n",
      "Epoch [50/500], Train Loss: 0.5088, Val Loss: 0.5886\n",
      "Epoch [60/500], Train Loss: 0.4969, Val Loss: 0.5784\n",
      "Epoch [70/500], Train Loss: 0.4857, Val Loss: 0.5753\n",
      "Epoch [80/500], Train Loss: 0.4766, Val Loss: 0.5702\n",
      "Epoch [90/500], Train Loss: 0.4746, Val Loss: 0.5661\n",
      "Early stopping at epoch 96\n",
      "Best f1_score for fold 5: 0.7650\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7480, Val Loss: 0.6807\n",
      "Epoch [10/500], Train Loss: 0.6170, Val Loss: 0.6120\n",
      "Epoch [20/500], Train Loss: 0.5784, Val Loss: 0.5986\n",
      "Epoch [30/500], Train Loss: 0.5470, Val Loss: 0.5784\n",
      "Epoch [40/500], Train Loss: 0.5270, Val Loss: 0.5713\n",
      "Epoch [50/500], Train Loss: 0.5095, Val Loss: 0.5589\n",
      "Epoch [60/500], Train Loss: 0.4978, Val Loss: 0.5539\n",
      "Epoch [70/500], Train Loss: 0.4876, Val Loss: 0.5448\n",
      "Epoch [80/500], Train Loss: 0.4790, Val Loss: 0.5432\n",
      "Epoch [90/500], Train Loss: 0.4680, Val Loss: 0.5364\n",
      "Epoch [100/500], Train Loss: 0.4627, Val Loss: 0.5283\n",
      "Epoch [110/500], Train Loss: 0.4621, Val Loss: 0.5207\n",
      "Epoch [120/500], Train Loss: 0.4566, Val Loss: 0.5194\n",
      "Epoch [130/500], Train Loss: 0.4542, Val Loss: 0.5203\n",
      "Early stopping at epoch 130\n",
      "Best f1_score for fold 6: 0.7679\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7449, Val Loss: 0.6838\n",
      "Epoch [10/500], Train Loss: 0.6163, Val Loss: 0.6280\n",
      "Epoch [20/500], Train Loss: 0.5774, Val Loss: 0.6093\n",
      "Epoch [30/500], Train Loss: 0.5477, Val Loss: 0.5913\n",
      "Epoch [40/500], Train Loss: 0.5275, Val Loss: 0.5858\n",
      "Epoch [50/500], Train Loss: 0.5109, Val Loss: 0.5752\n",
      "Epoch [60/500], Train Loss: 0.4971, Val Loss: 0.5654\n",
      "Epoch [70/500], Train Loss: 0.4855, Val Loss: 0.5635\n",
      "Epoch [80/500], Train Loss: 0.4776, Val Loss: 0.5592\n",
      "Early stopping at epoch 84\n",
      "Best f1_score for fold 7: 0.7679\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7467, Val Loss: 0.6870\n",
      "Epoch [10/500], Train Loss: 0.6148, Val Loss: 0.6345\n",
      "Epoch [20/500], Train Loss: 0.5753, Val Loss: 0.6154\n",
      "Epoch [30/500], Train Loss: 0.5435, Val Loss: 0.5973\n",
      "Epoch [40/500], Train Loss: 0.5205, Val Loss: 0.5821\n",
      "Epoch [50/500], Train Loss: 0.5022, Val Loss: 0.5790\n",
      "Epoch [60/500], Train Loss: 0.4891, Val Loss: 0.5679\n",
      "Epoch [70/500], Train Loss: 0.4812, Val Loss: 0.5633\n",
      "Epoch [80/500], Train Loss: 0.4724, Val Loss: 0.5630\n",
      "Epoch [90/500], Train Loss: 0.4687, Val Loss: 0.5540\n",
      "Epoch [100/500], Train Loss: 0.4609, Val Loss: 0.5562\n",
      "Epoch [110/500], Train Loss: 0.4588, Val Loss: 0.5500\n",
      "Early stopping at epoch 116\n",
      "Best f1_score for fold 8: 0.7679\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7519, Val Loss: 0.6955\n",
      "Epoch [10/500], Train Loss: 0.6196, Val Loss: 0.6348\n",
      "Epoch [20/500], Train Loss: 0.5812, Val Loss: 0.6156\n",
      "Epoch [30/500], Train Loss: 0.5545, Val Loss: 0.5977\n",
      "Epoch [40/500], Train Loss: 0.5300, Val Loss: 0.5860\n",
      "Epoch [50/500], Train Loss: 0.5112, Val Loss: 0.5720\n",
      "Epoch [60/500], Train Loss: 0.5011, Val Loss: 0.5668\n",
      "Epoch [70/500], Train Loss: 0.4906, Val Loss: 0.5629\n",
      "Epoch [80/500], Train Loss: 0.4829, Val Loss: 0.5587\n",
      "Epoch [90/500], Train Loss: 0.4754, Val Loss: 0.5541\n",
      "Early stopping at epoch 97\n",
      "Best f1_score for fold 9: 0.7679\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7461, Val Loss: 0.6999\n",
      "Epoch [10/500], Train Loss: 0.6166, Val Loss: 0.6245\n",
      "Epoch [20/500], Train Loss: 0.5779, Val Loss: 0.6091\n",
      "Epoch [30/500], Train Loss: 0.5497, Val Loss: 0.5960\n",
      "Epoch [40/500], Train Loss: 0.5288, Val Loss: 0.5851\n",
      "Epoch [50/500], Train Loss: 0.5102, Val Loss: 0.5740\n",
      "Epoch [60/500], Train Loss: 0.4993, Val Loss: 0.5703\n",
      "Epoch [70/500], Train Loss: 0.4858, Val Loss: 0.5586\n",
      "Epoch [80/500], Train Loss: 0.4783, Val Loss: 0.5578\n",
      "Early stopping at epoch 84\n",
      "Best f1_score for fold 10: 0.7679\n",
      "MLP - ROS data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7310, Val Loss: 0.6686\n",
      "Epoch [10/500], Train Loss: 0.5707, Val Loss: 0.5728\n",
      "Epoch [20/500], Train Loss: 0.5116, Val Loss: 0.5153\n",
      "Epoch [30/500], Train Loss: 0.4787, Val Loss: 0.4718\n",
      "Epoch [40/500], Train Loss: 0.4562, Val Loss: 0.4496\n",
      "Epoch [50/500], Train Loss: 0.4430, Val Loss: 0.4351\n",
      "Epoch [60/500], Train Loss: 0.4330, Val Loss: 0.4248\n",
      "Epoch [70/500], Train Loss: 0.4248, Val Loss: 0.4131\n",
      "Epoch [80/500], Train Loss: 0.4188, Val Loss: 0.4030\n",
      "Epoch [90/500], Train Loss: 0.4162, Val Loss: 0.4024\n",
      "Early stopping at epoch 94\n",
      "Best f1_score for fold 1: 0.8403\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7336, Val Loss: 0.6813\n",
      "Epoch [10/500], Train Loss: 0.5698, Val Loss: 0.5740\n",
      "Epoch [20/500], Train Loss: 0.5124, Val Loss: 0.5258\n",
      "Epoch [30/500], Train Loss: 0.4811, Val Loss: 0.4904\n",
      "Epoch [40/500], Train Loss: 0.4579, Val Loss: 0.4624\n",
      "Epoch [50/500], Train Loss: 0.4462, Val Loss: 0.4453\n",
      "Epoch [60/500], Train Loss: 0.4367, Val Loss: 0.4363\n",
      "Epoch [70/500], Train Loss: 0.4277, Val Loss: 0.4267\n",
      "Epoch [80/500], Train Loss: 0.4198, Val Loss: 0.4210\n",
      "Early stopping at epoch 88\n",
      "Best f1_score for fold 2: 0.8403\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7328, Val Loss: 0.6711\n",
      "Epoch [10/500], Train Loss: 0.5721, Val Loss: 0.5691\n",
      "Epoch [20/500], Train Loss: 0.5136, Val Loss: 0.5168\n",
      "Epoch [30/500], Train Loss: 0.4785, Val Loss: 0.4759\n",
      "Epoch [40/500], Train Loss: 0.4564, Val Loss: 0.4587\n",
      "Epoch [50/500], Train Loss: 0.4419, Val Loss: 0.4353\n",
      "Epoch [60/500], Train Loss: 0.4320, Val Loss: 0.4285\n",
      "Epoch [70/500], Train Loss: 0.4271, Val Loss: 0.4270\n",
      "Epoch [80/500], Train Loss: 0.4171, Val Loss: 0.4155\n",
      "Epoch [90/500], Train Loss: 0.4157, Val Loss: 0.4083\n",
      "Early stopping at epoch 99\n",
      "Best f1_score for fold 3: 0.8403\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7311, Val Loss: 0.6653\n",
      "Epoch [10/500], Train Loss: 0.5727, Val Loss: 0.5617\n",
      "Epoch [20/500], Train Loss: 0.5143, Val Loss: 0.5037\n",
      "Epoch [30/500], Train Loss: 0.4825, Val Loss: 0.4678\n",
      "Epoch [40/500], Train Loss: 0.4608, Val Loss: 0.4459\n",
      "Epoch [50/500], Train Loss: 0.4457, Val Loss: 0.4336\n",
      "Epoch [60/500], Train Loss: 0.4347, Val Loss: 0.4205\n",
      "Epoch [70/500], Train Loss: 0.4274, Val Loss: 0.4107\n",
      "Epoch [80/500], Train Loss: 0.4198, Val Loss: 0.4016\n",
      "Epoch [90/500], Train Loss: 0.4158, Val Loss: 0.3951\n",
      "Epoch [100/500], Train Loss: 0.4117, Val Loss: 0.3927\n",
      "Epoch [110/500], Train Loss: 0.4054, Val Loss: 0.3919\n",
      "Epoch [120/500], Train Loss: 0.4024, Val Loss: 0.3858\n",
      "Epoch [130/500], Train Loss: 0.4014, Val Loss: 0.3864\n",
      "Epoch [140/500], Train Loss: 0.3974, Val Loss: 0.3840\n",
      "Epoch [150/500], Train Loss: 0.3977, Val Loss: 0.3808\n",
      "Early stopping at epoch 157\n",
      "Best f1_score for fold 4: 0.8516\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7334, Val Loss: 0.6865\n",
      "Epoch [10/500], Train Loss: 0.5715, Val Loss: 0.5748\n",
      "Epoch [20/500], Train Loss: 0.5115, Val Loss: 0.5175\n",
      "Epoch [30/500], Train Loss: 0.4804, Val Loss: 0.4766\n",
      "Epoch [40/500], Train Loss: 0.4589, Val Loss: 0.4505\n",
      "Epoch [50/500], Train Loss: 0.4445, Val Loss: 0.4390\n",
      "Epoch [60/500], Train Loss: 0.4348, Val Loss: 0.4315\n",
      "Epoch [70/500], Train Loss: 0.4254, Val Loss: 0.4244\n",
      "Epoch [80/500], Train Loss: 0.4210, Val Loss: 0.4196\n",
      "Epoch [90/500], Train Loss: 0.4160, Val Loss: 0.4147\n",
      "Epoch [100/500], Train Loss: 0.4135, Val Loss: 0.4141\n",
      "Epoch [110/500], Train Loss: 0.4106, Val Loss: 0.4064\n",
      "Epoch [120/500], Train Loss: 0.4055, Val Loss: 0.4030\n",
      "Epoch [130/500], Train Loss: 0.4052, Val Loss: 0.4003\n",
      "Early stopping at epoch 133\n",
      "Best f1_score for fold 5: 0.8516\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7392, Val Loss: 0.6733\n",
      "Epoch [10/500], Train Loss: 0.5749, Val Loss: 0.5660\n",
      "Epoch [20/500], Train Loss: 0.5133, Val Loss: 0.5104\n",
      "Epoch [30/500], Train Loss: 0.4810, Val Loss: 0.4682\n",
      "Epoch [40/500], Train Loss: 0.4613, Val Loss: 0.4469\n",
      "Epoch [50/500], Train Loss: 0.4461, Val Loss: 0.4293\n",
      "Epoch [60/500], Train Loss: 0.4363, Val Loss: 0.4228\n",
      "Epoch [70/500], Train Loss: 0.4283, Val Loss: 0.4135\n",
      "Epoch [80/500], Train Loss: 0.4212, Val Loss: 0.4063\n",
      "Epoch [90/500], Train Loss: 0.4161, Val Loss: 0.3992\n",
      "Epoch [100/500], Train Loss: 0.4114, Val Loss: 0.3937\n",
      "Epoch [110/500], Train Loss: 0.4085, Val Loss: 0.3952\n",
      "Epoch [120/500], Train Loss: 0.4084, Val Loss: 0.3895\n",
      "Epoch [130/500], Train Loss: 0.4046, Val Loss: 0.3902\n",
      "Early stopping at epoch 133\n",
      "Best f1_score for fold 6: 0.8518\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7362, Val Loss: 0.6694\n",
      "Epoch [10/500], Train Loss: 0.5725, Val Loss: 0.5665\n",
      "Epoch [20/500], Train Loss: 0.5112, Val Loss: 0.5105\n",
      "Epoch [30/500], Train Loss: 0.4743, Val Loss: 0.4731\n",
      "Epoch [40/500], Train Loss: 0.4575, Val Loss: 0.4475\n",
      "Epoch [50/500], Train Loss: 0.4428, Val Loss: 0.4396\n",
      "Epoch [60/500], Train Loss: 0.4326, Val Loss: 0.4258\n",
      "Epoch [70/500], Train Loss: 0.4256, Val Loss: 0.4222\n",
      "Epoch [80/500], Train Loss: 0.4202, Val Loss: 0.4111\n",
      "Epoch [90/500], Train Loss: 0.4141, Val Loss: 0.4098\n",
      "Early stopping at epoch 98\n",
      "Best f1_score for fold 7: 0.8518\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7337, Val Loss: 0.6671\n",
      "Epoch [10/500], Train Loss: 0.5707, Val Loss: 0.5661\n",
      "Epoch [20/500], Train Loss: 0.5093, Val Loss: 0.5050\n",
      "Epoch [30/500], Train Loss: 0.4784, Val Loss: 0.4704\n",
      "Epoch [40/500], Train Loss: 0.4590, Val Loss: 0.4464\n",
      "Epoch [50/500], Train Loss: 0.4485, Val Loss: 0.4353\n",
      "Epoch [60/500], Train Loss: 0.4366, Val Loss: 0.4252\n",
      "Epoch [70/500], Train Loss: 0.4283, Val Loss: 0.4161\n",
      "Epoch [80/500], Train Loss: 0.4228, Val Loss: 0.4077\n",
      "Epoch [90/500], Train Loss: 0.4173, Val Loss: 0.4034\n",
      "Epoch [100/500], Train Loss: 0.4134, Val Loss: 0.4004\n",
      "Early stopping at epoch 102\n",
      "Best f1_score for fold 8: 0.8518\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7341, Val Loss: 0.6772\n",
      "Epoch [10/500], Train Loss: 0.5717, Val Loss: 0.5734\n",
      "Epoch [20/500], Train Loss: 0.5130, Val Loss: 0.5234\n",
      "Epoch [30/500], Train Loss: 0.4809, Val Loss: 0.4850\n",
      "Epoch [40/500], Train Loss: 0.4562, Val Loss: 0.4673\n",
      "Epoch [50/500], Train Loss: 0.4443, Val Loss: 0.4460\n",
      "Epoch [60/500], Train Loss: 0.4333, Val Loss: 0.4382\n",
      "Epoch [70/500], Train Loss: 0.4263, Val Loss: 0.4312\n",
      "Epoch [80/500], Train Loss: 0.4222, Val Loss: 0.4254\n",
      "Epoch [90/500], Train Loss: 0.4164, Val Loss: 0.4233\n",
      "Epoch [100/500], Train Loss: 0.4104, Val Loss: 0.4156\n",
      "Epoch [110/500], Train Loss: 0.4067, Val Loss: 0.4127\n",
      "Epoch [120/500], Train Loss: 0.4037, Val Loss: 0.4067\n",
      "Epoch [130/500], Train Loss: 0.4008, Val Loss: 0.4037\n",
      "Epoch [140/500], Train Loss: 0.3963, Val Loss: 0.4046\n",
      "Early stopping at epoch 146\n",
      "Best f1_score for fold 9: 0.8518\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7300, Val Loss: 0.6840\n",
      "Epoch [10/500], Train Loss: 0.5693, Val Loss: 0.5712\n",
      "Epoch [20/500], Train Loss: 0.5129, Val Loss: 0.5210\n",
      "Epoch [30/500], Train Loss: 0.4808, Val Loss: 0.4884\n",
      "Epoch [40/500], Train Loss: 0.4606, Val Loss: 0.4635\n",
      "Epoch [50/500], Train Loss: 0.4480, Val Loss: 0.4518\n",
      "Epoch [60/500], Train Loss: 0.4389, Val Loss: 0.4439\n",
      "Epoch [70/500], Train Loss: 0.4310, Val Loss: 0.4293\n",
      "Epoch [80/500], Train Loss: 0.4252, Val Loss: 0.4249\n",
      "Epoch [90/500], Train Loss: 0.4202, Val Loss: 0.4192\n",
      "Epoch [100/500], Train Loss: 0.4143, Val Loss: 0.4151\n",
      "Epoch [110/500], Train Loss: 0.4128, Val Loss: 0.4145\n",
      "Epoch [120/500], Train Loss: 0.4096, Val Loss: 0.4076\n",
      "Epoch [130/500], Train Loss: 0.4074, Val Loss: 0.4051\n",
      "Epoch [140/500], Train Loss: 0.4051, Val Loss: 0.4075\n",
      "Epoch [150/500], Train Loss: 0.4015, Val Loss: 0.4045\n",
      "Epoch [160/500], Train Loss: 0.3997, Val Loss: 0.4011\n",
      "Epoch [170/500], Train Loss: 0.3957, Val Loss: 0.3991\n",
      "Epoch [180/500], Train Loss: 0.3951, Val Loss: 0.3968\n",
      "Epoch [190/500], Train Loss: 0.3947, Val Loss: 0.3970\n",
      "Early stopping at epoch 194\n",
      "Best f1_score for fold 10: 0.8518\n",
      "MLP - SMOTE data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.6966, Val Loss: 0.6204\n",
      "Epoch [10/500], Train Loss: 0.5324, Val Loss: 0.5297\n",
      "Epoch [20/500], Train Loss: 0.4851, Val Loss: 0.4884\n",
      "Epoch [30/500], Train Loss: 0.4546, Val Loss: 0.4596\n",
      "Epoch [40/500], Train Loss: 0.4375, Val Loss: 0.4469\n",
      "Epoch [50/500], Train Loss: 0.4236, Val Loss: 0.4338\n",
      "Epoch [60/500], Train Loss: 0.4170, Val Loss: 0.4213\n",
      "Epoch [70/500], Train Loss: 0.4082, Val Loss: 0.4152\n",
      "Epoch [80/500], Train Loss: 0.4004, Val Loss: 0.4124\n",
      "Epoch [90/500], Train Loss: 0.3966, Val Loss: 0.4108\n",
      "Epoch [100/500], Train Loss: 0.3914, Val Loss: 0.4079\n",
      "Epoch [110/500], Train Loss: 0.3878, Val Loss: 0.4066\n",
      "Epoch [120/500], Train Loss: 0.3862, Val Loss: 0.4025\n",
      "Early stopping at epoch 125\n",
      "Best f1_score for fold 1: 0.8435\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6933, Val Loss: 0.6345\n",
      "Epoch [10/500], Train Loss: 0.5287, Val Loss: 0.5325\n",
      "Epoch [20/500], Train Loss: 0.4800, Val Loss: 0.4963\n",
      "Epoch [30/500], Train Loss: 0.4505, Val Loss: 0.4648\n",
      "Epoch [40/500], Train Loss: 0.4316, Val Loss: 0.4551\n",
      "Epoch [50/500], Train Loss: 0.4207, Val Loss: 0.4372\n",
      "Epoch [60/500], Train Loss: 0.4110, Val Loss: 0.4302\n",
      "Epoch [70/500], Train Loss: 0.4044, Val Loss: 0.4249\n",
      "Epoch [80/500], Train Loss: 0.3994, Val Loss: 0.4225\n",
      "Epoch [90/500], Train Loss: 0.3948, Val Loss: 0.4205\n",
      "Epoch [100/500], Train Loss: 0.3901, Val Loss: 0.4200\n",
      "Epoch [110/500], Train Loss: 0.3880, Val Loss: 0.4159\n",
      "Epoch [120/500], Train Loss: 0.3831, Val Loss: 0.4181\n",
      "Epoch [130/500], Train Loss: 0.3797, Val Loss: 0.4097\n",
      "Early stopping at epoch 139\n",
      "Best f1_score for fold 2: 0.8435\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.6959, Val Loss: 0.6316\n",
      "Epoch [10/500], Train Loss: 0.5315, Val Loss: 0.5403\n",
      "Epoch [20/500], Train Loss: 0.4877, Val Loss: 0.5051\n",
      "Epoch [30/500], Train Loss: 0.4584, Val Loss: 0.4743\n",
      "Epoch [40/500], Train Loss: 0.4386, Val Loss: 0.4667\n",
      "Epoch [50/500], Train Loss: 0.4269, Val Loss: 0.4521\n",
      "Epoch [60/500], Train Loss: 0.4166, Val Loss: 0.4432\n",
      "Epoch [70/500], Train Loss: 0.4099, Val Loss: 0.4370\n",
      "Epoch [80/500], Train Loss: 0.4038, Val Loss: 0.4321\n",
      "Epoch [90/500], Train Loss: 0.3962, Val Loss: 0.4307\n",
      "Epoch [100/500], Train Loss: 0.3944, Val Loss: 0.4286\n",
      "Early stopping at epoch 106\n",
      "Best f1_score for fold 3: 0.8435\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.6980, Val Loss: 0.6247\n",
      "Epoch [10/500], Train Loss: 0.5319, Val Loss: 0.5269\n",
      "Epoch [20/500], Train Loss: 0.4901, Val Loss: 0.4911\n",
      "Epoch [30/500], Train Loss: 0.4601, Val Loss: 0.4657\n",
      "Epoch [40/500], Train Loss: 0.4429, Val Loss: 0.4535\n",
      "Epoch [50/500], Train Loss: 0.4283, Val Loss: 0.4358\n",
      "Epoch [60/500], Train Loss: 0.4182, Val Loss: 0.4280\n",
      "Epoch [70/500], Train Loss: 0.4083, Val Loss: 0.4214\n",
      "Epoch [80/500], Train Loss: 0.4040, Val Loss: 0.4198\n",
      "Epoch [90/500], Train Loss: 0.3979, Val Loss: 0.4160\n",
      "Epoch [100/500], Train Loss: 0.3906, Val Loss: 0.4140\n",
      "Epoch [110/500], Train Loss: 0.3906, Val Loss: 0.4164\n",
      "Early stopping at epoch 110\n",
      "Best f1_score for fold 4: 0.8435\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7003, Val Loss: 0.6276\n",
      "Epoch [10/500], Train Loss: 0.5319, Val Loss: 0.5355\n",
      "Epoch [20/500], Train Loss: 0.4848, Val Loss: 0.4989\n",
      "Epoch [30/500], Train Loss: 0.4542, Val Loss: 0.4746\n",
      "Epoch [40/500], Train Loss: 0.4379, Val Loss: 0.4558\n",
      "Epoch [50/500], Train Loss: 0.4229, Val Loss: 0.4441\n",
      "Epoch [60/500], Train Loss: 0.4148, Val Loss: 0.4351\n",
      "Epoch [70/500], Train Loss: 0.4055, Val Loss: 0.4293\n",
      "Epoch [80/500], Train Loss: 0.4018, Val Loss: 0.4233\n",
      "Epoch [90/500], Train Loss: 0.3942, Val Loss: 0.4212\n",
      "Epoch [100/500], Train Loss: 0.3924, Val Loss: 0.4233\n",
      "Epoch [110/500], Train Loss: 0.3884, Val Loss: 0.4181\n",
      "Epoch [120/500], Train Loss: 0.3852, Val Loss: 0.4131\n",
      "Epoch [130/500], Train Loss: 0.3822, Val Loss: 0.4109\n",
      "Epoch [140/500], Train Loss: 0.3818, Val Loss: 0.4087\n",
      "Epoch [150/500], Train Loss: 0.3786, Val Loss: 0.4061\n",
      "Epoch [160/500], Train Loss: 0.3766, Val Loss: 0.4074\n",
      "Early stopping at epoch 166\n",
      "Best f1_score for fold 5: 0.8435\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.6955, Val Loss: 0.6219\n",
      "Epoch [10/500], Train Loss: 0.5296, Val Loss: 0.5223\n",
      "Epoch [20/500], Train Loss: 0.4830, Val Loss: 0.4884\n",
      "Epoch [30/500], Train Loss: 0.4532, Val Loss: 0.4642\n",
      "Epoch [40/500], Train Loss: 0.4351, Val Loss: 0.4444\n",
      "Epoch [50/500], Train Loss: 0.4216, Val Loss: 0.4352\n",
      "Epoch [60/500], Train Loss: 0.4114, Val Loss: 0.4252\n",
      "Epoch [70/500], Train Loss: 0.4033, Val Loss: 0.4191\n",
      "Epoch [80/500], Train Loss: 0.3997, Val Loss: 0.4159\n",
      "Epoch [90/500], Train Loss: 0.3981, Val Loss: 0.4138\n",
      "Epoch [100/500], Train Loss: 0.3927, Val Loss: 0.4063\n",
      "Epoch [110/500], Train Loss: 0.3890, Val Loss: 0.4065\n",
      "Epoch [120/500], Train Loss: 0.3856, Val Loss: 0.4036\n",
      "Epoch [130/500], Train Loss: 0.3840, Val Loss: 0.4049\n",
      "Epoch [140/500], Train Loss: 0.3807, Val Loss: 0.4006\n",
      "Early stopping at epoch 140\n",
      "Best f1_score for fold 6: 0.8460\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7005, Val Loss: 0.6200\n",
      "Epoch [10/500], Train Loss: 0.5286, Val Loss: 0.5218\n",
      "Epoch [20/500], Train Loss: 0.4828, Val Loss: 0.4933\n",
      "Epoch [30/500], Train Loss: 0.4532, Val Loss: 0.4673\n",
      "Epoch [40/500], Train Loss: 0.4383, Val Loss: 0.4545\n",
      "Epoch [50/500], Train Loss: 0.4225, Val Loss: 0.4455\n",
      "Epoch [60/500], Train Loss: 0.4142, Val Loss: 0.4345\n",
      "Epoch [70/500], Train Loss: 0.4060, Val Loss: 0.4333\n",
      "Epoch [80/500], Train Loss: 0.4002, Val Loss: 0.4255\n",
      "Epoch [90/500], Train Loss: 0.3956, Val Loss: 0.4222\n",
      "Early stopping at epoch 99\n",
      "Best f1_score for fold 7: 0.8460\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.6959, Val Loss: 0.6272\n",
      "Epoch [10/500], Train Loss: 0.5258, Val Loss: 0.5373\n",
      "Epoch [20/500], Train Loss: 0.4837, Val Loss: 0.4948\n",
      "Epoch [30/500], Train Loss: 0.4553, Val Loss: 0.4691\n",
      "Epoch [40/500], Train Loss: 0.4357, Val Loss: 0.4514\n",
      "Epoch [50/500], Train Loss: 0.4231, Val Loss: 0.4429\n",
      "Epoch [60/500], Train Loss: 0.4136, Val Loss: 0.4326\n",
      "Epoch [70/500], Train Loss: 0.4062, Val Loss: 0.4251\n",
      "Epoch [80/500], Train Loss: 0.4046, Val Loss: 0.4249\n",
      "Epoch [90/500], Train Loss: 0.3980, Val Loss: 0.4223\n",
      "Epoch [100/500], Train Loss: 0.3950, Val Loss: 0.4187\n",
      "Epoch [110/500], Train Loss: 0.3905, Val Loss: 0.4129\n",
      "Early stopping at epoch 117\n",
      "Best f1_score for fold 8: 0.8460\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.6962, Val Loss: 0.6385\n",
      "Epoch [10/500], Train Loss: 0.5306, Val Loss: 0.5379\n",
      "Epoch [20/500], Train Loss: 0.4861, Val Loss: 0.5004\n",
      "Epoch [30/500], Train Loss: 0.4589, Val Loss: 0.4752\n",
      "Epoch [40/500], Train Loss: 0.4373, Val Loss: 0.4604\n",
      "Epoch [50/500], Train Loss: 0.4251, Val Loss: 0.4506\n",
      "Epoch [60/500], Train Loss: 0.4155, Val Loss: 0.4400\n",
      "Epoch [70/500], Train Loss: 0.4086, Val Loss: 0.4328\n",
      "Epoch [80/500], Train Loss: 0.4010, Val Loss: 0.4239\n",
      "Epoch [90/500], Train Loss: 0.3958, Val Loss: 0.4240\n",
      "Epoch [100/500], Train Loss: 0.3945, Val Loss: 0.4213\n",
      "Early stopping at epoch 100\n",
      "Best f1_score for fold 9: 0.8460\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.6953, Val Loss: 0.6230\n",
      "Epoch [10/500], Train Loss: 0.5325, Val Loss: 0.5262\n",
      "Epoch [20/500], Train Loss: 0.4872, Val Loss: 0.4906\n",
      "Epoch [30/500], Train Loss: 0.4573, Val Loss: 0.4673\n",
      "Epoch [40/500], Train Loss: 0.4386, Val Loss: 0.4509\n",
      "Epoch [50/500], Train Loss: 0.4278, Val Loss: 0.4422\n",
      "Epoch [60/500], Train Loss: 0.4162, Val Loss: 0.4354\n",
      "Epoch [70/500], Train Loss: 0.4098, Val Loss: 0.4238\n",
      "Epoch [80/500], Train Loss: 0.4035, Val Loss: 0.4243\n",
      "Epoch [90/500], Train Loss: 0.3995, Val Loss: 0.4232\n",
      "Epoch [100/500], Train Loss: 0.3985, Val Loss: 0.4153\n",
      "Epoch [110/500], Train Loss: 0.3942, Val Loss: 0.4126\n",
      "Early stopping at epoch 110\n",
      "Best f1_score for fold 10: 0.8460\n",
      "Model state saved to ./models/mlp_best_state.pth\n",
      "Model state saved to ./models/mlp_best_ros_state.pth\n",
      "Model state saved to ./models/mlp_best_smote_state.pth\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "n_folds = 10\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "# MLP\n",
    "stratified_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_ros = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_smote = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print('MLP - original data')\n",
    "mlp_best_state, mlp_fold_results_list = find_best_model_state('MLP', X, y, input_dim=X.shape[1], num_classes=3, stratified_fold=stratified_fold, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "print('MLP - ROS data')\n",
    "mlp_best_ros_state, mlp_fold_results_list_ros = find_best_model_state('MLP', X_ros, y_ros, input_dim=X_ros.shape[1], num_classes=3, stratified_fold=stratified_fold_ros, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "print('MLP - SMOTE data')\n",
    "mlp_best_smote_state, mlp_fold_results_list_smote = find_best_model_state('MLP', X_smote, y_smote, input_dim=X_smote.shape[1], num_classes=3, stratified_fold=stratified_fold_smote, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "save_model_state(mlp_best_state, './models/mlp_best_state.pth')\n",
    "save_model_state(mlp_best_ros_state, './models/mlp_best_ros_state.pth')\n",
    "save_model_state(mlp_best_smote_state, './models/mlp_best_smote_state.pth')\n",
    "# Save csv\n",
    "mlp_fold_results_df = pd.DataFrame(mlp_fold_results_list).to_csv('./models/mlp_fold_results.csv', index=False)\n",
    "mlp_fold_results_df_ros = pd.DataFrame(mlp_fold_results_list_ros).to_csv('./models/mlp_fold_results_ros.csv', index=False)\n",
    "mlp_fold_results_df_smote = pd.DataFrame(mlp_fold_results_list_smote).to_csv('./models/mlp_fold_results_smote.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer - original data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7123, Val Loss: 0.6636\n",
      "Epoch [10/500], Train Loss: 0.6245, Val Loss: 0.6305\n",
      "Epoch [20/500], Train Loss: 0.5775, Val Loss: 0.6051\n",
      "Epoch [30/500], Train Loss: 0.5038, Val Loss: 0.5553\n",
      "Epoch [40/500], Train Loss: 0.4440, Val Loss: 0.5200\n",
      "Epoch [50/500], Train Loss: 0.3999, Val Loss: 0.5144\n",
      "Epoch [60/500], Train Loss: 0.3643, Val Loss: 0.5098\n",
      "Early stopping at epoch 61\n",
      "Best f1_score for fold 1: 0.7963\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7163, Val Loss: 0.6601\n",
      "Epoch [10/500], Train Loss: 0.6266, Val Loss: 0.6287\n",
      "Epoch [20/500], Train Loss: 0.5873, Val Loss: 0.6064\n",
      "Epoch [30/500], Train Loss: 0.5327, Val Loss: 0.5768\n",
      "Epoch [40/500], Train Loss: 0.4707, Val Loss: 0.5597\n",
      "Epoch [50/500], Train Loss: 0.4259, Val Loss: 0.5209\n",
      "Epoch [60/500], Train Loss: 0.3924, Val Loss: 0.5167\n",
      "Early stopping at epoch 63\n",
      "Best f1_score for fold 2: 0.7963\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7145, Val Loss: 0.6797\n",
      "Epoch [10/500], Train Loss: 0.6234, Val Loss: 0.6355\n",
      "Epoch [20/500], Train Loss: 0.5850, Val Loss: 0.6052\n",
      "Epoch [30/500], Train Loss: 0.5252, Val Loss: 0.5686\n",
      "Epoch [40/500], Train Loss: 0.4649, Val Loss: 0.5390\n",
      "Epoch [50/500], Train Loss: 0.4206, Val Loss: 0.5248\n",
      "Early stopping at epoch 58\n",
      "Best f1_score for fold 3: 0.7963\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7140, Val Loss: 0.6629\n",
      "Epoch [10/500], Train Loss: 0.6250, Val Loss: 0.6304\n",
      "Epoch [20/500], Train Loss: 0.5859, Val Loss: 0.6082\n",
      "Epoch [30/500], Train Loss: 0.5217, Val Loss: 0.5643\n",
      "Epoch [40/500], Train Loss: 0.4576, Val Loss: 0.5442\n",
      "Epoch [50/500], Train Loss: 0.4102, Val Loss: 0.5246\n",
      "Epoch [60/500], Train Loss: 0.3758, Val Loss: 0.5309\n",
      "Early stopping at epoch 68\n",
      "Best f1_score for fold 4: 0.7963\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7055, Val Loss: 0.6629\n",
      "Epoch [10/500], Train Loss: 0.6237, Val Loss: 0.6344\n",
      "Epoch [20/500], Train Loss: 0.5832, Val Loss: 0.6124\n",
      "Epoch [30/500], Train Loss: 0.5166, Val Loss: 0.5728\n",
      "Epoch [40/500], Train Loss: 0.4488, Val Loss: 0.5578\n",
      "Epoch [50/500], Train Loss: 0.4041, Val Loss: 0.5256\n",
      "Early stopping at epoch 59\n",
      "Best f1_score for fold 5: 0.7963\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7162, Val Loss: 0.6486\n",
      "Epoch [10/500], Train Loss: 0.6335, Val Loss: 0.6163\n",
      "Epoch [20/500], Train Loss: 0.6041, Val Loss: 0.5988\n",
      "Epoch [30/500], Train Loss: 0.5596, Val Loss: 0.5709\n",
      "Epoch [40/500], Train Loss: 0.5049, Val Loss: 0.5363\n",
      "Epoch [50/500], Train Loss: 0.4522, Val Loss: 0.5045\n",
      "Epoch [60/500], Train Loss: 0.4084, Val Loss: 0.5071\n",
      "Epoch [70/500], Train Loss: 0.3838, Val Loss: 0.5138\n",
      "Early stopping at epoch 75\n",
      "Best f1_score for fold 6: 0.7983\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7138, Val Loss: 0.6707\n",
      "Epoch [10/500], Train Loss: 0.6297, Val Loss: 0.6235\n",
      "Epoch [20/500], Train Loss: 0.5978, Val Loss: 0.6058\n",
      "Epoch [30/500], Train Loss: 0.5475, Val Loss: 0.5807\n",
      "Epoch [40/500], Train Loss: 0.4931, Val Loss: 0.5519\n",
      "Epoch [50/500], Train Loss: 0.4486, Val Loss: 0.5461\n",
      "Epoch [60/500], Train Loss: 0.4161, Val Loss: 0.5237\n",
      "Early stopping at epoch 62\n",
      "Best f1_score for fold 7: 0.7983\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7064, Val Loss: 0.6638\n",
      "Epoch [10/500], Train Loss: 0.6235, Val Loss: 0.6228\n",
      "Epoch [20/500], Train Loss: 0.5833, Val Loss: 0.6054\n",
      "Epoch [30/500], Train Loss: 0.5191, Val Loss: 0.5720\n",
      "Epoch [40/500], Train Loss: 0.4567, Val Loss: 0.5325\n",
      "Epoch [50/500], Train Loss: 0.4144, Val Loss: 0.5321\n",
      "Epoch [60/500], Train Loss: 0.3766, Val Loss: 0.5573\n",
      "Early stopping at epoch 60\n",
      "Best f1_score for fold 8: 0.7983\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7089, Val Loss: 0.6626\n",
      "Epoch [10/500], Train Loss: 0.6208, Val Loss: 0.6305\n",
      "Epoch [20/500], Train Loss: 0.5799, Val Loss: 0.6038\n",
      "Epoch [30/500], Train Loss: 0.5038, Val Loss: 0.5694\n",
      "Epoch [40/500], Train Loss: 0.4416, Val Loss: 0.5565\n",
      "Epoch [50/500], Train Loss: 0.3963, Val Loss: 0.5589\n",
      "Epoch [60/500], Train Loss: 0.3620, Val Loss: 0.5298\n",
      "Early stopping at epoch 62\n",
      "Best f1_score for fold 9: 0.7983\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7170, Val Loss: 0.6544\n",
      "Epoch [10/500], Train Loss: 0.6262, Val Loss: 0.6173\n",
      "Epoch [20/500], Train Loss: 0.5923, Val Loss: 0.6055\n",
      "Epoch [30/500], Train Loss: 0.5306, Val Loss: 0.5597\n",
      "Epoch [40/500], Train Loss: 0.4649, Val Loss: 0.5249\n",
      "Epoch [50/500], Train Loss: 0.4154, Val Loss: 0.5211\n",
      "Epoch [60/500], Train Loss: 0.3771, Val Loss: 0.5159\n",
      "Early stopping at epoch 62\n",
      "Best f1_score for fold 10: 0.7983\n",
      "Transformer - ROS data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.6957, Val Loss: 0.6461\n",
      "Epoch [10/500], Train Loss: 0.5619, Val Loss: 0.5534\n",
      "Epoch [20/500], Train Loss: 0.4372, Val Loss: 0.4269\n",
      "Epoch [30/500], Train Loss: 0.3734, Val Loss: 0.3705\n",
      "Epoch [40/500], Train Loss: 0.3314, Val Loss: 0.3356\n",
      "Epoch [50/500], Train Loss: 0.3017, Val Loss: 0.3285\n",
      "Epoch [60/500], Train Loss: 0.2812, Val Loss: 0.3215\n",
      "Epoch [70/500], Train Loss: 0.2644, Val Loss: 0.3196\n",
      "Early stopping at epoch 78\n",
      "Best f1_score for fold 1: 0.8966\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6978, Val Loss: 0.6542\n",
      "Epoch [10/500], Train Loss: 0.5543, Val Loss: 0.5384\n",
      "Epoch [20/500], Train Loss: 0.4236, Val Loss: 0.4172\n",
      "Epoch [30/500], Train Loss: 0.3574, Val Loss: 0.3706\n",
      "Epoch [40/500], Train Loss: 0.3153, Val Loss: 0.3536\n",
      "Epoch [50/500], Train Loss: 0.2900, Val Loss: 0.3474\n",
      "Epoch [60/500], Train Loss: 0.2682, Val Loss: 0.3480\n",
      "Epoch [70/500], Train Loss: 0.2477, Val Loss: 0.3412\n",
      "Early stopping at epoch 77\n",
      "Best f1_score for fold 2: 0.8966\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7054, Val Loss: 0.6480\n",
      "Epoch [10/500], Train Loss: 0.5727, Val Loss: 0.5556\n",
      "Epoch [20/500], Train Loss: 0.4547, Val Loss: 0.4395\n",
      "Epoch [30/500], Train Loss: 0.3885, Val Loss: 0.3958\n",
      "Epoch [40/500], Train Loss: 0.3445, Val Loss: 0.3607\n",
      "Epoch [50/500], Train Loss: 0.3129, Val Loss: 0.3488\n",
      "Epoch [60/500], Train Loss: 0.2910, Val Loss: 0.3441\n",
      "Epoch [70/500], Train Loss: 0.2724, Val Loss: 0.3457\n",
      "Epoch [80/500], Train Loss: 0.2570, Val Loss: 0.3376\n",
      "Early stopping at epoch 81\n",
      "Best f1_score for fold 3: 0.8966\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7003, Val Loss: 0.6514\n",
      "Epoch [10/500], Train Loss: 0.5740, Val Loss: 0.5523\n",
      "Epoch [20/500], Train Loss: 0.4589, Val Loss: 0.4373\n",
      "Epoch [30/500], Train Loss: 0.3868, Val Loss: 0.3799\n",
      "Epoch [40/500], Train Loss: 0.3474, Val Loss: 0.3500\n",
      "Epoch [50/500], Train Loss: 0.3154, Val Loss: 0.3388\n",
      "Early stopping at epoch 53\n",
      "Best f1_score for fold 4: 0.8966\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.6995, Val Loss: 0.6631\n",
      "Epoch [10/500], Train Loss: 0.5839, Val Loss: 0.5725\n",
      "Epoch [20/500], Train Loss: 0.4655, Val Loss: 0.4476\n",
      "Epoch [30/500], Train Loss: 0.3834, Val Loss: 0.3849\n",
      "Epoch [40/500], Train Loss: 0.3377, Val Loss: 0.3551\n",
      "Epoch [50/500], Train Loss: 0.3069, Val Loss: 0.3420\n",
      "Epoch [60/500], Train Loss: 0.2856, Val Loss: 0.3330\n",
      "Epoch [70/500], Train Loss: 0.2672, Val Loss: 0.3453\n",
      "Epoch [80/500], Train Loss: 0.2525, Val Loss: 0.3246\n",
      "Early stopping at epoch 83\n",
      "Best f1_score for fold 5: 0.8966\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.6985, Val Loss: 0.6480\n",
      "Epoch [10/500], Train Loss: 0.5628, Val Loss: 0.5405\n",
      "Epoch [20/500], Train Loss: 0.4275, Val Loss: 0.4022\n",
      "Epoch [30/500], Train Loss: 0.3599, Val Loss: 0.3499\n",
      "Epoch [40/500], Train Loss: 0.3196, Val Loss: 0.3309\n",
      "Epoch [50/500], Train Loss: 0.2926, Val Loss: 0.3142\n",
      "Epoch [60/500], Train Loss: 0.2712, Val Loss: 0.3135\n",
      "Epoch [70/500], Train Loss: 0.2568, Val Loss: 0.3041\n",
      "Early stopping at epoch 79\n",
      "Best f1_score for fold 6: 0.9045\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7030, Val Loss: 0.6705\n",
      "Epoch [10/500], Train Loss: 0.5979, Val Loss: 0.5854\n",
      "Epoch [20/500], Train Loss: 0.4999, Val Loss: 0.4807\n",
      "Epoch [30/500], Train Loss: 0.4159, Val Loss: 0.4063\n",
      "Epoch [40/500], Train Loss: 0.3638, Val Loss: 0.3674\n",
      "Epoch [50/500], Train Loss: 0.3302, Val Loss: 0.3479\n",
      "Epoch [60/500], Train Loss: 0.3099, Val Loss: 0.3346\n",
      "Epoch [70/500], Train Loss: 0.2888, Val Loss: 0.3361\n",
      "Epoch [80/500], Train Loss: 0.2731, Val Loss: 0.3267\n",
      "Early stopping at epoch 83\n",
      "Best f1_score for fold 7: 0.9045\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7024, Val Loss: 0.6440\n",
      "Epoch [10/500], Train Loss: 0.5599, Val Loss: 0.5321\n",
      "Epoch [20/500], Train Loss: 0.4388, Val Loss: 0.4121\n",
      "Epoch [30/500], Train Loss: 0.3684, Val Loss: 0.3588\n",
      "Epoch [40/500], Train Loss: 0.3288, Val Loss: 0.3361\n",
      "Epoch [50/500], Train Loss: 0.2942, Val Loss: 0.3147\n",
      "Epoch [60/500], Train Loss: 0.2773, Val Loss: 0.3094\n",
      "Epoch [70/500], Train Loss: 0.2585, Val Loss: 0.3081\n",
      "Early stopping at epoch 74\n",
      "Best f1_score for fold 8: 0.9045\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.6986, Val Loss: 0.6552\n",
      "Epoch [10/500], Train Loss: 0.5587, Val Loss: 0.5481\n",
      "Epoch [20/500], Train Loss: 0.4367, Val Loss: 0.4365\n",
      "Epoch [30/500], Train Loss: 0.3710, Val Loss: 0.3819\n",
      "Epoch [40/500], Train Loss: 0.3309, Val Loss: 0.3641\n",
      "Epoch [50/500], Train Loss: 0.3013, Val Loss: 0.3501\n",
      "Epoch [60/500], Train Loss: 0.2837, Val Loss: 0.3424\n",
      "Epoch [70/500], Train Loss: 0.2668, Val Loss: 0.3381\n",
      "Early stopping at epoch 78\n",
      "Best f1_score for fold 9: 0.9045\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.6959, Val Loss: 0.6490\n",
      "Epoch [10/500], Train Loss: 0.5665, Val Loss: 0.5554\n",
      "Epoch [20/500], Train Loss: 0.4451, Val Loss: 0.4207\n",
      "Epoch [30/500], Train Loss: 0.3741, Val Loss: 0.3648\n",
      "Epoch [40/500], Train Loss: 0.3296, Val Loss: 0.3345\n",
      "Epoch [50/500], Train Loss: 0.3011, Val Loss: 0.3407\n",
      "Epoch [60/500], Train Loss: 0.2789, Val Loss: 0.3269\n",
      "Early stopping at epoch 65\n",
      "Best f1_score for fold 10: 0.9045\n",
      "Transformer - SMOTE data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.6701, Val Loss: 0.5957\n",
      "Epoch [10/500], Train Loss: 0.4949, Val Loss: 0.4724\n",
      "Epoch [20/500], Train Loss: 0.3920, Val Loss: 0.3898\n",
      "Epoch [30/500], Train Loss: 0.3384, Val Loss: 0.3582\n",
      "Epoch [40/500], Train Loss: 0.3013, Val Loss: 0.3393\n",
      "Epoch [50/500], Train Loss: 0.2784, Val Loss: 0.3338\n",
      "Epoch [60/500], Train Loss: 0.2596, Val Loss: 0.3400\n",
      "Epoch [70/500], Train Loss: 0.2473, Val Loss: 0.3446\n",
      "Early stopping at epoch 73\n",
      "Best f1_score for fold 1: 0.8843\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6646, Val Loss: 0.5942\n",
      "Epoch [10/500], Train Loss: 0.5094, Val Loss: 0.5039\n",
      "Epoch [20/500], Train Loss: 0.4099, Val Loss: 0.4153\n",
      "Epoch [30/500], Train Loss: 0.3507, Val Loss: 0.3764\n",
      "Epoch [40/500], Train Loss: 0.3114, Val Loss: 0.3599\n",
      "Epoch [50/500], Train Loss: 0.2823, Val Loss: 0.3561\n",
      "Epoch [60/500], Train Loss: 0.2621, Val Loss: 0.3536\n",
      "Early stopping at epoch 67\n",
      "Best f1_score for fold 2: 0.8843\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.6723, Val Loss: 0.6244\n",
      "Epoch [10/500], Train Loss: 0.5034, Val Loss: 0.5023\n",
      "Epoch [20/500], Train Loss: 0.4081, Val Loss: 0.4203\n",
      "Epoch [30/500], Train Loss: 0.3509, Val Loss: 0.3844\n",
      "Epoch [40/500], Train Loss: 0.3160, Val Loss: 0.3613\n",
      "Epoch [50/500], Train Loss: 0.2910, Val Loss: 0.3700\n",
      "Epoch [60/500], Train Loss: 0.2726, Val Loss: 0.3631\n",
      "Epoch [70/500], Train Loss: 0.2573, Val Loss: 0.3591\n",
      "Early stopping at epoch 72\n",
      "Best f1_score for fold 3: 0.8843\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.6750, Val Loss: 0.6052\n",
      "Epoch [10/500], Train Loss: 0.5116, Val Loss: 0.4990\n",
      "Epoch [20/500], Train Loss: 0.4047, Val Loss: 0.4030\n",
      "Epoch [30/500], Train Loss: 0.3443, Val Loss: 0.3643\n",
      "Epoch [40/500], Train Loss: 0.3061, Val Loss: 0.3537\n",
      "Epoch [50/500], Train Loss: 0.2834, Val Loss: 0.3429\n",
      "Early stopping at epoch 59\n",
      "Best f1_score for fold 4: 0.8843\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.6657, Val Loss: 0.5830\n",
      "Epoch [10/500], Train Loss: 0.5041, Val Loss: 0.4927\n",
      "Epoch [20/500], Train Loss: 0.4070, Val Loss: 0.4139\n",
      "Epoch [30/500], Train Loss: 0.3497, Val Loss: 0.3712\n",
      "Epoch [40/500], Train Loss: 0.3143, Val Loss: 0.3642\n",
      "Early stopping at epoch 46\n",
      "Best f1_score for fold 5: 0.8843\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.6658, Val Loss: 0.5962\n",
      "Epoch [10/500], Train Loss: 0.5008, Val Loss: 0.4782\n",
      "Epoch [20/500], Train Loss: 0.4035, Val Loss: 0.3986\n",
      "Epoch [30/500], Train Loss: 0.3457, Val Loss: 0.3624\n",
      "Epoch [40/500], Train Loss: 0.3081, Val Loss: 0.3521\n",
      "Epoch [50/500], Train Loss: 0.2857, Val Loss: 0.3440\n",
      "Epoch [60/500], Train Loss: 0.2616, Val Loss: 0.3366\n",
      "Early stopping at epoch 68\n",
      "Best f1_score for fold 6: 0.8848\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.6658, Val Loss: 0.6007\n",
      "Epoch [10/500], Train Loss: 0.5137, Val Loss: 0.5064\n",
      "Epoch [20/500], Train Loss: 0.4204, Val Loss: 0.4221\n",
      "Epoch [30/500], Train Loss: 0.3613, Val Loss: 0.3790\n",
      "Epoch [40/500], Train Loss: 0.3219, Val Loss: 0.3642\n",
      "Epoch [50/500], Train Loss: 0.2928, Val Loss: 0.3581\n",
      "Epoch [60/500], Train Loss: 0.2741, Val Loss: 0.3625\n",
      "Epoch [70/500], Train Loss: 0.2516, Val Loss: 0.3757\n",
      "Early stopping at epoch 71\n",
      "Best f1_score for fold 7: 0.8848\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.6647, Val Loss: 0.6121\n",
      "Epoch [10/500], Train Loss: 0.5024, Val Loss: 0.4909\n",
      "Epoch [20/500], Train Loss: 0.4070, Val Loss: 0.4294\n",
      "Epoch [30/500], Train Loss: 0.3471, Val Loss: 0.3714\n",
      "Epoch [40/500], Train Loss: 0.3114, Val Loss: 0.3667\n",
      "Epoch [50/500], Train Loss: 0.2825, Val Loss: 0.3619\n",
      "Epoch [60/500], Train Loss: 0.2651, Val Loss: 0.3574\n",
      "Early stopping at epoch 63\n",
      "Best f1_score for fold 8: 0.8848\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.6788, Val Loss: 0.5993\n",
      "Epoch [10/500], Train Loss: 0.5230, Val Loss: 0.5228\n",
      "Epoch [20/500], Train Loss: 0.4530, Val Loss: 0.4555\n",
      "Epoch [30/500], Train Loss: 0.3977, Val Loss: 0.4193\n",
      "Epoch [40/500], Train Loss: 0.3575, Val Loss: 0.3841\n",
      "Epoch [50/500], Train Loss: 0.3283, Val Loss: 0.3655\n",
      "Epoch [60/500], Train Loss: 0.3086, Val Loss: 0.3583\n",
      "Early stopping at epoch 67\n",
      "Best f1_score for fold 9: 0.8848\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.6717, Val Loss: 0.6036\n",
      "Epoch [10/500], Train Loss: 0.5029, Val Loss: 0.4905\n",
      "Epoch [20/500], Train Loss: 0.4055, Val Loss: 0.4042\n",
      "Epoch [30/500], Train Loss: 0.3476, Val Loss: 0.3651\n",
      "Epoch [40/500], Train Loss: 0.3135, Val Loss: 0.3468\n",
      "Epoch [50/500], Train Loss: 0.2924, Val Loss: 0.3468\n",
      "Epoch [60/500], Train Loss: 0.2691, Val Loss: 0.3513\n",
      "Early stopping at epoch 60\n",
      "Best f1_score for fold 10: 0.8848\n",
      "Model state saved to ./models/transformer_best_state.pth\n",
      "Model state saved to ./models/transformer_best_ros_state.pth\n",
      "Model state saved to ./models/transformer_best_smote_state.pth\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "n_folds = 10\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "# Transformer\n",
    "stratified_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_ros = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_smote = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "print('Transformer - original data')\n",
    "transformer_best_state, transformer_fold_results_list = find_best_model_state('Transformer', X, y, input_dim=X.shape[1], num_classes=3, stratified_fold=stratified_fold, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "print('Transformer - ROS data')\n",
    "transformer_best_ros_state, transformer_fold_results_list_ros = find_best_model_state('Transformer', X_ros, y_ros, input_dim=X_ros.shape[1], num_classes=3, stratified_fold=stratified_fold_ros, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "print('Transformer - SMOTE data')\n",
    "transformer_best_smote_state, transformer_fold_results_list_smote = find_best_model_state('Transformer', X_smote, y_smote, input_dim=X_smote.shape[1], num_classes=3, stratified_fold=stratified_fold_smote, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "save_model_state(transformer_best_state, './models/transformer_best_state.pth')\n",
    "save_model_state(transformer_best_ros_state, './models/transformer_best_ros_state.pth')\n",
    "save_model_state(transformer_best_smote_state, './models/transformer_best_smote_state.pth')\n",
    "# Save csv\n",
    "transformer_fold_results_df = pd.DataFrame(transformer_fold_results_list).to_csv('./models/transformer_fold_results.csv', index=False)\n",
    "transformer_fold_results_df_ros = pd.DataFrame(transformer_fold_results_list_ros).to_csv('./models/transformer_fold_results_ros.csv', index=False)\n",
    "transformer_fold_results_df_smote = pd.DataFrame(transformer_fold_results_list_smote).to_csv('./models/transformer_fold_results_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabTransformer - original data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7177, Val Loss: 0.6738\n",
      "Epoch [10/500], Train Loss: 0.5999, Val Loss: 0.6273\n",
      "Epoch [20/500], Train Loss: 0.2390, Val Loss: 0.7052\n",
      "Early stopping at epoch 24\n",
      "Best f1_score for fold 1: 0.7346\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7190, Val Loss: 0.6679\n",
      "Epoch [10/500], Train Loss: 0.5941, Val Loss: 0.6275\n",
      "Epoch [20/500], Train Loss: 0.2279, Val Loss: 0.7234\n",
      "Early stopping at epoch 24\n",
      "Best f1_score for fold 2: 0.7492\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7197, Val Loss: 0.6839\n",
      "Epoch [10/500], Train Loss: 0.6041, Val Loss: 0.6322\n",
      "Epoch [20/500], Train Loss: 0.2793, Val Loss: 0.7147\n",
      "Early stopping at epoch 24\n",
      "Best f1_score for fold 3: 0.7492\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7250, Val Loss: 0.6813\n",
      "Epoch [10/500], Train Loss: 0.5999, Val Loss: 0.6242\n",
      "Epoch [20/500], Train Loss: 0.2517, Val Loss: 0.7397\n",
      "Early stopping at epoch 24\n",
      "Best f1_score for fold 4: 0.7492\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7248, Val Loss: 0.6810\n",
      "Epoch [10/500], Train Loss: 0.5917, Val Loss: 0.6243\n",
      "Epoch [20/500], Train Loss: 0.2371, Val Loss: 0.7661\n",
      "Early stopping at epoch 24\n",
      "Best f1_score for fold 5: 0.7492\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7199, Val Loss: 0.6602\n",
      "Epoch [10/500], Train Loss: 0.6008, Val Loss: 0.6092\n",
      "Epoch [20/500], Train Loss: 0.2204, Val Loss: 0.7219\n",
      "Early stopping at epoch 23\n",
      "Best f1_score for fold 6: 0.7492\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7224, Val Loss: 0.6714\n",
      "Epoch [10/500], Train Loss: 0.6013, Val Loss: 0.6237\n",
      "Epoch [20/500], Train Loss: 0.2453, Val Loss: 0.7023\n",
      "Early stopping at epoch 25\n",
      "Best f1_score for fold 7: 0.7492\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7218, Val Loss: 0.6805\n",
      "Epoch [10/500], Train Loss: 0.6019, Val Loss: 0.6239\n",
      "Epoch [20/500], Train Loss: 0.2596, Val Loss: 0.7216\n",
      "Early stopping at epoch 22\n",
      "Best f1_score for fold 8: 0.7492\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7180, Val Loss: 0.7011\n",
      "Epoch [10/500], Train Loss: 0.5984, Val Loss: 0.6325\n",
      "Epoch [20/500], Train Loss: 0.2436, Val Loss: 0.7564\n",
      "Early stopping at epoch 23\n",
      "Best f1_score for fold 9: 0.7492\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7250, Val Loss: 0.6741\n",
      "Epoch [10/500], Train Loss: 0.5972, Val Loss: 0.6209\n",
      "Epoch [20/500], Train Loss: 0.2237, Val Loss: 0.7886\n",
      "Early stopping at epoch 23\n",
      "Best f1_score for fold 10: 0.7492\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7051, Val Loss: 0.6691\n",
      "Epoch [10/500], Train Loss: 0.2992, Val Loss: 0.3941\n",
      "Epoch [20/500], Train Loss: 0.0596, Val Loss: 0.5540\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 1: 0.8840\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7114, Val Loss: 0.6684\n",
      "Epoch [10/500], Train Loss: 0.3136, Val Loss: 0.4156\n",
      "Epoch [20/500], Train Loss: 0.0593, Val Loss: 0.5605\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 2: 0.8841\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7078, Val Loss: 0.6645\n",
      "Epoch [10/500], Train Loss: 0.2996, Val Loss: 0.4076\n",
      "Epoch [20/500], Train Loss: 0.0620, Val Loss: 0.5631\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 3: 0.8841\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7064, Val Loss: 0.6653\n",
      "Epoch [10/500], Train Loss: 0.3028, Val Loss: 0.3955\n",
      "Epoch [20/500], Train Loss: 0.0643, Val Loss: 0.5635\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 4: 0.8841\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7017, Val Loss: 0.6799\n",
      "Epoch [10/500], Train Loss: 0.3076, Val Loss: 0.3996\n",
      "Epoch [20/500], Train Loss: 0.0663, Val Loss: 0.5958\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 5: 0.8854\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7098, Val Loss: 0.6752\n",
      "Epoch [10/500], Train Loss: 0.3161, Val Loss: 0.3942\n",
      "Epoch [20/500], Train Loss: 0.0630, Val Loss: 0.5507\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 6: 0.8854\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7054, Val Loss: 0.6622\n",
      "Epoch [10/500], Train Loss: 0.3089, Val Loss: 0.4067\n",
      "Epoch [20/500], Train Loss: 0.0735, Val Loss: 0.5605\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 7: 0.8854\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7045, Val Loss: 0.6620\n",
      "Epoch [10/500], Train Loss: 0.3135, Val Loss: 0.3938\n",
      "Epoch [20/500], Train Loss: 0.0696, Val Loss: 0.5044\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 8: 0.8854\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7054, Val Loss: 0.6694\n",
      "Epoch [10/500], Train Loss: 0.3196, Val Loss: 0.4265\n",
      "Epoch [20/500], Train Loss: 0.0668, Val Loss: 0.5425\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 9: 0.8854\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7047, Val Loss: 0.6899\n",
      "Epoch [10/500], Train Loss: 0.3094, Val Loss: 0.4052\n",
      "Epoch [20/500], Train Loss: 0.0669, Val Loss: 0.5332\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 10: 0.8870\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.6811, Val Loss: 0.6352\n",
      "Epoch [10/500], Train Loss: 0.3433, Val Loss: 0.4347\n",
      "Early stopping at epoch 19\n",
      "Best f1_score for fold 1: 0.8474\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6796, Val Loss: 0.6498\n",
      "Epoch [10/500], Train Loss: 0.3554, Val Loss: 0.4431\n",
      "Epoch [20/500], Train Loss: 0.0929, Val Loss: 0.6982\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 2: 0.8474\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.6869, Val Loss: 0.6499\n",
      "Epoch [10/500], Train Loss: 0.3560, Val Loss: 0.4427\n",
      "Epoch [20/500], Train Loss: 0.0901, Val Loss: 0.6298\n",
      "Early stopping at epoch 22\n",
      "Best f1_score for fold 3: 0.8474\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.6808, Val Loss: 0.6261\n",
      "Epoch [10/500], Train Loss: 0.3497, Val Loss: 0.4444\n",
      "Epoch [20/500], Train Loss: 0.0826, Val Loss: 0.6082\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 4: 0.8488\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.6823, Val Loss: 0.6394\n",
      "Epoch [10/500], Train Loss: 0.3553, Val Loss: 0.4477\n",
      "Epoch [20/500], Train Loss: 0.0951, Val Loss: 0.6491\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 5: 0.8488\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.6822, Val Loss: 0.6557\n",
      "Epoch [10/500], Train Loss: 0.3302, Val Loss: 0.4401\n",
      "Epoch [20/500], Train Loss: 0.0797, Val Loss: 0.6552\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 6: 0.8488\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.6830, Val Loss: 0.6250\n",
      "Epoch [10/500], Train Loss: 0.3492, Val Loss: 0.4298\n",
      "Epoch [20/500], Train Loss: 0.0844, Val Loss: 0.6375\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 7: 0.8488\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.6790, Val Loss: 0.6564\n",
      "Epoch [10/500], Train Loss: 0.3433, Val Loss: 0.4461\n",
      "Epoch [20/500], Train Loss: 0.0877, Val Loss: 0.6498\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 8: 0.8488\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.6816, Val Loss: 0.6502\n",
      "Epoch [10/500], Train Loss: 0.3506, Val Loss: 0.4448\n",
      "Early stopping at epoch 19\n",
      "Best f1_score for fold 9: 0.8488\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.6833, Val Loss: 0.6395\n",
      "Epoch [10/500], Train Loss: 0.3487, Val Loss: 0.4365\n",
      "Epoch [20/500], Train Loss: 0.0869, Val Loss: 0.6458\n",
      "Early stopping at epoch 22\n",
      "Best f1_score for fold 10: 0.8524\n",
      "Model state saved to ./models/tab_transformer_best_state.pth\n",
      "Model state saved to ./models/tab_transformer_best_ros_state.pth\n",
      "Model state saved to ./models/tab_transformer_best_smote_state.pth\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "n_folds = 10\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "# TabTransformer\n",
    "stratified_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_ros = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_smote = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print('TabTransformer - original data')\n",
    "tab_transformer_best_state, tab_transformer_fold_results_list = find_best_model_state('TabTransformer', X, y, input_dim=X.shape[1], num_classes=3, stratified_fold=stratified_fold, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "tab_transformer_best_ros_state, tab_transformer_fold_results_list_ros = find_best_model_state('TabTransformer', X_ros, y_ros, input_dim=X_ros.shape[1], num_classes=3, stratified_fold=stratified_fold_ros, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "tab_transformer_best_smote_state, tab_transformer_fold_results_list_smote = find_best_model_state('TabTransformer', X_smote, y_smote, input_dim=X_smote.shape[1], num_classes=3, stratified_fold=stratified_fold_smote, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "save_model_state(tab_transformer_best_state, './models/tab_transformer_best_state.pth')\n",
    "save_model_state(tab_transformer_best_ros_state, './models/tab_transformer_best_ros_state.pth')\n",
    "save_model_state(tab_transformer_best_smote_state, './models/tab_transformer_best_smote_state.pth')\n",
    "# Save csv\n",
    "tab_transformer_fold_results_df = pd.DataFrame(tab_transformer_fold_results_list).to_csv('./models/tab_transformer_fold_results.csv', index=False)\n",
    "tab_transformer_fold_results_df_ros = pd.DataFrame(tab_transformer_fold_results_list_ros).to_csv('./models/tab_transformer_fold_results_ros.csv', index=False)\n",
    "tab_transformer_fold_results_df_smote = pd.DataFrame(tab_transformer_fold_results_list_smote).to_csv('./models/tab_transformer_fold_results_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTTransformer - original data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7190, Val Loss: 0.6755\n",
      "Epoch [10/500], Train Loss: 0.6264, Val Loss: 0.6249\n",
      "Epoch [20/500], Train Loss: 0.5990, Val Loss: 0.6119\n",
      "Epoch [30/500], Train Loss: 0.5581, Val Loss: 0.5866\n",
      "Epoch [40/500], Train Loss: 0.5139, Val Loss: 0.5537\n",
      "Epoch [50/500], Train Loss: 0.4755, Val Loss: 0.5247\n",
      "Epoch [60/500], Train Loss: 0.4429, Val Loss: 0.5116\n",
      "Epoch [70/500], Train Loss: 0.4179, Val Loss: 0.4994\n",
      "Epoch [80/500], Train Loss: 0.4009, Val Loss: 0.4984\n",
      "Epoch [90/500], Train Loss: 0.3848, Val Loss: 0.4794\n",
      "Early stopping at epoch 99\n",
      "Best f1_score for fold 1: 0.8098\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7207, Val Loss: 0.6832\n",
      "Epoch [10/500], Train Loss: 0.6259, Val Loss: 0.6269\n",
      "Epoch [20/500], Train Loss: 0.5914, Val Loss: 0.5998\n",
      "Epoch [30/500], Train Loss: 0.5406, Val Loss: 0.5575\n",
      "Epoch [40/500], Train Loss: 0.4913, Val Loss: 0.5257\n",
      "Epoch [50/500], Train Loss: 0.4552, Val Loss: 0.5118\n",
      "Epoch [60/500], Train Loss: 0.4232, Val Loss: 0.5050\n",
      "Epoch [70/500], Train Loss: 0.3991, Val Loss: 0.4839\n",
      "Epoch [80/500], Train Loss: 0.3830, Val Loss: 0.4855\n",
      "Early stopping at epoch 81\n",
      "Best f1_score for fold 2: 0.8098\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7123, Val Loss: 0.6690\n",
      "Epoch [10/500], Train Loss: 0.6281, Val Loss: 0.6364\n",
      "Epoch [20/500], Train Loss: 0.6029, Val Loss: 0.6204\n",
      "Epoch [30/500], Train Loss: 0.5672, Val Loss: 0.5925\n",
      "Epoch [40/500], Train Loss: 0.5231, Val Loss: 0.5518\n",
      "Epoch [50/500], Train Loss: 0.4883, Val Loss: 0.5330\n",
      "Epoch [60/500], Train Loss: 0.4582, Val Loss: 0.5196\n",
      "Epoch [70/500], Train Loss: 0.4331, Val Loss: 0.5107\n",
      "Early stopping at epoch 78\n",
      "Best f1_score for fold 3: 0.8098\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7216, Val Loss: 0.6797\n",
      "Epoch [10/500], Train Loss: 0.6255, Val Loss: 0.6328\n",
      "Epoch [20/500], Train Loss: 0.6075, Val Loss: 0.6237\n",
      "Epoch [30/500], Train Loss: 0.5765, Val Loss: 0.5999\n",
      "Epoch [40/500], Train Loss: 0.5330, Val Loss: 0.5794\n",
      "Epoch [50/500], Train Loss: 0.4927, Val Loss: 0.5451\n",
      "Epoch [60/500], Train Loss: 0.4621, Val Loss: 0.5254\n",
      "Epoch [70/500], Train Loss: 0.4347, Val Loss: 0.5182\n",
      "Epoch [80/500], Train Loss: 0.4142, Val Loss: 0.5161\n",
      "Early stopping at epoch 81\n",
      "Best f1_score for fold 4: 0.8098\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7124, Val Loss: 0.6834\n",
      "Epoch [10/500], Train Loss: 0.6261, Val Loss: 0.6332\n",
      "Epoch [20/500], Train Loss: 0.6021, Val Loss: 0.6134\n",
      "Epoch [30/500], Train Loss: 0.5631, Val Loss: 0.5814\n",
      "Epoch [40/500], Train Loss: 0.5216, Val Loss: 0.5588\n",
      "Epoch [50/500], Train Loss: 0.4843, Val Loss: 0.5345\n",
      "Epoch [60/500], Train Loss: 0.4515, Val Loss: 0.5120\n",
      "Epoch [70/500], Train Loss: 0.4294, Val Loss: 0.5039\n",
      "Epoch [80/500], Train Loss: 0.4048, Val Loss: 0.4979\n",
      "Epoch [90/500], Train Loss: 0.3919, Val Loss: 0.4849\n",
      "Epoch [100/500], Train Loss: 0.3763, Val Loss: 0.4935\n",
      "Epoch [110/500], Train Loss: 0.3627, Val Loss: 0.4874\n",
      "Early stopping at epoch 115\n",
      "Best f1_score for fold 5: 0.8098\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7167, Val Loss: 0.6630\n",
      "Epoch [10/500], Train Loss: 0.6269, Val Loss: 0.6141\n",
      "Epoch [20/500], Train Loss: 0.6006, Val Loss: 0.5988\n",
      "Epoch [30/500], Train Loss: 0.5671, Val Loss: 0.5720\n",
      "Epoch [40/500], Train Loss: 0.5284, Val Loss: 0.5460\n",
      "Epoch [50/500], Train Loss: 0.4922, Val Loss: 0.5310\n",
      "Epoch [60/500], Train Loss: 0.4619, Val Loss: 0.5076\n",
      "Epoch [70/500], Train Loss: 0.4333, Val Loss: 0.5057\n",
      "Epoch [80/500], Train Loss: 0.4157, Val Loss: 0.5059\n",
      "Epoch [90/500], Train Loss: 0.3968, Val Loss: 0.4881\n",
      "Epoch [100/500], Train Loss: 0.3831, Val Loss: 0.4838\n",
      "Early stopping at epoch 105\n",
      "Best f1_score for fold 6: 0.8098\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7135, Val Loss: 0.6697\n",
      "Epoch [10/500], Train Loss: 0.6299, Val Loss: 0.6234\n",
      "Epoch [20/500], Train Loss: 0.6066, Val Loss: 0.6104\n",
      "Epoch [30/500], Train Loss: 0.5684, Val Loss: 0.5796\n",
      "Epoch [40/500], Train Loss: 0.5234, Val Loss: 0.5472\n",
      "Epoch [50/500], Train Loss: 0.4824, Val Loss: 0.5185\n",
      "Epoch [60/500], Train Loss: 0.4514, Val Loss: 0.5013\n",
      "Epoch [70/500], Train Loss: 0.4269, Val Loss: 0.5032\n",
      "Epoch [80/500], Train Loss: 0.4015, Val Loss: 0.4901\n",
      "Epoch [90/500], Train Loss: 0.3877, Val Loss: 0.4841\n",
      "Epoch [100/500], Train Loss: 0.3678, Val Loss: 0.4871\n",
      "Early stopping at epoch 100\n",
      "Best f1_score for fold 7: 0.8098\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7153, Val Loss: 0.6680\n",
      "Epoch [10/500], Train Loss: 0.6266, Val Loss: 0.6247\n",
      "Epoch [20/500], Train Loss: 0.6035, Val Loss: 0.6137\n",
      "Epoch [30/500], Train Loss: 0.5682, Val Loss: 0.5911\n",
      "Epoch [40/500], Train Loss: 0.5245, Val Loss: 0.5660\n",
      "Epoch [50/500], Train Loss: 0.4822, Val Loss: 0.5299\n",
      "Epoch [60/500], Train Loss: 0.4539, Val Loss: 0.5106\n",
      "Epoch [70/500], Train Loss: 0.4280, Val Loss: 0.5083\n",
      "Epoch [80/500], Train Loss: 0.4030, Val Loss: 0.4935\n",
      "Epoch [90/500], Train Loss: 0.3926, Val Loss: 0.4875\n",
      "Epoch [100/500], Train Loss: 0.3751, Val Loss: 0.4786\n",
      "Early stopping at epoch 109\n",
      "Best f1_score for fold 8: 0.8098\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7213, Val Loss: 0.6872\n",
      "Epoch [10/500], Train Loss: 0.6257, Val Loss: 0.6354\n",
      "Epoch [20/500], Train Loss: 0.5980, Val Loss: 0.6181\n",
      "Epoch [30/500], Train Loss: 0.5547, Val Loss: 0.5932\n",
      "Epoch [40/500], Train Loss: 0.5138, Val Loss: 0.5581\n",
      "Epoch [50/500], Train Loss: 0.4770, Val Loss: 0.5337\n",
      "Epoch [60/500], Train Loss: 0.4454, Val Loss: 0.5253\n",
      "Epoch [70/500], Train Loss: 0.4221, Val Loss: 0.5157\n",
      "Epoch [80/500], Train Loss: 0.3998, Val Loss: 0.5055\n",
      "Epoch [90/500], Train Loss: 0.3820, Val Loss: 0.5053\n",
      "Epoch [100/500], Train Loss: 0.3681, Val Loss: 0.5094\n",
      "Early stopping at epoch 102\n",
      "Best f1_score for fold 9: 0.8098\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7232, Val Loss: 0.6908\n",
      "Epoch [10/500], Train Loss: 0.6308, Val Loss: 0.6280\n",
      "Epoch [20/500], Train Loss: 0.6104, Val Loss: 0.6125\n",
      "Epoch [30/500], Train Loss: 0.5823, Val Loss: 0.5982\n",
      "Epoch [40/500], Train Loss: 0.5454, Val Loss: 0.5801\n",
      "Epoch [50/500], Train Loss: 0.5094, Val Loss: 0.5610\n",
      "Epoch [60/500], Train Loss: 0.4755, Val Loss: 0.5418\n",
      "Epoch [70/500], Train Loss: 0.4483, Val Loss: 0.5298\n",
      "Epoch [80/500], Train Loss: 0.4229, Val Loss: 0.5133\n",
      "Epoch [90/500], Train Loss: 0.4024, Val Loss: 0.5228\n",
      "Early stopping at epoch 90\n",
      "Best f1_score for fold 10: 0.8098\n",
      "FTTransformer - ROS data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7102, Val Loss: 0.6551\n",
      "Epoch [10/500], Train Loss: 0.6003, Val Loss: 0.5904\n",
      "Epoch [20/500], Train Loss: 0.5066, Val Loss: 0.4789\n",
      "Epoch [30/500], Train Loss: 0.4400, Val Loss: 0.4084\n",
      "Epoch [40/500], Train Loss: 0.3992, Val Loss: 0.3702\n",
      "Epoch [50/500], Train Loss: 0.3707, Val Loss: 0.3589\n",
      "Epoch [60/500], Train Loss: 0.3477, Val Loss: 0.3412\n",
      "Epoch [70/500], Train Loss: 0.3320, Val Loss: 0.3412\n",
      "Epoch [80/500], Train Loss: 0.3162, Val Loss: 0.3221\n",
      "Epoch [90/500], Train Loss: 0.3051, Val Loss: 0.3296\n",
      "Epoch [100/500], Train Loss: 0.2928, Val Loss: 0.3241\n",
      "Early stopping at epoch 107\n",
      "Best f1_score for fold 1: 0.8896\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6940, Val Loss: 0.6524\n",
      "Epoch [10/500], Train Loss: 0.5986, Val Loss: 0.5923\n",
      "Epoch [20/500], Train Loss: 0.5085, Val Loss: 0.5003\n",
      "Epoch [30/500], Train Loss: 0.4378, Val Loss: 0.4228\n",
      "Epoch [40/500], Train Loss: 0.3957, Val Loss: 0.3923\n",
      "Epoch [50/500], Train Loss: 0.3635, Val Loss: 0.3667\n",
      "Epoch [60/500], Train Loss: 0.3432, Val Loss: 0.3495\n",
      "Epoch [70/500], Train Loss: 0.3267, Val Loss: 0.3396\n",
      "Epoch [80/500], Train Loss: 0.3116, Val Loss: 0.3375\n",
      "Early stopping at epoch 84\n",
      "Best f1_score for fold 2: 0.8896\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7070, Val Loss: 0.6480\n",
      "Epoch [10/500], Train Loss: 0.6012, Val Loss: 0.5879\n",
      "Epoch [20/500], Train Loss: 0.5106, Val Loss: 0.4966\n",
      "Epoch [30/500], Train Loss: 0.4384, Val Loss: 0.4314\n",
      "Epoch [40/500], Train Loss: 0.3953, Val Loss: 0.3839\n",
      "Epoch [50/500], Train Loss: 0.3654, Val Loss: 0.3627\n",
      "Epoch [60/500], Train Loss: 0.3444, Val Loss: 0.3506\n",
      "Epoch [70/500], Train Loss: 0.3226, Val Loss: 0.3456\n",
      "Epoch [80/500], Train Loss: 0.3127, Val Loss: 0.3397\n",
      "Epoch [90/500], Train Loss: 0.2979, Val Loss: 0.3382\n",
      "Epoch [100/500], Train Loss: 0.2836, Val Loss: 0.3329\n",
      "Early stopping at epoch 105\n",
      "Best f1_score for fold 3: 0.8896\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7064, Val Loss: 0.6491\n",
      "Epoch [10/500], Train Loss: 0.5964, Val Loss: 0.5832\n",
      "Epoch [20/500], Train Loss: 0.5146, Val Loss: 0.4874\n",
      "Epoch [30/500], Train Loss: 0.4437, Val Loss: 0.4035\n",
      "Epoch [40/500], Train Loss: 0.4014, Val Loss: 0.3661\n",
      "Epoch [50/500], Train Loss: 0.3709, Val Loss: 0.3567\n",
      "Epoch [60/500], Train Loss: 0.3474, Val Loss: 0.3306\n",
      "Epoch [70/500], Train Loss: 0.3291, Val Loss: 0.3219\n",
      "Epoch [80/500], Train Loss: 0.3123, Val Loss: 0.3127\n",
      "Epoch [90/500], Train Loss: 0.3015, Val Loss: 0.3063\n",
      "Epoch [100/500], Train Loss: 0.2928, Val Loss: 0.2996\n",
      "Early stopping at epoch 102\n",
      "Best f1_score for fold 4: 0.8946\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7006, Val Loss: 0.6608\n",
      "Epoch [10/500], Train Loss: 0.6055, Val Loss: 0.6035\n",
      "Epoch [20/500], Train Loss: 0.5337, Val Loss: 0.5178\n",
      "Epoch [30/500], Train Loss: 0.4662, Val Loss: 0.4508\n",
      "Epoch [40/500], Train Loss: 0.4187, Val Loss: 0.4049\n",
      "Epoch [50/500], Train Loss: 0.3852, Val Loss: 0.3780\n",
      "Epoch [60/500], Train Loss: 0.3617, Val Loss: 0.3564\n",
      "Epoch [70/500], Train Loss: 0.3408, Val Loss: 0.3466\n",
      "Epoch [80/500], Train Loss: 0.3241, Val Loss: 0.3393\n",
      "Epoch [90/500], Train Loss: 0.3110, Val Loss: 0.3301\n",
      "Epoch [100/500], Train Loss: 0.2978, Val Loss: 0.3306\n",
      "Epoch [110/500], Train Loss: 0.2859, Val Loss: 0.3201\n",
      "Early stopping at epoch 110\n",
      "Best f1_score for fold 5: 0.8946\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7067, Val Loss: 0.6433\n",
      "Epoch [10/500], Train Loss: 0.6022, Val Loss: 0.5807\n",
      "Epoch [20/500], Train Loss: 0.5038, Val Loss: 0.4802\n",
      "Epoch [30/500], Train Loss: 0.4268, Val Loss: 0.3971\n",
      "Epoch [40/500], Train Loss: 0.3854, Val Loss: 0.3513\n",
      "Epoch [50/500], Train Loss: 0.3555, Val Loss: 0.3408\n",
      "Epoch [60/500], Train Loss: 0.3320, Val Loss: 0.3286\n",
      "Epoch [70/500], Train Loss: 0.3148, Val Loss: 0.3144\n",
      "Epoch [80/500], Train Loss: 0.3006, Val Loss: 0.3151\n",
      "Epoch [90/500], Train Loss: 0.2869, Val Loss: 0.3042\n",
      "Epoch [100/500], Train Loss: 0.2797, Val Loss: 0.3017\n",
      "Early stopping at epoch 100\n",
      "Best f1_score for fold 6: 0.8949\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7065, Val Loss: 0.6642\n",
      "Epoch [10/500], Train Loss: 0.6014, Val Loss: 0.5977\n",
      "Epoch [20/500], Train Loss: 0.5215, Val Loss: 0.5032\n",
      "Epoch [30/500], Train Loss: 0.4486, Val Loss: 0.4327\n",
      "Epoch [40/500], Train Loss: 0.3992, Val Loss: 0.3955\n",
      "Epoch [50/500], Train Loss: 0.3683, Val Loss: 0.3572\n",
      "Epoch [60/500], Train Loss: 0.3457, Val Loss: 0.3476\n",
      "Epoch [70/500], Train Loss: 0.3291, Val Loss: 0.3372\n",
      "Epoch [80/500], Train Loss: 0.3144, Val Loss: 0.3400\n",
      "Epoch [90/500], Train Loss: 0.2993, Val Loss: 0.3261\n",
      "Epoch [100/500], Train Loss: 0.2894, Val Loss: 0.3211\n",
      "Early stopping at epoch 108\n",
      "Best f1_score for fold 7: 0.8949\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7021, Val Loss: 0.6396\n",
      "Epoch [10/500], Train Loss: 0.5902, Val Loss: 0.5724\n",
      "Epoch [20/500], Train Loss: 0.4931, Val Loss: 0.4659\n",
      "Epoch [30/500], Train Loss: 0.4276, Val Loss: 0.4000\n",
      "Epoch [40/500], Train Loss: 0.3853, Val Loss: 0.3714\n",
      "Epoch [50/500], Train Loss: 0.3579, Val Loss: 0.3444\n",
      "Epoch [60/500], Train Loss: 0.3372, Val Loss: 0.3243\n",
      "Epoch [70/500], Train Loss: 0.3201, Val Loss: 0.3190\n",
      "Epoch [80/500], Train Loss: 0.3063, Val Loss: 0.3118\n",
      "Early stopping at epoch 81\n",
      "Best f1_score for fold 8: 0.8949\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7010, Val Loss: 0.6602\n",
      "Epoch [10/500], Train Loss: 0.6011, Val Loss: 0.6012\n",
      "Epoch [20/500], Train Loss: 0.5149, Val Loss: 0.5034\n",
      "Epoch [30/500], Train Loss: 0.4404, Val Loss: 0.4203\n",
      "Epoch [40/500], Train Loss: 0.3926, Val Loss: 0.3737\n",
      "Epoch [50/500], Train Loss: 0.3625, Val Loss: 0.3505\n",
      "Epoch [60/500], Train Loss: 0.3395, Val Loss: 0.3382\n",
      "Epoch [70/500], Train Loss: 0.3210, Val Loss: 0.3347\n",
      "Epoch [80/500], Train Loss: 0.3088, Val Loss: 0.3273\n",
      "Early stopping at epoch 84\n",
      "Best f1_score for fold 9: 0.8949\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7037, Val Loss: 0.6602\n",
      "Epoch [10/500], Train Loss: 0.6060, Val Loss: 0.6075\n",
      "Epoch [20/500], Train Loss: 0.5297, Val Loss: 0.5225\n",
      "Epoch [30/500], Train Loss: 0.4629, Val Loss: 0.4387\n",
      "Epoch [40/500], Train Loss: 0.4161, Val Loss: 0.3982\n",
      "Epoch [50/500], Train Loss: 0.3825, Val Loss: 0.3672\n",
      "Epoch [60/500], Train Loss: 0.3568, Val Loss: 0.3551\n",
      "Epoch [70/500], Train Loss: 0.3375, Val Loss: 0.3434\n",
      "Epoch [80/500], Train Loss: 0.3219, Val Loss: 0.3389\n",
      "Epoch [90/500], Train Loss: 0.3080, Val Loss: 0.3212\n",
      "Epoch [100/500], Train Loss: 0.3004, Val Loss: 0.3224\n",
      "Epoch [110/500], Train Loss: 0.2892, Val Loss: 0.3112\n",
      "Early stopping at epoch 117\n",
      "Best f1_score for fold 10: 0.8949\n",
      "FTTransformer - SMOTE data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.6814, Val Loss: 0.6070\n",
      "Epoch [10/500], Train Loss: 0.5247, Val Loss: 0.5125\n",
      "Epoch [20/500], Train Loss: 0.4410, Val Loss: 0.4352\n",
      "Epoch [30/500], Train Loss: 0.3828, Val Loss: 0.3789\n",
      "Epoch [40/500], Train Loss: 0.3469, Val Loss: 0.3598\n",
      "Epoch [50/500], Train Loss: 0.3232, Val Loss: 0.3594\n",
      "Epoch [60/500], Train Loss: 0.3048, Val Loss: 0.3454\n",
      "Epoch [70/500], Train Loss: 0.2911, Val Loss: 0.3302\n",
      "Early stopping at epoch 79\n",
      "Best f1_score for fold 1: 0.8762\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6845, Val Loss: 0.6211\n",
      "Epoch [10/500], Train Loss: 0.5336, Val Loss: 0.5339\n",
      "Epoch [20/500], Train Loss: 0.4341, Val Loss: 0.4294\n",
      "Epoch [30/500], Train Loss: 0.3693, Val Loss: 0.3762\n",
      "Epoch [40/500], Train Loss: 0.3296, Val Loss: 0.3587\n",
      "Epoch [50/500], Train Loss: 0.3103, Val Loss: 0.3469\n",
      "Epoch [60/500], Train Loss: 0.2915, Val Loss: 0.3401\n",
      "Early stopping at epoch 62\n",
      "Best f1_score for fold 2: 0.8762\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.6832, Val Loss: 0.6203\n",
      "Epoch [10/500], Train Loss: 0.5328, Val Loss: 0.5345\n",
      "Epoch [20/500], Train Loss: 0.4492, Val Loss: 0.4493\n",
      "Epoch [30/500], Train Loss: 0.3885, Val Loss: 0.3922\n",
      "Epoch [40/500], Train Loss: 0.3461, Val Loss: 0.3703\n",
      "Epoch [50/500], Train Loss: 0.3184, Val Loss: 0.3653\n",
      "Epoch [60/500], Train Loss: 0.2992, Val Loss: 0.3610\n",
      "Epoch [70/500], Train Loss: 0.2831, Val Loss: 0.3599\n",
      "Epoch [80/500], Train Loss: 0.2708, Val Loss: 0.3585\n",
      "Epoch [90/500], Train Loss: 0.2567, Val Loss: 0.3549\n",
      "Early stopping at epoch 93\n",
      "Best f1_score for fold 3: 0.8762\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.6751, Val Loss: 0.5930\n",
      "Epoch [10/500], Train Loss: 0.5201, Val Loss: 0.5107\n",
      "Epoch [20/500], Train Loss: 0.4135, Val Loss: 0.4145\n",
      "Epoch [30/500], Train Loss: 0.3594, Val Loss: 0.3760\n",
      "Epoch [40/500], Train Loss: 0.3252, Val Loss: 0.3558\n",
      "Epoch [50/500], Train Loss: 0.3036, Val Loss: 0.3492\n",
      "Epoch [60/500], Train Loss: 0.2840, Val Loss: 0.3432\n",
      "Epoch [70/500], Train Loss: 0.2703, Val Loss: 0.3445\n",
      "Early stopping at epoch 72\n",
      "Best f1_score for fold 4: 0.8771\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.6743, Val Loss: 0.5975\n",
      "Epoch [10/500], Train Loss: 0.5357, Val Loss: 0.5327\n",
      "Epoch [20/500], Train Loss: 0.4633, Val Loss: 0.4574\n",
      "Epoch [30/500], Train Loss: 0.3972, Val Loss: 0.4064\n",
      "Epoch [40/500], Train Loss: 0.3545, Val Loss: 0.3754\n",
      "Epoch [50/500], Train Loss: 0.3284, Val Loss: 0.3616\n",
      "Epoch [60/500], Train Loss: 0.3117, Val Loss: 0.3571\n",
      "Epoch [70/500], Train Loss: 0.2983, Val Loss: 0.3477\n",
      "Epoch [80/500], Train Loss: 0.2838, Val Loss: 0.3524\n",
      "Epoch [90/500], Train Loss: 0.2731, Val Loss: 0.3433\n",
      "Early stopping at epoch 90\n",
      "Best f1_score for fold 5: 0.8771\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.6771, Val Loss: 0.6011\n",
      "Epoch [10/500], Train Loss: 0.5287, Val Loss: 0.5158\n",
      "Epoch [20/500], Train Loss: 0.4486, Val Loss: 0.4381\n",
      "Epoch [30/500], Train Loss: 0.3858, Val Loss: 0.3936\n",
      "Epoch [40/500], Train Loss: 0.3508, Val Loss: 0.3630\n",
      "Epoch [50/500], Train Loss: 0.3243, Val Loss: 0.3520\n",
      "Epoch [60/500], Train Loss: 0.3081, Val Loss: 0.3471\n",
      "Epoch [70/500], Train Loss: 0.2951, Val Loss: 0.3393\n",
      "Early stopping at epoch 75\n",
      "Best f1_score for fold 6: 0.8771\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.6907, Val Loss: 0.6040\n",
      "Epoch [10/500], Train Loss: 0.5309, Val Loss: 0.5227\n",
      "Epoch [20/500], Train Loss: 0.4573, Val Loss: 0.4443\n",
      "Epoch [30/500], Train Loss: 0.3966, Val Loss: 0.4008\n",
      "Epoch [40/500], Train Loss: 0.3591, Val Loss: 0.3762\n",
      "Epoch [50/500], Train Loss: 0.3319, Val Loss: 0.3602\n",
      "Epoch [60/500], Train Loss: 0.3145, Val Loss: 0.3455\n",
      "Early stopping at epoch 69\n",
      "Best f1_score for fold 7: 0.8771\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.6692, Val Loss: 0.6036\n",
      "Epoch [10/500], Train Loss: 0.5321, Val Loss: 0.5284\n",
      "Epoch [20/500], Train Loss: 0.4608, Val Loss: 0.4773\n",
      "Epoch [30/500], Train Loss: 0.3976, Val Loss: 0.4005\n",
      "Epoch [40/500], Train Loss: 0.3560, Val Loss: 0.3733\n",
      "Epoch [50/500], Train Loss: 0.3276, Val Loss: 0.3608\n",
      "Epoch [60/500], Train Loss: 0.3072, Val Loss: 0.3505\n",
      "Epoch [70/500], Train Loss: 0.2928, Val Loss: 0.3404\n",
      "Early stopping at epoch 77\n",
      "Best f1_score for fold 8: 0.8771\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.6804, Val Loss: 0.6253\n",
      "Epoch [10/500], Train Loss: 0.5283, Val Loss: 0.5297\n",
      "Epoch [20/500], Train Loss: 0.4454, Val Loss: 0.4545\n",
      "Epoch [30/500], Train Loss: 0.3820, Val Loss: 0.3997\n",
      "Epoch [40/500], Train Loss: 0.3453, Val Loss: 0.3697\n",
      "Epoch [50/500], Train Loss: 0.3197, Val Loss: 0.3536\n",
      "Epoch [60/500], Train Loss: 0.2999, Val Loss: 0.3535\n",
      "Epoch [70/500], Train Loss: 0.2863, Val Loss: 0.3469\n",
      "Epoch [80/500], Train Loss: 0.2708, Val Loss: 0.3500\n",
      "Early stopping at epoch 83\n",
      "Best f1_score for fold 9: 0.8791\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.6726, Val Loss: 0.6023\n",
      "Epoch [10/500], Train Loss: 0.5338, Val Loss: 0.5296\n",
      "Epoch [20/500], Train Loss: 0.4524, Val Loss: 0.4403\n",
      "Epoch [30/500], Train Loss: 0.3900, Val Loss: 0.3878\n",
      "Epoch [40/500], Train Loss: 0.3517, Val Loss: 0.3596\n",
      "Epoch [50/500], Train Loss: 0.3234, Val Loss: 0.3516\n",
      "Epoch [60/500], Train Loss: 0.3054, Val Loss: 0.3367\n",
      "Epoch [70/500], Train Loss: 0.2899, Val Loss: 0.3396\n",
      "Epoch [80/500], Train Loss: 0.2765, Val Loss: 0.3361\n",
      "Epoch [90/500], Train Loss: 0.2669, Val Loss: 0.3312\n",
      "Early stopping at epoch 95\n",
      "Best f1_score for fold 10: 0.8791\n",
      "Model state saved to ./models/ft_transformer_best_state.pth\n",
      "Model state saved to ./models/ft_transformer_best_ros_state.pth\n",
      "Model state saved to ./models/ft_transformer_best_smote_state.pth\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "n_folds = 10\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "# FTTransformer\n",
    "stratified_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_ros = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_smote = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print('FTTransformer - original data')\n",
    "ft_transformer_best_state, ft_transformer_fold_results_list = find_best_model_state('FTTransformer', X, y, input_dim=X.shape[1], num_classes=3, stratified_fold=stratified_fold, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "print('FTTransformer - ROS data')\n",
    "ft_transformer_best_ros_state, ft_transformer_fold_results_list_ros = find_best_model_state('FTTransformer', X_ros, y_ros, input_dim=X_ros.shape[1], num_classes=3, stratified_fold=stratified_fold_ros, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "print('FTTransformer - SMOTE data')\n",
    "ft_transformer_best_smote_state, ft_transformer_fold_results_list_smote = find_best_model_state('FTTransformer', X_smote, y_smote, input_dim=X_smote.shape[1], num_classes=3, stratified_fold=stratified_fold_smote, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "save_model_state(ft_transformer_best_state, './models/ft_transformer_best_state.pth')\n",
    "save_model_state(ft_transformer_best_ros_state, './models/ft_transformer_best_ros_state.pth')\n",
    "save_model_state(ft_transformer_best_smote_state, './models/ft_transformer_best_smote_state.pth')\n",
    "# Save csv\n",
    "ft_transformer_fold_results_df = pd.DataFrame(ft_transformer_fold_results_list).to_csv('./models/ft_transformer_fold_results.csv', index=False)\n",
    "ft_transformer_fold_results_df_ros = pd.DataFrame(ft_transformer_fold_results_list_ros).to_csv('./models/ft_transformer_fold_results_ros.csv', index=False)\n",
    "ft_transformer_fold_results_df_smote = pd.DataFrame(ft_transformer_fold_results_list_smote).to_csv('./models/ft_transformer_fold_results_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Fold",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "roc_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "attention_weights",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a7611056-5f73-42df-a438-1d868bba66a0",
       "rows": [
        [
         "0",
         "1",
         "MLP",
         "0.7722925457102672",
         "0.7562375564259752",
         "0.7601958971339181",
         "0.7578178584708665",
         "0.905976937263489",
         null
        ],
        [
         "1",
         "2",
         "MLP",
         "0.7630098452883263",
         "0.74633725831325",
         "0.7555399875044055",
         "0.7507039652685714",
         "0.8994683934058368",
         null
        ],
        [
         "2",
         "3",
         "MLP",
         "0.7630098452883263",
         "0.7434964853948433",
         "0.7655724456026519",
         "0.7532235462947168",
         "0.9025752225707723",
         null
        ],
        [
         "3",
         "4",
         "MLP",
         "0.7748241912798874",
         "0.7593978701569913",
         "0.7715365060952063",
         "0.7649809438008702",
         "0.9069063830307141",
         null
        ],
        [
         "4",
         "5",
         "MLP",
         "0.7624472573839662",
         "0.7453299568221511",
         "0.7579605005466036",
         "0.7511716417494553",
         "0.8985704590745766",
         null
        ],
        [
         "5",
         "6",
         "MLP",
         "0.7784810126582279",
         "0.7601821661117806",
         "0.7772956753117982",
         "0.7679328564899587",
         "0.9108676109787193",
         null
        ],
        [
         "6",
         "7",
         "MLP",
         "0.759774964838256",
         "0.7390067714902652",
         "0.7550099345728518",
         "0.746306447061365",
         "0.899783516938049",
         null
        ],
        [
         "7",
         "8",
         "MLP",
         "0.7676185117456745",
         "0.7503648652681361",
         "0.7602415547411883",
         "0.7549801211402155",
         "0.9038009028646429",
         null
        ],
        [
         "8",
         "9",
         "MLP",
         "0.7624138416092278",
         "0.744389579490965",
         "0.7468777497790775",
         "0.7455313578991327",
         "0.9017003145490667",
         null
        ],
        [
         "9",
         "10",
         "MLP",
         "0.7646645097763398",
         "0.7485345701466329",
         "0.7613256651941924",
         "0.7542417034855524",
         "0.9010425365632891",
         null
        ],
        [
         "10",
         "1",
         "MLP",
         "0.8457627118644068",
         "0.8478200928149141",
         "0.8457668955448123",
         "0.8402649724803256",
         "0.9405896248291358",
         null
        ],
        [
         "11",
         "2",
         "MLP",
         "0.8380909901873327",
         "0.8389797754059821",
         "0.8380947599071499",
         "0.8324222251849301",
         "0.9349975383572794",
         null
        ],
        [
         "12",
         "3",
         "MLP",
         "0.8421944692239072",
         "0.8461548839213485",
         "0.8422012393380087",
         "0.8351756191913133",
         "0.936841191570108",
         null
        ],
        [
         "13",
         "4",
         "MLP",
         "0.8570026761819803",
         "0.8602793723623386",
         "0.8570085790626635",
         "0.851574073559704",
         "0.9438086452663907",
         null
        ],
        [
         "14",
         "5",
         "MLP",
         "0.8459411239964317",
         "0.8486209166474924",
         "0.8459458170193429",
         "0.8399918330435089",
         "0.938918567158736",
         null
        ],
        [
         "15",
         "6",
         "MLP",
         "0.8569899188152378",
         "0.8597420928941953",
         "0.8569842023845883",
         "0.8517857989357741",
         "0.9429462617346028",
         null
        ],
        [
         "16",
         "7",
         "MLP",
         "0.8444107413685431",
         "0.8501249745674548",
         "0.8444041174763927",
         "0.8374587547079537",
         "0.9354057618232359",
         null
        ],
        [
         "17",
         "8",
         "MLP",
         "0.8489606566152199",
         "0.8531307754015197",
         "0.848955369812113",
         "0.8430143851503242",
         "0.9403744861954234",
         null
        ],
        [
         "18",
         "9",
         "MLP",
         "0.8487822285663307",
         "0.8511968871799889",
         "0.8487767348411444",
         "0.8430389162941454",
         "0.9374404182614322",
         null
        ],
        [
         "19",
         "10",
         "MLP",
         "0.8502096529574449",
         "0.853328333799817",
         "0.850204167213411",
         "0.844495068514188",
         "0.9393686875582076",
         null
        ],
        [
         "20",
         "1",
         "MLP",
         "0.8456735057983943",
         "0.8446420657946082",
         "0.8456742355177703",
         "0.8435428282097752",
         "0.9479836112689859",
         null
        ],
        [
         "21",
         "2",
         "MLP",
         "0.8390722569134701",
         "0.8391434141616915",
         "0.8390764404873846",
         "0.8364185998842962",
         "0.9459528803347004",
         null
        ],
        [
         "22",
         "3",
         "MLP",
         "0.8324710080285459",
         "0.8312494924130881",
         "0.832472151376258",
         "0.8301991292589502",
         "0.9430626614959695",
         null
        ],
        [
         "23",
         "4",
         "MLP",
         "0.8392506690454951",
         "0.8390862997316271",
         "0.8392515180391239",
         "0.8367767409634977",
         "0.9461329232037768",
         null
        ],
        [
         "24",
         "5",
         "MLP",
         "0.8405887600356824",
         "0.842515515644035",
         "0.840591901079106",
         "0.8368751129761347",
         "0.9469690170297614",
         null
        ],
        [
         "25",
         "6",
         "MLP",
         "0.8477116602729949",
         "0.8469081705784637",
         "0.8477112280936989",
         "0.8460367376180775",
         "0.948341412651133",
         null
        ],
        [
         "26",
         "7",
         "MLP",
         "0.8389686858774199",
         "0.8379439091926151",
         "0.8389675213831934",
         "0.8366034489376458",
         "0.9438308846165828",
         null
        ],
        [
         "27",
         "8",
         "MLP",
         "0.8376304755107503",
         "0.8364902637278494",
         "0.8376302182565035",
         "0.8356893559078039",
         "0.9458187473323866",
         null
        ],
        [
         "28",
         "9",
         "MLP",
         "0.8336158444107413",
         "0.8333373410971175",
         "0.8336135815676596",
         "0.8307351516563505",
         "0.9443525915289527",
         null
        ],
        [
         "29",
         "10",
         "MLP",
         "0.8393255419751985",
         "0.8382353293492275",
         "0.8393242421933031",
         "0.8372414949854067",
         "0.9464758614612473",
         null
        ],
        [
         "30",
         "1",
         "Transformer",
         "0.8036568213783404",
         "0.787590057175311",
         "0.8073980105338098",
         "0.7962897871278308",
         "0.9219625560246852",
         null
        ],
        [
         "31",
         "2",
         "Transformer",
         "0.7980309423347398",
         "0.7851309477251881",
         "0.80223485778012",
         "0.7922330456592297",
         "0.9186711329439768",
         null
        ],
        [
         "32",
         "3",
         "Transformer",
         "0.7933895921237694",
         "0.7777899038715148",
         "0.7924626797513089",
         "0.7843712778811597",
         "0.9189160517200116",
         null
        ],
        [
         "33",
         "4",
         "Transformer",
         "0.7988748241912799",
         "0.7815558755230798",
         "0.8015371765102769",
         "0.7903124182484803",
         "0.9208731839726679",
         null
        ],
        [
         "34",
         "5",
         "Transformer",
         "0.79971870604782",
         "0.7848460467602784",
         "0.7979353324762641",
         "0.7905640326528033",
         "0.9188107207654652",
         null
        ],
        [
         "35",
         "6",
         "Transformer",
         "0.8075949367088607",
         "0.7921634508191557",
         "0.8054250465353879",
         "0.7983480765443346",
         "0.9255182905649404",
         null
        ],
        [
         "36",
         "7",
         "Transformer",
         "0.7880450070323488",
         "0.7734781306422617",
         "0.782409000496921",
         "0.7776952268152235",
         "0.9149232252715503",
         null
        ],
        [
         "37",
         "8",
         "Transformer",
         "0.7929385286256857",
         "0.7823359277245401",
         "0.7824644452274062",
         "0.7822521829550081",
         "0.9182631920505061",
         null
        ],
        [
         "38",
         "9",
         "Transformer",
         "0.7995498663665775",
         "0.7826952322141053",
         "0.8006551310359596",
         "0.7907739523409326",
         "0.920592354256755",
         null
        ],
        [
         "39",
         "10",
         "Transformer",
         "0.7992685328456886",
         "0.7867798615583409",
         "0.7909249854379911",
         "0.7883992544101329",
         "0.9231076375778716",
         null
        ],
        [
         "40",
         "1",
         "Transformer",
         "0.8992863514719001",
         "0.9015255615367465",
         "0.8992910851360615",
         "0.896636773180992",
         "0.9599998769312633",
         null
        ],
        [
         "41",
         "2",
         "Transformer",
         "0.8950044603033006",
         "0.8976254621037917",
         "0.8950091461487141",
         "0.8920854013678444",
         "0.9571416165032277",
         null
        ],
        [
         "42",
         "3",
         "Transformer",
         "0.8938447814451382",
         "0.8971088791051823",
         "0.8938497617340877",
         "0.8906961236719488",
         "0.9558573950942942",
         null
        ],
        [
         "43",
         "4",
         "Transformer",
         "0.8867975022301516",
         "0.8891428958815072",
         "0.8868031588736742",
         "0.8836351452416918",
         "0.9535782791922177",
         null
        ],
        [
         "44",
         "5",
         "Transformer",
         "0.8982158786797502",
         "0.9013364344804381",
         "0.8982208510798415",
         "0.8951746789571436",
         "0.9596442821211109",
         null
        ],
        [
         "45",
         "6",
         "Transformer",
         "0.9067713444553483",
         "0.908798827314802",
         "0.9067674433396231",
         "0.9045158859080468",
         "0.9625085476367792",
         null
        ],
        [
         "46",
         "7",
         "Transformer",
         "0.8960656615219913",
         "0.8980844994812487",
         "0.8960612588546314",
         "0.8933431973905283",
         "0.9553958198015177",
         null
        ],
        [
         "47",
         "8",
         "Transformer",
         "0.9013292889642252",
         "0.9049837663659271",
         "0.9013243292903862",
         "0.8984731530350171",
         "0.9598898257336904",
         null
        ],
        [
         "48",
         "9",
         "Transformer",
         "0.8965117316442145",
         "0.8989978689723742",
         "0.896506748018398",
         "0.8937024743119507",
         "0.9548593216235873",
         null
        ],
        [
         "49",
         "10",
         "Transformer",
         "0.8975822999375502",
         "0.8997422373308618",
         "0.8975778177100077",
         "0.8950141957878947",
         "0.9566108732318961",
         null
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 120
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>attention_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.772293</td>\n",
       "      <td>0.756238</td>\n",
       "      <td>0.760196</td>\n",
       "      <td>0.757818</td>\n",
       "      <td>0.905977</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.763010</td>\n",
       "      <td>0.746337</td>\n",
       "      <td>0.755540</td>\n",
       "      <td>0.750704</td>\n",
       "      <td>0.899468</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.763010</td>\n",
       "      <td>0.743496</td>\n",
       "      <td>0.765572</td>\n",
       "      <td>0.753224</td>\n",
       "      <td>0.902575</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.774824</td>\n",
       "      <td>0.759398</td>\n",
       "      <td>0.771537</td>\n",
       "      <td>0.764981</td>\n",
       "      <td>0.906906</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.762447</td>\n",
       "      <td>0.745330</td>\n",
       "      <td>0.757961</td>\n",
       "      <td>0.751172</td>\n",
       "      <td>0.898570</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6</td>\n",
       "      <td>FTTransformer</td>\n",
       "      <td>0.873851</td>\n",
       "      <td>0.874368</td>\n",
       "      <td>0.873848</td>\n",
       "      <td>0.872224</td>\n",
       "      <td>0.960346</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>7</td>\n",
       "      <td>FTTransformer</td>\n",
       "      <td>0.874565</td>\n",
       "      <td>0.875111</td>\n",
       "      <td>0.874562</td>\n",
       "      <td>0.872525</td>\n",
       "      <td>0.958017</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>8</td>\n",
       "      <td>FTTransformer</td>\n",
       "      <td>0.877777</td>\n",
       "      <td>0.877824</td>\n",
       "      <td>0.877774</td>\n",
       "      <td>0.876277</td>\n",
       "      <td>0.961730</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>9</td>\n",
       "      <td>FTTransformer</td>\n",
       "      <td>0.880364</td>\n",
       "      <td>0.880113</td>\n",
       "      <td>0.880361</td>\n",
       "      <td>0.879117</td>\n",
       "      <td>0.959837</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>10</td>\n",
       "      <td>FTTransformer</td>\n",
       "      <td>0.877688</td>\n",
       "      <td>0.877599</td>\n",
       "      <td>0.877685</td>\n",
       "      <td>0.876110</td>\n",
       "      <td>0.962655</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Fold     Model Name  accuracy  precision    recall  f1_score   roc_auc  \\\n",
       "0       1            MLP  0.772293   0.756238  0.760196  0.757818  0.905977   \n",
       "1       2            MLP  0.763010   0.746337  0.755540  0.750704  0.899468   \n",
       "2       3            MLP  0.763010   0.743496  0.765572  0.753224  0.902575   \n",
       "3       4            MLP  0.774824   0.759398  0.771537  0.764981  0.906906   \n",
       "4       5            MLP  0.762447   0.745330  0.757961  0.751172  0.898570   \n",
       "..    ...            ...       ...        ...       ...       ...       ...   \n",
       "115     6  FTTransformer  0.873851   0.874368  0.873848  0.872224  0.960346   \n",
       "116     7  FTTransformer  0.874565   0.875111  0.874562  0.872525  0.958017   \n",
       "117     8  FTTransformer  0.877777   0.877824  0.877774  0.876277  0.961730   \n",
       "118     9  FTTransformer  0.880364   0.880113  0.880361  0.879117  0.959837   \n",
       "119    10  FTTransformer  0.877688   0.877599  0.877685  0.876110  0.962655   \n",
       "\n",
       "    attention_weights  \n",
       "0                None  \n",
       "1                None  \n",
       "2                None  \n",
       "3                None  \n",
       "4                None  \n",
       "..                ...  \n",
       "115              None  \n",
       "116              None  \n",
       "117              None  \n",
       "118              None  \n",
       "119              None  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_fold_results_df = pd.concat([pd.DataFrame(mlp_fold_results_list), pd.DataFrame(mlp_fold_results_list_ros), pd.DataFrame(mlp_fold_results_list_smote), pd.DataFrame(transformer_fold_results_list), pd.DataFrame(transformer_fold_results_list_ros), pd.DataFrame(transformer_fold_results_list_smote), pd.DataFrame(tab_transformer_fold_results_list), pd.DataFrame(tab_transformer_fold_results_list_ros), pd.DataFrame(tab_transformer_fold_results_list_smote), pd.DataFrame(ft_transformer_fold_results_list), pd.DataFrame(ft_transformer_fold_results_list_ros), pd.DataFrame(ft_transformer_fold_results_list_smote)], ignore_index=True)\n",
    "final_fold_results_df.to_csv('./data/5_final_fold_results.csv', index=False)\n",
    "final_fold_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Model Name  Fold  accuracy  precision    recall  f1_score  \\\n",
      "0               FTTransformer   5.5  0.802129   0.787394  0.806742  0.795623   \n",
      "1      FTTransformer with ROS   5.5  0.891762   0.895721  0.891762  0.888227   \n",
      "2    FTTransformer with SMOTE   5.5  0.876435   0.876573  0.876435  0.874892   \n",
      "3                         MLP   5.5  0.766854   0.749328  0.761156  0.754689   \n",
      "4                MLP with ROS   5.5  0.847835   0.850938  0.847834  0.841922   \n",
      "5              MLP with SMOTE   5.5  0.839431   0.838955  0.839431  0.837012   \n",
      "6              TabTransformer   5.5  0.749595   0.735625  0.730490  0.732660   \n",
      "7     TabTransformer with ROS   5.5  0.883162   0.882723  0.883162  0.881073   \n",
      "8   TabTransformer with SMOTE   5.5  0.847326   0.846277  0.847326  0.846413   \n",
      "9                 Transformer   5.5  0.798107   0.783437  0.796345  0.789124   \n",
      "10       Transformer with ROS   5.5  0.897141   0.899735  0.897141  0.894328   \n",
      "11     Transformer with SMOTE   5.5  0.878835   0.878687  0.878835  0.877380   \n",
      "\n",
      "     roc_auc  attention_weights  \n",
      "0   0.920352                NaN  \n",
      "1   0.954834                NaN  \n",
      "2   0.960731                NaN  \n",
      "3   0.903069                NaN  \n",
      "4   0.939069                NaN  \n",
      "5   0.945892                NaN  \n",
      "6   0.895364                NaN  \n",
      "7   0.957207                NaN  \n",
      "8   0.947924                NaN  \n",
      "9   0.920164                NaN  \n",
      "10  0.957549                NaN  \n",
      "11  0.961757                NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Get average training results\n",
    "final_fold_results_df = pd.read_csv('./data/5_final_fold_results.csv')\n",
    "num_folds = 10\n",
    "model_names = ['MLP'] * num_folds + ['MLP with ROS'] * num_folds + ['MLP with SMOTE'] * num_folds + ['Transformer'] * num_folds + ['Transformer with ROS'] * num_folds + ['Transformer with SMOTE'] * num_folds + ['TabTransformer'] * num_folds + ['TabTransformer with ROS'] * num_folds + ['TabTransformer with SMOTE'] * num_folds + ['FTTransformer'] * num_folds + ['FTTransformer with ROS'] * num_folds + ['FTTransformer with SMOTE'] * num_folds\n",
    "final_fold_results_df['Model Name'] = model_names\n",
    "final_fold_results_df = final_fold_results_df.groupby(['Model Name']).mean().reset_index()\n",
    "print(final_fold_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "mlp_best_model_state = torch.load('./models/mlp_best_state.pth')\n",
    "mlp_best_model = MLP(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "mlp_best_model.load_state_dict(mlp_best_model_state)\n",
    "mlp_best_roc_model_state = torch.load('./models/mlp_best_ros_state.pth')\n",
    "mlp_best_roc_model = MLP(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "mlp_best_roc_model.load_state_dict(mlp_best_roc_model_state)\n",
    "mlp_best_smote_model_state = torch.load('./models/mlp_best_smote_state.pth')\n",
    "mlp_best_smote_model = MLP(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "mlp_best_smote_model.load_state_dict(mlp_best_smote_model_state)\n",
    "transformer_best_model_state = torch.load('./models/transformer_best_state.pth')\n",
    "transformer_best_model = Transformer(input_dim=X.shape[1], num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "transformer_best_model.load_state_dict(transformer_best_model_state)\n",
    "transformer_best_roc_model_state = torch.load('./models/transformer_best_ros_state.pth')\n",
    "transformer_best_roc_model = Transformer(input_dim=X.shape[1], num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "transformer_best_roc_model.load_state_dict(transformer_best_roc_model_state)\n",
    "transformer_best_smote_model_state = torch.load('./models/transformer_best_smote_state.pth')\n",
    "transformer_best_smote_model = Transformer(input_dim=X.shape[1], num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "transformer_best_smote_model.load_state_dict(transformer_best_smote_model_state)\n",
    "ft_transformer_best_model_state = torch.load('./models/ft_transformer_best_state.pth')\n",
    "ft_transformer_best_model = FTTransformerWrapper(cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs, num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "ft_transformer_best_model.load_state_dict(ft_transformer_best_model_state)\n",
    "ft_transformer_best_roc_model_state = torch.load('./models/ft_transformer_best_ros_state.pth')\n",
    "ft_transformer_best_roc_model = FTTransformerWrapper(cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs, num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "ft_transformer_best_roc_model.load_state_dict(ft_transformer_best_roc_model_state)\n",
    "ft_transformer_best_smote_model_state = torch.load('./models/ft_transformer_best_smote_state.pth')\n",
    "ft_transformer_best_smote_model = FTTransformerWrapper(cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs, num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "ft_transformer_best_smote_model.load_state_dict(ft_transformer_best_smote_model_state)\n",
    "tab_transformer_best_model_state = torch.load('./models/tab_transformer_best_state.pth')\n",
    "tab_transformer_best_model = TabTransformerWrapper(cat_dims=cat_dims,cat_idxs=cat_idxs,num_idxs=num_idxs,num_classes=3,num_heads=4,num_layers=2, dim_model=64, dropout=0.1).to(device)\n",
    "tab_transformer_best_model.load_state_dict(tab_transformer_best_model_state)\n",
    "tab_transformer_best_roc_model_state = torch.load('./models/tab_transformer_best_ros_state.pth')\n",
    "tab_transformer_best_roc_model = TabTransformerWrapper(cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs, num_classes=3, num_heads=4, num_layers=2, dim_model=64, dropout=0.1).to(device)\n",
    "tab_transformer_best_roc_model.load_state_dict(tab_transformer_best_roc_model_state)\n",
    "tab_transformer_best_smote_model_state = torch.load('./models/tab_transformer_best_smote_state.pth')\n",
    "tab_transformer_best_smote_model = TabTransformerWrapper(cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs, num_classes=3, num_heads=4, num_layers=2, dim_model=64, dropout=0.1).to(device)\n",
    "tab_transformer_best_smote_model.load_state_dict(tab_transformer_best_smote_model_state)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, device, name='MLP'):\n",
    "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(X_test, y_test),\n",
    "        batch_size=128, num_workers=0\n",
    "    )\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "\n",
    "    results = {\n",
    "        'name': name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([results])\n",
    "\n",
    "# Evaluate best model\n",
    "result_mlp_df = evaluate_model(mlp_best_model, X_test, y_test, device, name='MLP')\n",
    "result_mlp_roc_df = evaluate_model(mlp_best_roc_model, X_test, y_test, device, name='MLP_ROS')\n",
    "result_mlp_smote_df = evaluate_model(mlp_best_smote_model, X_test, y_test, device, name='MLP_SMOTE')\n",
    "result_transformer_df = evaluate_model(transformer_best_model, X_test, y_test, device, name='Transformer')\n",
    "result_transformer_roc_df = evaluate_model(transformer_best_roc_model, X_test, y_test, device, name='Transformer_ROS')\n",
    "result_transformer_smote_df = evaluate_model(transformer_best_smote_model, X_test, y_test, device, name='Transformer_SMOTE')\n",
    "result_ft_transformer_df = evaluate_model(ft_transformer_best_model, X_test, y_test, device, name='FTTransformer')\n",
    "result_ft_transformer_roc_df = evaluate_model(ft_transformer_best_roc_model, X_test, y_test, device, name='FTTransformer_ROS')\n",
    "result_ft_transformer_smote_df = evaluate_model(ft_transformer_best_smote_model, X_test, y_test, device, name='FTTransformer_SMOTE')\n",
    "result_tab_transformer_df = evaluate_model(tab_transformer_best_model, X_test, y_test, device, name='TabTransformer')\n",
    "result_tab_transformer_roc_df = evaluate_model(tab_transformer_best_roc_model, X_test, y_test, device, name='TabTransformer_ROS')\n",
    "result_tab_transformer_smote_df = evaluate_model(tab_transformer_best_smote_model, X_test, y_test, device, name='TabTransformer_SMOTE')\n",
    "\n",
    "# Combine results\n",
    "result_df = pd.concat([\n",
    "    result_mlp_df, \n",
    "    result_mlp_roc_df, \n",
    "    result_mlp_smote_df, \n",
    "    result_transformer_df, \n",
    "    result_transformer_roc_df, \n",
    "    result_transformer_smote_df,\n",
    "    result_tab_transformer_df,\n",
    "    result_tab_transformer_roc_df,\n",
    "    result_tab_transformer_smote_df,\n",
    "    result_ft_transformer_df,\n",
    "    result_ft_transformer_roc_df,\n",
    "    result_ft_transformer_smote_df\n",
    "], ignore_index=True)\n",
    "result_df.to_csv('./data/5_test_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "roc_auc",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "78a75000-adcf-4039-adc1-cd0e08090e87",
       "rows": [
        [
         "0",
         "MLP",
         "0.7682700421940928",
         "0.751039661921313",
         "0.7692312489905838",
         "0.7589198717664285",
         "0.9039418597437701"
        ],
        [
         "1",
         "MLP_ROS",
         "0.7617440225035161",
         "0.7446064020832462",
         "0.8039007785301644",
         "0.7602983562123359",
         "0.9063958820179389"
        ],
        [
         "2",
         "MLP_SMOTE",
         "0.7590998593530239",
         "0.7383375568133239",
         "0.7688561632702168",
         "0.7509445349263345",
         "0.8974711605693511"
        ],
        [
         "3",
         "Transformer",
         "0.7994374120956399",
         "0.7836344773534071",
         "0.8012660114640751",
         "0.7916590414332586",
         "0.9202194243814298"
        ],
        [
         "4",
         "Transformer_ROS",
         "0.8085513361462728",
         "0.790537109834265",
         "0.8296925960856093",
         "0.805809318754894",
         "0.9226234170895627"
        ],
        [
         "5",
         "Transformer_SMOTE",
         "0.809563994374121",
         "0.796752446392689",
         "0.8165135242380964",
         "0.8053116087703573",
         "0.9245632236773736"
        ],
        [
         "6",
         "TabTransformer",
         "0.7538115330520394",
         "0.7418020351456884",
         "0.7344297634872848",
         "0.7379299569729301",
         "0.8982710421530798"
        ],
        [
         "7",
         "TabTransformer_ROS",
         "0.7515049226441631",
         "0.7364873356004554",
         "0.7389473437239119",
         "0.7376691552021626",
         "0.8956663613932273"
        ],
        [
         "8",
         "TabTransformer_SMOTE",
         "0.7531926863572433",
         "0.7350380712034358",
         "0.7529720107707284",
         "0.743140036192577",
         "0.8952288463161336"
        ],
        [
         "9",
         "FTTransformer",
         "0.8066385372714486",
         "0.7915448041628016",
         "0.8169894177405072",
         "0.801723386878702",
         "0.9201352466533899"
        ],
        [
         "10",
         "FTTransformer_ROS",
         "0.7956680731364275",
         "0.7806636364975864",
         "0.825456831466763",
         "0.7953120032429077",
         "0.9186430655746879"
        ],
        [
         "11",
         "FTTransformer_SMOTE",
         "0.8037130801687764",
         "0.7908329880093964",
         "0.8085482840536766",
         "0.7982527949751298",
         "0.9196976070090915"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 12
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.768270</td>\n",
       "      <td>0.751040</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.758920</td>\n",
       "      <td>0.903942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP_ROS</td>\n",
       "      <td>0.761744</td>\n",
       "      <td>0.744606</td>\n",
       "      <td>0.803901</td>\n",
       "      <td>0.760298</td>\n",
       "      <td>0.906396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP_SMOTE</td>\n",
       "      <td>0.759100</td>\n",
       "      <td>0.738338</td>\n",
       "      <td>0.768856</td>\n",
       "      <td>0.750945</td>\n",
       "      <td>0.897471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformer</td>\n",
       "      <td>0.799437</td>\n",
       "      <td>0.783634</td>\n",
       "      <td>0.801266</td>\n",
       "      <td>0.791659</td>\n",
       "      <td>0.920219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformer_ROS</td>\n",
       "      <td>0.808551</td>\n",
       "      <td>0.790537</td>\n",
       "      <td>0.829693</td>\n",
       "      <td>0.805809</td>\n",
       "      <td>0.922623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformer_SMOTE</td>\n",
       "      <td>0.809564</td>\n",
       "      <td>0.796752</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.805312</td>\n",
       "      <td>0.924563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.753812</td>\n",
       "      <td>0.741802</td>\n",
       "      <td>0.734430</td>\n",
       "      <td>0.737930</td>\n",
       "      <td>0.898271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TabTransformer_ROS</td>\n",
       "      <td>0.751505</td>\n",
       "      <td>0.736487</td>\n",
       "      <td>0.738947</td>\n",
       "      <td>0.737669</td>\n",
       "      <td>0.895666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TabTransformer_SMOTE</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0.735038</td>\n",
       "      <td>0.752972</td>\n",
       "      <td>0.743140</td>\n",
       "      <td>0.895229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FTTransformer</td>\n",
       "      <td>0.806639</td>\n",
       "      <td>0.791545</td>\n",
       "      <td>0.816989</td>\n",
       "      <td>0.801723</td>\n",
       "      <td>0.920135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FTTransformer_ROS</td>\n",
       "      <td>0.795668</td>\n",
       "      <td>0.780664</td>\n",
       "      <td>0.825457</td>\n",
       "      <td>0.795312</td>\n",
       "      <td>0.918643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FTTransformer_SMOTE</td>\n",
       "      <td>0.803713</td>\n",
       "      <td>0.790833</td>\n",
       "      <td>0.808548</td>\n",
       "      <td>0.798253</td>\n",
       "      <td>0.919698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  accuracy  precision    recall  f1_score   roc_auc\n",
       "0                    MLP  0.768270   0.751040  0.769231  0.758920  0.903942\n",
       "1                MLP_ROS  0.761744   0.744606  0.803901  0.760298  0.906396\n",
       "2              MLP_SMOTE  0.759100   0.738338  0.768856  0.750945  0.897471\n",
       "3            Transformer  0.799437   0.783634  0.801266  0.791659  0.920219\n",
       "4        Transformer_ROS  0.808551   0.790537  0.829693  0.805809  0.922623\n",
       "5      Transformer_SMOTE  0.809564   0.796752  0.816514  0.805312  0.924563\n",
       "6         TabTransformer  0.753812   0.741802  0.734430  0.737930  0.898271\n",
       "7     TabTransformer_ROS  0.751505   0.736487  0.738947  0.737669  0.895666\n",
       "8   TabTransformer_SMOTE  0.753193   0.735038  0.752972  0.743140  0.895229\n",
       "9          FTTransformer  0.806639   0.791545  0.816989  0.801723  0.920135\n",
       "10     FTTransformer_ROS  0.795668   0.780664  0.825457  0.795312  0.918643\n",
       "11   FTTransformer_SMOTE  0.803713   0.790833  0.808548  0.798253  0.919698"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name  accuracy  precision    recall  f1_score   roc_auc\n",
      "0                    MLP  0.768270   0.751040  0.769231  0.758920  0.903942\n",
      "1                MLP_ROS  0.761744   0.744606  0.803901  0.760298  0.906396\n",
      "2              MLP_SMOTE  0.759100   0.738338  0.768856  0.750945  0.897471\n",
      "3            Transformer  0.799437   0.783634  0.801266  0.791659  0.920219\n",
      "4        Transformer_ROS  0.808551   0.790537  0.829693  0.805809  0.922623\n",
      "5      Transformer_SMOTE  0.809564   0.796752  0.816514  0.805312  0.924563\n",
      "6         TabTransformer  0.753812   0.741802  0.734430  0.737930  0.898271\n",
      "7     TabTransformer_ROS  0.751505   0.736487  0.738947  0.737669  0.895666\n",
      "8   TabTransformer_SMOTE  0.753193   0.735038  0.752972  0.743140  0.895229\n",
      "9          FTTransformer  0.806639   0.791545  0.816989  0.801723  0.920135\n",
      "10     FTTransformer_ROS  0.795668   0.780664  0.825457  0.795312  0.918643\n",
      "11   FTTransformer_SMOTE  0.803713   0.790833  0.808548  0.798253  0.919698\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.read_csv('./data/5_test_result.csv')\n",
    "print(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
