{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from models.mlp import MLP\n",
    "from models.tab_transformer import TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model_state(model_name, X, y, input_dim, num_classes, stratified_fold, batch_size=64, num_epochs=200, n_folds=5, device='cuda', metric='f1_score'):\n",
    "    \"\"\"Get best model out of Cross-Validation, using the specified metric.\"\"\"\n",
    "    best_model_state = None\n",
    "    best_metric = -np.inf\n",
    "    fold_results_list = []\n",
    "    for fold, (train_ids, val_ids) in enumerate(stratified_fold.split(X, y)):\n",
    "        print(f'Fold {fold + 1}/{n_folds}')\n",
    "        # Prepare data loaders\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X[train_ids], y[train_ids]),\n",
    "            batch_size=batch_size, num_workers=8, shuffle=True\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X[val_ids], y[val_ids]),\n",
    "            batch_size=batch_size, num_workers=8, \n",
    "        )\n",
    "        # Initialize model, criterion, and optimizer\n",
    "        if model_name == 'MLP':\n",
    "            model = MLP(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "        else:\n",
    "            model = TabTransformer(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # Train model\n",
    "        model_state = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=num_epochs)\n",
    "        model.load_state_dict(model_state)\n",
    "        # Evaluate model\n",
    "        result_df = evaluate_model(model, val_loader, device)\n",
    "        metric_result = result_df[metric].values[0]\n",
    "\n",
    "        fold_result = {\n",
    "            'Model Name': model_name,\n",
    "            'Fold': fold + 1,\n",
    "            'Best Model': '',\n",
    "            'Best f1_score': ''\n",
    "        }\n",
    "        fold_result.update(result_df.to_dict(orient='records')[0])\n",
    "        fold_results_list.append(fold_result)\n",
    "\n",
    "        if metric_result > best_metric:\n",
    "            best_metric = metric_result\n",
    "            best_model_state = model.state_dict()\n",
    "        print(f'Best {metric} for fold {fold + 1}: {best_metric:.4f}')\n",
    "        \n",
    "    return best_model_state, fold_results_list\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=500, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_attentions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "            # Get attention weights\n",
    "            if hasattr(model, 'get_attention_weights') and callable(model.get_attention_weights):\n",
    "                try:\n",
    "                    attention_weights = model.get_attention_weights(batch_x)\n",
    "                    if attention_weights is not None:  # Make sure it's not None\n",
    "                        all_attentions.append(attention_weights.cpu().numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting attention weights: {e}\")\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'attention_weights': np.concatenate(all_attentions, axis=0) if len(all_attentions) > 0 else None\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([results])\n",
    "\n",
    "def save_model_state(model_state, output_path):\n",
    "    torch.save(model_state, output_path)\n",
    "    print(f'Model state saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System settings\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('./data/3_train_processed.csv')\n",
    "train_ros_df = pd.read_csv('./data/3_train_ros_processed.csv')\n",
    "train_smote_df = pd.read_csv('./data/3_train_smote_processed.csv')\n",
    "test_df = pd.read_csv('./data/3_test_processed.csv')\n",
    "\n",
    "# Preprocess data\n",
    "train_features = train_df.drop(['credit_score'], axis=1)\n",
    "train_ros_features = train_ros_df.drop(['credit_score'], axis=1)\n",
    "train_smote_features = train_smote_df.drop(['credit_score'], axis=1)\n",
    "test_features = test_df.drop(['credit_score'], axis=1)\n",
    "\n",
    "train_labels = train_df['credit_score']\n",
    "train_ros_labels = train_ros_df['credit_score']\n",
    "train_smote_labels = train_smote_df['credit_score']\n",
    "test_labels = test_df['credit_score']\n",
    "\n",
    "X = torch.FloatTensor(train_features.values)\n",
    "X_ros = torch.FloatTensor(train_ros_features.values)\n",
    "X_smote = torch.FloatTensor(train_smote_features.values)\n",
    "X_test = torch.FloatTensor(test_features.values)\n",
    "y = torch.LongTensor(train_labels.values)\n",
    "y_ros = torch.LongTensor(train_ros_labels.values)\n",
    "y_smote = torch.LongTensor(train_smote_labels.values)\n",
    "y_test = torch.LongTensor(test_labels.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - original data\n",
      "Fold 1/5\n",
      "Epoch [1/500], Train Loss: 0.7486, Val Loss: 0.6931\n",
      "Epoch [10/500], Train Loss: 0.6187, Val Loss: 0.6359\n",
      "Epoch [20/500], Train Loss: 0.5792, Val Loss: 0.6234\n",
      "Epoch [30/500], Train Loss: 0.5467, Val Loss: 0.6114\n",
      "Epoch [40/500], Train Loss: 0.5260, Val Loss: 0.5994\n",
      "Epoch [50/500], Train Loss: 0.5088, Val Loss: 0.5872\n",
      "Epoch [60/500], Train Loss: 0.4939, Val Loss: 0.5787\n",
      "Epoch [70/500], Train Loss: 0.4857, Val Loss: 0.5756\n",
      "Epoch [80/500], Train Loss: 0.4764, Val Loss: 0.5713\n",
      "Epoch [90/500], Train Loss: 0.4667, Val Loss: 0.5677\n",
      "Early stopping at epoch 96\n",
      "Best f1_score for fold 1: 0.7439\n",
      "Fold 2/5\n",
      "Epoch [1/500], Train Loss: 0.7481, Val Loss: 0.7000\n",
      "Epoch [10/500], Train Loss: 0.6193, Val Loss: 0.6425\n",
      "Epoch [20/500], Train Loss: 0.5811, Val Loss: 0.6342\n",
      "Epoch [30/500], Train Loss: 0.5517, Val Loss: 0.6103\n",
      "Epoch [40/500], Train Loss: 0.5264, Val Loss: 0.6015\n",
      "Epoch [50/500], Train Loss: 0.5108, Val Loss: 0.5991\n",
      "Epoch [60/500], Train Loss: 0.4953, Val Loss: 0.5859\n",
      "Early stopping at epoch 69\n",
      "Best f1_score for fold 2: 0.7439\n",
      "Fold 3/5\n",
      "Epoch [1/500], Train Loss: 0.7510, Val Loss: 0.6976\n",
      "Epoch [10/500], Train Loss: 0.6182, Val Loss: 0.6331\n",
      "Epoch [20/500], Train Loss: 0.5801, Val Loss: 0.6186\n",
      "Epoch [30/500], Train Loss: 0.5487, Val Loss: 0.6097\n",
      "Epoch [40/500], Train Loss: 0.5264, Val Loss: 0.5963\n",
      "Epoch [50/500], Train Loss: 0.5090, Val Loss: 0.5966\n",
      "Epoch [60/500], Train Loss: 0.4952, Val Loss: 0.5914\n",
      "Epoch [70/500], Train Loss: 0.4838, Val Loss: 0.5870\n",
      "Epoch [80/500], Train Loss: 0.4799, Val Loss: 0.5918\n",
      "Early stopping at epoch 81\n",
      "Best f1_score for fold 3: 0.7439\n",
      "Fold 4/5\n",
      "Epoch [1/500], Train Loss: 0.7564, Val Loss: 0.6914\n",
      "Epoch [10/500], Train Loss: 0.6188, Val Loss: 0.6293\n",
      "Epoch [20/500], Train Loss: 0.5757, Val Loss: 0.6131\n",
      "Epoch [30/500], Train Loss: 0.5427, Val Loss: 0.6003\n",
      "Epoch [40/500], Train Loss: 0.5253, Val Loss: 0.5916\n",
      "Epoch [50/500], Train Loss: 0.5072, Val Loss: 0.5823\n",
      "Epoch [60/500], Train Loss: 0.4939, Val Loss: 0.5815\n",
      "Epoch [70/500], Train Loss: 0.4821, Val Loss: 0.5721\n",
      "Epoch [80/500], Train Loss: 0.4760, Val Loss: 0.5724\n",
      "Epoch [90/500], Train Loss: 0.4712, Val Loss: 0.5709\n",
      "Epoch [100/500], Train Loss: 0.4637, Val Loss: 0.5659\n",
      "Epoch [110/500], Train Loss: 0.4570, Val Loss: 0.5664\n",
      "Early stopping at epoch 111\n",
      "Best f1_score for fold 4: 0.7455\n",
      "Fold 5/5\n",
      "Epoch [1/500], Train Loss: 0.7497, Val Loss: 0.7000\n",
      "Epoch [10/500], Train Loss: 0.6172, Val Loss: 0.6341\n",
      "Epoch [20/500], Train Loss: 0.5771, Val Loss: 0.6222\n",
      "Epoch [30/500], Train Loss: 0.5452, Val Loss: 0.6107\n",
      "Epoch [40/500], Train Loss: 0.5274, Val Loss: 0.5993\n",
      "Epoch [50/500], Train Loss: 0.5061, Val Loss: 0.5949\n",
      "Epoch [60/500], Train Loss: 0.4973, Val Loss: 0.5866\n",
      "Epoch [70/500], Train Loss: 0.4817, Val Loss: 0.5842\n",
      "Epoch [80/500], Train Loss: 0.4753, Val Loss: 0.5746\n",
      "Epoch [90/500], Train Loss: 0.4661, Val Loss: 0.5728\n",
      "Epoch [100/500], Train Loss: 0.4601, Val Loss: 0.5722\n",
      "Epoch [110/500], Train Loss: 0.4549, Val Loss: 0.5754\n",
      "Early stopping at epoch 118\n",
      "Best f1_score for fold 5: 0.7455\n",
      "MLP - ROS data\n",
      "Fold 1/5\n",
      "Epoch [1/500], Train Loss: 0.7357, Val Loss: 0.6780\n",
      "Epoch [10/500], Train Loss: 0.5800, Val Loss: 0.5843\n",
      "Epoch [20/500], Train Loss: 0.5213, Val Loss: 0.5312\n",
      "Epoch [30/500], Train Loss: 0.4844, Val Loss: 0.4941\n",
      "Epoch [40/500], Train Loss: 0.4629, Val Loss: 0.4695\n",
      "Epoch [50/500], Train Loss: 0.4488, Val Loss: 0.4499\n",
      "Epoch [60/500], Train Loss: 0.4357, Val Loss: 0.4421\n",
      "Epoch [70/500], Train Loss: 0.4298, Val Loss: 0.4329\n",
      "Epoch [80/500], Train Loss: 0.4204, Val Loss: 0.4263\n",
      "Epoch [90/500], Train Loss: 0.4174, Val Loss: 0.4238\n",
      "Epoch [100/500], Train Loss: 0.4121, Val Loss: 0.4242\n",
      "Epoch [110/500], Train Loss: 0.4098, Val Loss: 0.4173\n",
      "Epoch [120/500], Train Loss: 0.4055, Val Loss: 0.4155\n",
      "Epoch [130/500], Train Loss: 0.4016, Val Loss: 0.4089\n",
      "Epoch [140/500], Train Loss: 0.3993, Val Loss: 0.4087\n",
      "Epoch [150/500], Train Loss: 0.3973, Val Loss: 0.4055\n",
      "Epoch [160/500], Train Loss: 0.3972, Val Loss: 0.4083\n",
      "Epoch [170/500], Train Loss: 0.3921, Val Loss: 0.4043\n",
      "Early stopping at epoch 172\n",
      "Best f1_score for fold 1: 0.8426\n",
      "Fold 2/5\n",
      "Epoch [1/500], Train Loss: 0.7408, Val Loss: 0.6792\n",
      "Epoch [10/500], Train Loss: 0.5790, Val Loss: 0.5803\n",
      "Epoch [20/500], Train Loss: 0.5210, Val Loss: 0.5213\n",
      "Epoch [30/500], Train Loss: 0.4852, Val Loss: 0.4842\n",
      "Epoch [40/500], Train Loss: 0.4605, Val Loss: 0.4628\n",
      "Epoch [50/500], Train Loss: 0.4446, Val Loss: 0.4487\n",
      "Epoch [60/500], Train Loss: 0.4329, Val Loss: 0.4368\n",
      "Epoch [70/500], Train Loss: 0.4259, Val Loss: 0.4259\n",
      "Epoch [80/500], Train Loss: 0.4180, Val Loss: 0.4174\n",
      "Epoch [90/500], Train Loss: 0.4126, Val Loss: 0.4160\n",
      "Epoch [100/500], Train Loss: 0.4079, Val Loss: 0.4148\n",
      "Epoch [110/500], Train Loss: 0.4047, Val Loss: 0.4101\n",
      "Epoch [120/500], Train Loss: 0.4035, Val Loss: 0.4057\n",
      "Early stopping at epoch 128\n",
      "Best f1_score for fold 2: 0.8426\n",
      "Fold 3/5\n",
      "Epoch [1/500], Train Loss: 0.7392, Val Loss: 0.6770\n",
      "Epoch [10/500], Train Loss: 0.5822, Val Loss: 0.5882\n",
      "Epoch [20/500], Train Loss: 0.5227, Val Loss: 0.5290\n",
      "Epoch [30/500], Train Loss: 0.4893, Val Loss: 0.4931\n",
      "Epoch [40/500], Train Loss: 0.4652, Val Loss: 0.4702\n",
      "Epoch [50/500], Train Loss: 0.4538, Val Loss: 0.4548\n",
      "Epoch [60/500], Train Loss: 0.4429, Val Loss: 0.4448\n",
      "Epoch [70/500], Train Loss: 0.4345, Val Loss: 0.4379\n",
      "Epoch [80/500], Train Loss: 0.4252, Val Loss: 0.4306\n",
      "Epoch [90/500], Train Loss: 0.4233, Val Loss: 0.4284\n",
      "Epoch [100/500], Train Loss: 0.4188, Val Loss: 0.4230\n",
      "Epoch [110/500], Train Loss: 0.4143, Val Loss: 0.4164\n",
      "Epoch [120/500], Train Loss: 0.4105, Val Loss: 0.4123\n",
      "Epoch [130/500], Train Loss: 0.4072, Val Loss: 0.4096\n",
      "Epoch [140/500], Train Loss: 0.4062, Val Loss: 0.4130\n",
      "Epoch [150/500], Train Loss: 0.4017, Val Loss: 0.4069\n",
      "Epoch [160/500], Train Loss: 0.4002, Val Loss: 0.4039\n",
      "Epoch [170/500], Train Loss: 0.3996, Val Loss: 0.4028\n",
      "Early stopping at epoch 178\n",
      "Best f1_score for fold 3: 0.8426\n",
      "Fold 4/5\n",
      "Epoch [1/500], Train Loss: 0.7413, Val Loss: 0.6727\n",
      "Epoch [10/500], Train Loss: 0.5770, Val Loss: 0.5785\n",
      "Epoch [20/500], Train Loss: 0.5180, Val Loss: 0.5200\n",
      "Epoch [30/500], Train Loss: 0.4825, Val Loss: 0.4827\n",
      "Epoch [40/500], Train Loss: 0.4602, Val Loss: 0.4607\n",
      "Epoch [50/500], Train Loss: 0.4470, Val Loss: 0.4470\n",
      "Epoch [60/500], Train Loss: 0.4368, Val Loss: 0.4399\n",
      "Epoch [70/500], Train Loss: 0.4289, Val Loss: 0.4300\n",
      "Epoch [80/500], Train Loss: 0.4210, Val Loss: 0.4257\n",
      "Epoch [90/500], Train Loss: 0.4138, Val Loss: 0.4201\n",
      "Epoch [100/500], Train Loss: 0.4116, Val Loss: 0.4122\n",
      "Epoch [110/500], Train Loss: 0.4078, Val Loss: 0.4113\n",
      "Epoch [120/500], Train Loss: 0.4061, Val Loss: 0.4079\n",
      "Epoch [130/500], Train Loss: 0.4028, Val Loss: 0.4073\n",
      "Early stopping at epoch 133\n",
      "Best f1_score for fold 4: 0.8426\n",
      "Fold 5/5\n",
      "Epoch [1/500], Train Loss: 0.7396, Val Loss: 0.6925\n",
      "Epoch [10/500], Train Loss: 0.5760, Val Loss: 0.5770\n",
      "Epoch [20/500], Train Loss: 0.5178, Val Loss: 0.5251\n",
      "Epoch [30/500], Train Loss: 0.4815, Val Loss: 0.4865\n",
      "Epoch [40/500], Train Loss: 0.4614, Val Loss: 0.4625\n",
      "Epoch [50/500], Train Loss: 0.4457, Val Loss: 0.4503\n",
      "Epoch [60/500], Train Loss: 0.4347, Val Loss: 0.4417\n",
      "Epoch [70/500], Train Loss: 0.4261, Val Loss: 0.4322\n",
      "Epoch [80/500], Train Loss: 0.4160, Val Loss: 0.4299\n",
      "Epoch [90/500], Train Loss: 0.4145, Val Loss: 0.4203\n",
      "Epoch [100/500], Train Loss: 0.4087, Val Loss: 0.4202\n",
      "Epoch [110/500], Train Loss: 0.4062, Val Loss: 0.4160\n",
      "Epoch [120/500], Train Loss: 0.4054, Val Loss: 0.4154\n",
      "Epoch [130/500], Train Loss: 0.3994, Val Loss: 0.4094\n",
      "Epoch [140/500], Train Loss: 0.3988, Val Loss: 0.4100\n",
      "Early stopping at epoch 141\n",
      "Best f1_score for fold 5: 0.8426\n",
      "MLP - SMOTE data\n",
      "Fold 1/5\n",
      "Epoch [1/500], Train Loss: 0.7011, Val Loss: 0.6342\n",
      "Epoch [10/500], Train Loss: 0.5336, Val Loss: 0.5406\n",
      "Epoch [20/500], Train Loss: 0.4875, Val Loss: 0.5003\n",
      "Epoch [30/500], Train Loss: 0.4595, Val Loss: 0.4776\n",
      "Epoch [40/500], Train Loss: 0.4395, Val Loss: 0.4671\n",
      "Epoch [50/500], Train Loss: 0.4277, Val Loss: 0.4532\n",
      "Epoch [60/500], Train Loss: 0.4157, Val Loss: 0.4439\n",
      "Epoch [70/500], Train Loss: 0.4087, Val Loss: 0.4395\n",
      "Epoch [80/500], Train Loss: 0.3996, Val Loss: 0.4336\n",
      "Epoch [90/500], Train Loss: 0.3953, Val Loss: 0.4290\n",
      "Epoch [100/500], Train Loss: 0.3921, Val Loss: 0.4261\n",
      "Epoch [110/500], Train Loss: 0.3877, Val Loss: 0.4243\n",
      "Epoch [120/500], Train Loss: 0.3876, Val Loss: 0.4234\n",
      "Epoch [130/500], Train Loss: 0.3799, Val Loss: 0.4235\n",
      "Early stopping at epoch 134\n",
      "Best f1_score for fold 1: 0.8347\n",
      "Fold 2/5\n",
      "Epoch [1/500], Train Loss: 0.7004, Val Loss: 0.6353\n",
      "Epoch [10/500], Train Loss: 0.5343, Val Loss: 0.5396\n",
      "Epoch [20/500], Train Loss: 0.4888, Val Loss: 0.5012\n",
      "Epoch [30/500], Train Loss: 0.4573, Val Loss: 0.4826\n",
      "Epoch [40/500], Train Loss: 0.4388, Val Loss: 0.4632\n",
      "Epoch [50/500], Train Loss: 0.4242, Val Loss: 0.4534\n",
      "Epoch [60/500], Train Loss: 0.4134, Val Loss: 0.4464\n",
      "Epoch [70/500], Train Loss: 0.4043, Val Loss: 0.4379\n",
      "Epoch [80/500], Train Loss: 0.3979, Val Loss: 0.4323\n",
      "Epoch [90/500], Train Loss: 0.3930, Val Loss: 0.4318\n",
      "Epoch [100/500], Train Loss: 0.3893, Val Loss: 0.4290\n",
      "Epoch [110/500], Train Loss: 0.3840, Val Loss: 0.4288\n",
      "Early stopping at epoch 117\n",
      "Best f1_score for fold 2: 0.8347\n",
      "Fold 3/5\n",
      "Epoch [1/500], Train Loss: 0.6991, Val Loss: 0.6308\n",
      "Epoch [10/500], Train Loss: 0.5357, Val Loss: 0.5391\n",
      "Epoch [20/500], Train Loss: 0.4919, Val Loss: 0.5012\n",
      "Epoch [30/500], Train Loss: 0.4599, Val Loss: 0.4788\n",
      "Epoch [40/500], Train Loss: 0.4391, Val Loss: 0.4613\n",
      "Epoch [50/500], Train Loss: 0.4246, Val Loss: 0.4494\n",
      "Epoch [60/500], Train Loss: 0.4122, Val Loss: 0.4399\n",
      "Epoch [70/500], Train Loss: 0.4072, Val Loss: 0.4324\n",
      "Epoch [80/500], Train Loss: 0.3999, Val Loss: 0.4326\n",
      "Early stopping at epoch 84\n",
      "Best f1_score for fold 3: 0.8347\n",
      "Fold 4/5\n",
      "Epoch [1/500], Train Loss: 0.7012, Val Loss: 0.6281\n",
      "Epoch [10/500], Train Loss: 0.5342, Val Loss: 0.5392\n",
      "Epoch [20/500], Train Loss: 0.4909, Val Loss: 0.5037\n",
      "Epoch [30/500], Train Loss: 0.4628, Val Loss: 0.4804\n",
      "Epoch [40/500], Train Loss: 0.4427, Val Loss: 0.4670\n",
      "Epoch [50/500], Train Loss: 0.4294, Val Loss: 0.4521\n",
      "Epoch [60/500], Train Loss: 0.4183, Val Loss: 0.4443\n",
      "Epoch [70/500], Train Loss: 0.4103, Val Loss: 0.4397\n",
      "Epoch [80/500], Train Loss: 0.4053, Val Loss: 0.4335\n",
      "Epoch [90/500], Train Loss: 0.4013, Val Loss: 0.4314\n",
      "Epoch [100/500], Train Loss: 0.3944, Val Loss: 0.4313\n",
      "Epoch [110/500], Train Loss: 0.3909, Val Loss: 0.4268\n",
      "Epoch [120/500], Train Loss: 0.3861, Val Loss: 0.4260\n",
      "Early stopping at epoch 122\n",
      "Best f1_score for fold 4: 0.8347\n",
      "Fold 5/5\n",
      "Epoch [1/500], Train Loss: 0.7066, Val Loss: 0.6371\n",
      "Epoch [10/500], Train Loss: 0.5316, Val Loss: 0.5371\n",
      "Epoch [20/500], Train Loss: 0.4841, Val Loss: 0.5047\n",
      "Epoch [30/500], Train Loss: 0.4535, Val Loss: 0.4786\n",
      "Epoch [40/500], Train Loss: 0.4341, Val Loss: 0.4628\n",
      "Epoch [50/500], Train Loss: 0.4220, Val Loss: 0.4553\n",
      "Epoch [60/500], Train Loss: 0.4110, Val Loss: 0.4501\n",
      "Epoch [70/500], Train Loss: 0.4043, Val Loss: 0.4404\n",
      "Epoch [80/500], Train Loss: 0.3993, Val Loss: 0.4373\n",
      "Epoch [90/500], Train Loss: 0.3951, Val Loss: 0.4337\n",
      "Epoch [100/500], Train Loss: 0.3881, Val Loss: 0.4313\n",
      "Epoch [110/500], Train Loss: 0.3865, Val Loss: 0.4311\n",
      "Epoch [120/500], Train Loss: 0.3842, Val Loss: 0.4258\n",
      "Epoch [130/500], Train Loss: 0.3807, Val Loss: 0.4274\n",
      "Epoch [140/500], Train Loss: 0.3777, Val Loss: 0.4252\n",
      "Epoch [150/500], Train Loss: 0.3754, Val Loss: 0.4241\n",
      "Early stopping at epoch 150\n",
      "Best f1_score for fold 5: 0.8347\n",
      "Model state saved to ./models/mlp_best_state.pth\n",
      "Model state saved to ./models/mlp_best_ros_state.pth\n",
      "Model state saved to ./models/mlp_best_smote_state.pth\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "n_folds = 5\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "# MLP\n",
    "stratified_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_ros = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_smote = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "print('MLP - original data')\n",
    "mlp_best_state, mlp_fold_results_list = find_best_model_state('MLP', X, y, input_dim=X.shape[1], num_classes=3, stratified_fold=stratified_fold, batch_size=batch_size, num_epochs=num_epochs, device=device)\n",
    "print('MLP - ROS data')\n",
    "mlp_best_ros_state, mlp_fold_results_list_ros = find_best_model_state('MLP', X_ros, y_ros, input_dim=X_ros.shape[1], num_classes=3, stratified_fold=stratified_fold_ros, batch_size=batch_size, num_epochs=num_epochs, device=device)\n",
    "print('MLP - SMOTE data')\n",
    "mlp_best_smote_state, mlp_fold_results_list_smote = find_best_model_state('MLP', X_smote, y_smote, input_dim=X_smote.shape[1], num_classes=3, stratified_fold=stratified_fold_smote, batch_size=batch_size, num_epochs=num_epochs, device=device)\n",
    "save_model_state(mlp_best_state, './models/mlp_best_state.pth')\n",
    "save_model_state(mlp_best_ros_state, './models/mlp_best_ros_state.pth')\n",
    "save_model_state(mlp_best_smote_state, './models/mlp_best_smote_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabTransformer - original data\n",
      "Fold 1/5\n",
      "Epoch [1/500], Train Loss: 0.7095, Val Loss: 0.6770\n",
      "Epoch [10/500], Train Loss: 0.6271, Val Loss: 0.6383\n",
      "Epoch [20/500], Train Loss: 0.5801, Val Loss: 0.6056\n",
      "Epoch [30/500], Train Loss: 0.5305, Val Loss: 0.5772\n",
      "Epoch [40/500], Train Loss: 0.4928, Val Loss: 0.5675\n",
      "Epoch [50/500], Train Loss: 0.4620, Val Loss: 0.5409\n",
      "Epoch [60/500], Train Loss: 0.4427, Val Loss: 0.5393\n",
      "Epoch [70/500], Train Loss: 0.4264, Val Loss: 0.5372\n",
      "Epoch [80/500], Train Loss: 0.4117, Val Loss: 0.5233\n",
      "Epoch [90/500], Train Loss: 0.4035, Val Loss: 0.5186\n",
      "Early stopping at epoch 98\n",
      "Best f1_score for fold 1: 0.7829\n",
      "Fold 2/5\n",
      "Epoch [1/500], Train Loss: 0.7089, Val Loss: 0.6743\n",
      "Epoch [10/500], Train Loss: 0.6283, Val Loss: 0.6436\n",
      "Epoch [20/500], Train Loss: 0.5839, Val Loss: 0.6217\n",
      "Epoch [30/500], Train Loss: 0.5299, Val Loss: 0.5897\n",
      "Epoch [40/500], Train Loss: 0.4897, Val Loss: 0.5734\n",
      "Epoch [50/500], Train Loss: 0.4590, Val Loss: 0.5841\n",
      "Epoch [60/500], Train Loss: 0.4368, Val Loss: 0.5415\n",
      "Epoch [70/500], Train Loss: 0.4232, Val Loss: 0.5471\n",
      "Early stopping at epoch 74\n",
      "Best f1_score for fold 2: 0.7829\n",
      "Fold 3/5\n",
      "Epoch [1/500], Train Loss: 0.7105, Val Loss: 0.6779\n",
      "Epoch [10/500], Train Loss: 0.6308, Val Loss: 0.6384\n",
      "Epoch [20/500], Train Loss: 0.5822, Val Loss: 0.6087\n",
      "Epoch [30/500], Train Loss: 0.5297, Val Loss: 0.5863\n",
      "Epoch [40/500], Train Loss: 0.4919, Val Loss: 0.5709\n",
      "Epoch [50/500], Train Loss: 0.4638, Val Loss: 0.5550\n",
      "Epoch [60/500], Train Loss: 0.4457, Val Loss: 0.5606\n",
      "Early stopping at epoch 66\n",
      "Best f1_score for fold 3: 0.7829\n",
      "Fold 4/5\n",
      "Epoch [1/500], Train Loss: 0.7146, Val Loss: 0.6787\n",
      "Epoch [10/500], Train Loss: 0.6301, Val Loss: 0.6417\n",
      "Epoch [20/500], Train Loss: 0.5794, Val Loss: 0.6169\n",
      "Epoch [30/500], Train Loss: 0.5290, Val Loss: 0.5815\n",
      "Epoch [40/500], Train Loss: 0.4852, Val Loss: 0.5561\n",
      "Epoch [50/500], Train Loss: 0.4595, Val Loss: 0.5498\n",
      "Epoch [60/500], Train Loss: 0.4415, Val Loss: 0.5372\n",
      "Epoch [70/500], Train Loss: 0.4276, Val Loss: 0.5446\n",
      "Epoch [80/500], Train Loss: 0.4179, Val Loss: 0.5318\n",
      "Early stopping at epoch 82\n",
      "Best f1_score for fold 4: 0.7829\n",
      "Fold 5/5\n",
      "Epoch [1/500], Train Loss: 0.7108, Val Loss: 0.6714\n",
      "Epoch [10/500], Train Loss: 0.6299, Val Loss: 0.6355\n",
      "Epoch [20/500], Train Loss: 0.5865, Val Loss: 0.6115\n",
      "Epoch [30/500], Train Loss: 0.5332, Val Loss: 0.5908\n",
      "Epoch [40/500], Train Loss: 0.4947, Val Loss: 0.5788\n",
      "Epoch [50/500], Train Loss: 0.4667, Val Loss: 0.5611\n",
      "Epoch [60/500], Train Loss: 0.4469, Val Loss: 0.5586\n",
      "Epoch [70/500], Train Loss: 0.4284, Val Loss: 0.5551\n",
      "Early stopping at epoch 72\n",
      "Best f1_score for fold 5: 0.7829\n",
      "TabTransformer - ROS data\n",
      "Fold 1/5\n",
      "Epoch [1/500], Train Loss: 0.7025, Val Loss: 0.6635\n",
      "Epoch [10/500], Train Loss: 0.5700, Val Loss: 0.5755\n",
      "Epoch [20/500], Train Loss: 0.4685, Val Loss: 0.4797\n",
      "Epoch [30/500], Train Loss: 0.4191, Val Loss: 0.4351\n",
      "Epoch [40/500], Train Loss: 0.3929, Val Loss: 0.4145\n",
      "Epoch [50/500], Train Loss: 0.3756, Val Loss: 0.3948\n",
      "Epoch [60/500], Train Loss: 0.3648, Val Loss: 0.3815\n",
      "Epoch [70/500], Train Loss: 0.3549, Val Loss: 0.3738\n",
      "Epoch [80/500], Train Loss: 0.3462, Val Loss: 0.3683\n",
      "Epoch [90/500], Train Loss: 0.3401, Val Loss: 0.3591\n",
      "Early stopping at epoch 95\n",
      "Best f1_score for fold 1: 0.8650\n",
      "Fold 2/5\n",
      "Epoch [1/500], Train Loss: 0.7079, Val Loss: 0.6596\n",
      "Epoch [10/500], Train Loss: 0.5697, Val Loss: 0.5636\n",
      "Epoch [20/500], Train Loss: 0.4662, Val Loss: 0.4637\n",
      "Epoch [30/500], Train Loss: 0.4167, Val Loss: 0.4264\n",
      "Epoch [40/500], Train Loss: 0.3929, Val Loss: 0.4087\n",
      "Epoch [50/500], Train Loss: 0.3771, Val Loss: 0.3940\n",
      "Early stopping at epoch 56\n",
      "Best f1_score for fold 2: 0.8650\n",
      "Fold 3/5\n",
      "Epoch [1/500], Train Loss: 0.7061, Val Loss: 0.6643\n",
      "Epoch [10/500], Train Loss: 0.5675, Val Loss: 0.5600\n",
      "Epoch [20/500], Train Loss: 0.4698, Val Loss: 0.4690\n",
      "Epoch [30/500], Train Loss: 0.4168, Val Loss: 0.4110\n",
      "Epoch [40/500], Train Loss: 0.3938, Val Loss: 0.4052\n",
      "Epoch [50/500], Train Loss: 0.3751, Val Loss: 0.3865\n",
      "Epoch [60/500], Train Loss: 0.3604, Val Loss: 0.3751\n",
      "Epoch [70/500], Train Loss: 0.3550, Val Loss: 0.3751\n",
      "Epoch [80/500], Train Loss: 0.3459, Val Loss: 0.3581\n",
      "Epoch [90/500], Train Loss: 0.3411, Val Loss: 0.3499\n",
      "Early stopping at epoch 99\n",
      "Best f1_score for fold 3: 0.8684\n",
      "Fold 4/5\n",
      "Epoch [1/500], Train Loss: 0.7045, Val Loss: 0.6557\n",
      "Epoch [10/500], Train Loss: 0.5758, Val Loss: 0.5650\n",
      "Epoch [20/500], Train Loss: 0.4753, Val Loss: 0.4644\n",
      "Epoch [30/500], Train Loss: 0.4282, Val Loss: 0.4244\n",
      "Epoch [40/500], Train Loss: 0.3997, Val Loss: 0.4045\n",
      "Epoch [50/500], Train Loss: 0.3847, Val Loss: 0.3903\n",
      "Epoch [60/500], Train Loss: 0.3737, Val Loss: 0.3799\n",
      "Epoch [70/500], Train Loss: 0.3622, Val Loss: 0.3699\n",
      "Epoch [80/500], Train Loss: 0.3529, Val Loss: 0.3741\n",
      "Epoch [90/500], Train Loss: 0.3467, Val Loss: 0.3639\n",
      "Epoch [100/500], Train Loss: 0.3380, Val Loss: 0.3649\n",
      "Epoch [110/500], Train Loss: 0.3346, Val Loss: 0.3567\n",
      "Early stopping at epoch 114\n",
      "Best f1_score for fold 4: 0.8684\n",
      "Fold 5/5\n",
      "Epoch [1/500], Train Loss: 0.7079, Val Loss: 0.6700\n",
      "Epoch [10/500], Train Loss: 0.5755, Val Loss: 0.5765\n",
      "Epoch [20/500], Train Loss: 0.4765, Val Loss: 0.4824\n",
      "Epoch [30/500], Train Loss: 0.4259, Val Loss: 0.4296\n",
      "Epoch [40/500], Train Loss: 0.3990, Val Loss: 0.4090\n",
      "Epoch [50/500], Train Loss: 0.3797, Val Loss: 0.3978\n",
      "Epoch [60/500], Train Loss: 0.3686, Val Loss: 0.3798\n",
      "Epoch [70/500], Train Loss: 0.3629, Val Loss: 0.3797\n",
      "Epoch [80/500], Train Loss: 0.3537, Val Loss: 0.3718\n",
      "Epoch [90/500], Train Loss: 0.3463, Val Loss: 0.3565\n",
      "Early stopping at epoch 99\n",
      "Best f1_score for fold 5: 0.8684\n",
      "TabTransformer - SMOTE data\n",
      "Fold 1/5\n",
      "Epoch [1/500], Train Loss: 0.6655, Val Loss: 0.6128\n",
      "Epoch [10/500], Train Loss: 0.5297, Val Loss: 0.5407\n",
      "Epoch [20/500], Train Loss: 0.4471, Val Loss: 0.4635\n",
      "Epoch [30/500], Train Loss: 0.4007, Val Loss: 0.4242\n",
      "Epoch [40/500], Train Loss: 0.3716, Val Loss: 0.4092\n",
      "Epoch [50/500], Train Loss: 0.3547, Val Loss: 0.3955\n",
      "Epoch [60/500], Train Loss: 0.3413, Val Loss: 0.3906\n",
      "Epoch [70/500], Train Loss: 0.3333, Val Loss: 0.3887\n",
      "Epoch [80/500], Train Loss: 0.3250, Val Loss: 0.3837\n",
      "Early stopping at epoch 84\n",
      "Best f1_score for fold 1: 0.8579\n",
      "Fold 2/5\n",
      "Epoch [1/500], Train Loss: 0.6699, Val Loss: 0.6161\n",
      "Epoch [10/500], Train Loss: 0.5261, Val Loss: 0.5306\n",
      "Epoch [20/500], Train Loss: 0.4421, Val Loss: 0.4607\n",
      "Epoch [30/500], Train Loss: 0.3981, Val Loss: 0.4314\n",
      "Epoch [40/500], Train Loss: 0.3743, Val Loss: 0.4103\n",
      "Epoch [50/500], Train Loss: 0.3548, Val Loss: 0.4016\n",
      "Epoch [60/500], Train Loss: 0.3440, Val Loss: 0.3878\n",
      "Epoch [70/500], Train Loss: 0.3316, Val Loss: 0.3897\n",
      "Epoch [80/500], Train Loss: 0.3281, Val Loss: 0.3874\n",
      "Early stopping at epoch 85\n",
      "Best f1_score for fold 2: 0.8579\n",
      "Fold 3/5\n",
      "Epoch [1/500], Train Loss: 0.6654, Val Loss: 0.6257\n",
      "Epoch [10/500], Train Loss: 0.5295, Val Loss: 0.5260\n",
      "Epoch [20/500], Train Loss: 0.4432, Val Loss: 0.4567\n",
      "Epoch [30/500], Train Loss: 0.3999, Val Loss: 0.4269\n",
      "Epoch [40/500], Train Loss: 0.3763, Val Loss: 0.4087\n",
      "Epoch [50/500], Train Loss: 0.3577, Val Loss: 0.3927\n",
      "Epoch [60/500], Train Loss: 0.3446, Val Loss: 0.3921\n",
      "Epoch [70/500], Train Loss: 0.3341, Val Loss: 0.3893\n",
      "Epoch [80/500], Train Loss: 0.3281, Val Loss: 0.3901\n",
      "Early stopping at epoch 84\n",
      "Best f1_score for fold 3: 0.8579\n",
      "Fold 4/5\n",
      "Epoch [1/500], Train Loss: 0.6694, Val Loss: 0.6161\n",
      "Epoch [10/500], Train Loss: 0.5305, Val Loss: 0.5307\n",
      "Epoch [20/500], Train Loss: 0.4475, Val Loss: 0.4632\n",
      "Epoch [30/500], Train Loss: 0.4008, Val Loss: 0.4179\n",
      "Epoch [40/500], Train Loss: 0.3751, Val Loss: 0.4005\n",
      "Epoch [50/500], Train Loss: 0.3585, Val Loss: 0.3942\n",
      "Early stopping at epoch 56\n",
      "Best f1_score for fold 4: 0.8579\n",
      "Fold 5/5\n",
      "Epoch [1/500], Train Loss: 0.6712, Val Loss: 0.6168\n",
      "Epoch [10/500], Train Loss: 0.5238, Val Loss: 0.5294\n",
      "Epoch [20/500], Train Loss: 0.4453, Val Loss: 0.4706\n",
      "Epoch [30/500], Train Loss: 0.3975, Val Loss: 0.4295\n",
      "Epoch [40/500], Train Loss: 0.3712, Val Loss: 0.4136\n",
      "Epoch [50/500], Train Loss: 0.3545, Val Loss: 0.4090\n",
      "Epoch [60/500], Train Loss: 0.3438, Val Loss: 0.3886\n",
      "Early stopping at epoch 69\n",
      "Best f1_score for fold 5: 0.8579\n",
      "Model state saved to ./models/tab_transformer_best_state.pth\n",
      "Model state saved to ./models/tab_transformer_best_ros_state.pth\n",
      "Model state saved to ./models/tab_transformer_best_smote_state.pth\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "n_folds = 5\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "# TabTransformer\n",
    "stratified_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_ros = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_smote = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "print('TabTransformer - original data')\n",
    "tab_transformer_best_state, tab_transformer_fold_results = find_best_model_state('TabTransformer', X, y, input_dim=X.shape[1], num_classes=3, stratified_fold=stratified_fold, batch_size=batch_size, num_epochs=num_epochs, device=device)\n",
    "print('TabTransformer - ROS data')\n",
    "tab_transformer_best_ros_state, tab_transformer_fold_results_ros = find_best_model_state('TabTransformer', X_ros, y_ros, input_dim=X_ros.shape[1], num_classes=3, stratified_fold=stratified_fold_ros, batch_size=batch_size, num_epochs=num_epochs, device=device)\n",
    "print('TabTransformer - SMOTE data')\n",
    "tab_transformer_best_smote_state, tab_transformer_fold_results_smote = find_best_model_state('TabTransformer', X_smote, y_smote, input_dim=X_smote.shape[1], num_classes=3, stratified_fold=stratified_fold_smote, batch_size=batch_size, num_epochs=num_epochs, device=device)\n",
    "save_model_state(tab_transformer_best_state, './models/tab_transformer_best_state.pth')\n",
    "save_model_state(tab_transformer_best_ros_state, './models/tab_transformer_best_ros_state.pth')\n",
    "save_model_state(tab_transformer_best_smote_state, './models/tab_transformer_best_smote_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(mlp_fold_results_list), pd.DataFrame(mlp_fold_results_list_ros), pd.DataFrame(mlp_fold_results_list_smote), pd.DataFrame(tab_transformer_fold_results), pd.DataFrame(tab_transformer_fold_results_ros), pd.DataFrame(tab_transformer_fold_results_smote)], axis=0).to_csv('./data/5_final_fold_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "mlp_best_model_state = torch.load('./models/mlp_best_state.pth')\n",
    "mlp_best_model = MLP(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "mlp_best_model.load_state_dict(mlp_best_model_state)\n",
    "mlp_best_roc_model_state = torch.load('./models/mlp_best_ros_state.pth')\n",
    "mlp_best_roc_model = MLP(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "mlp_best_roc_model.load_state_dict(mlp_best_roc_model_state)\n",
    "mlp_best_smote_model_state = torch.load('./models/mlp_best_smote_state.pth')\n",
    "mlp_best_smote_model = MLP(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "mlp_best_smote_model.load_state_dict(mlp_best_smote_model_state)\n",
    "tab_transformer_best_model_state = torch.load('./models/tab_transformer_best_state.pth')\n",
    "tab_transformer_best_model = TabTransformer(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "tab_transformer_best_model.load_state_dict(tab_transformer_best_model_state)\n",
    "tab_transformer_best_roc_model_state = torch.load('./models/tab_transformer_best_ros_state.pth')\n",
    "tab_transformer_best_roc_model = TabTransformer(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "tab_transformer_best_roc_model.load_state_dict(tab_transformer_best_roc_model_state)\n",
    "tab_transformer_best_smote_model_state = torch.load('./models/tab_transformer_best_smote_state.pth')\n",
    "tab_transformer_best_smote_model = TabTransformer(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "tab_transformer_best_smote_model.load_state_dict(tab_transformer_best_smote_model_state)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, device, name='MLP'):\n",
    "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(X_test, y_test),\n",
    "        batch_size=128, num_workers=8\n",
    "    )\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "\n",
    "    results = {\n",
    "        'name': name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([results])\n",
    "\n",
    "# Evaluate best model\n",
    "result_mlp_df = evaluate_model(mlp_best_model, X_test, y_test, device, name='MLP')\n",
    "result_mlp_roc_df = evaluate_model(mlp_best_roc_model, X_test, y_test, device, name='MLP_ROS')\n",
    "result_mlp_smote_df = evaluate_model(mlp_best_smote_model, X_test, y_test, device, name='MLP_SMOTE')\n",
    "result_tab_transformer_df = evaluate_model(tab_transformer_best_model, X_test, y_test, device, name='TabTransformer')\n",
    "result_tab_transformer_roc_df = evaluate_model(tab_transformer_best_roc_model, X_test, y_test, device, name='TabTransformer_ROS')\n",
    "result_tab_transformer_smote_df = evaluate_model(tab_transformer_best_smote_model, X_test, y_test, device, name='TabTransformer_SMOTE')\n",
    "\n",
    "# Combine results\n",
    "result_df = pd.concat([result_mlp_df, result_mlp_roc_df, result_mlp_smote_df, result_tab_transformer_df, result_tab_transformer_roc_df, result_tab_transformer_smote_df], axis=0)\n",
    "result_df.to_csv('./data/5_test_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "roc_auc",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "d2093a53-0171-4899-9926-ca47c7d18505",
       "rows": [
        [
         "0",
         "MLP",
         "0.7552742616033755",
         "0.7431174453981032",
         "0.7427854860888488",
         "0.7419848215678554",
         "0.8968920398745962"
        ],
        [
         "0",
         "MLP_ROS",
         "0.749198312236287",
         "0.7336566021709766",
         "0.7970163718407036",
         "0.7485633128164845",
         "0.902188893451301"
        ],
        [
         "0",
         "MLP_SMOTE",
         "0.7508860759493671",
         "0.7276331843033897",
         "0.7696991856391637",
         "0.7430049138686535",
         "0.8922948851548034"
        ],
        [
         "0",
         "TabTransformer",
         "0.7854852320675105",
         "0.7711859631728147",
         "0.7959496173309123",
         "0.7813335603863028",
         "0.9093225436969181"
        ],
        [
         "0",
         "TabTransformer_ROS",
         "0.7818284106891702",
         "0.7640553767334953",
         "0.8209883441963025",
         "0.7809815750832764",
         "0.912706214412149"
        ],
        [
         "0",
         "TabTransformer_SMOTE",
         "0.7766526019690576",
         "0.760519312870894",
         "0.7901600951350082",
         "0.7714485484288952",
         "0.9077417499285737"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.755274</td>\n",
       "      <td>0.743117</td>\n",
       "      <td>0.742785</td>\n",
       "      <td>0.741985</td>\n",
       "      <td>0.896892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP_ROS</td>\n",
       "      <td>0.749198</td>\n",
       "      <td>0.733657</td>\n",
       "      <td>0.797016</td>\n",
       "      <td>0.748563</td>\n",
       "      <td>0.902189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP_SMOTE</td>\n",
       "      <td>0.750886</td>\n",
       "      <td>0.727633</td>\n",
       "      <td>0.769699</td>\n",
       "      <td>0.743005</td>\n",
       "      <td>0.892295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.785485</td>\n",
       "      <td>0.771186</td>\n",
       "      <td>0.795950</td>\n",
       "      <td>0.781334</td>\n",
       "      <td>0.909323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TabTransformer_ROS</td>\n",
       "      <td>0.781828</td>\n",
       "      <td>0.764055</td>\n",
       "      <td>0.820988</td>\n",
       "      <td>0.780982</td>\n",
       "      <td>0.912706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TabTransformer_SMOTE</td>\n",
       "      <td>0.776653</td>\n",
       "      <td>0.760519</td>\n",
       "      <td>0.790160</td>\n",
       "      <td>0.771449</td>\n",
       "      <td>0.907742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  accuracy  precision    recall  f1_score   roc_auc\n",
       "0                   MLP  0.755274   0.743117  0.742785  0.741985  0.896892\n",
       "0               MLP_ROS  0.749198   0.733657  0.797016  0.748563  0.902189\n",
       "0             MLP_SMOTE  0.750886   0.727633  0.769699  0.743005  0.892295\n",
       "0        TabTransformer  0.785485   0.771186  0.795950  0.781334  0.909323\n",
       "0    TabTransformer_ROS  0.781828   0.764055  0.820988  0.780982  0.912706\n",
       "0  TabTransformer_SMOTE  0.776653   0.760519  0.790160  0.771449  0.907742"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name  accuracy  precision    recall  f1_score   roc_auc\n",
      "0                   MLP  0.755274   0.743117  0.742785  0.741985  0.896892\n",
      "0               MLP_ROS  0.749198   0.733657  0.797016  0.748563  0.902189\n",
      "0             MLP_SMOTE  0.750886   0.727633  0.769699  0.743005  0.892295\n",
      "0        TabTransformer  0.785485   0.771186  0.795950  0.781334  0.909323\n",
      "0    TabTransformer_ROS  0.781828   0.764055  0.820988  0.780982  0.912706\n",
      "0  TabTransformer_SMOTE  0.776653   0.760519  0.790160  0.771449  0.907742\n"
     ]
    }
   ],
   "source": [
    "print(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
