{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Importing the models\n",
    "from models.mlp import MLP\n",
    "from models.transformer import Transformer\n",
    "from models.ft_transformer_wrapper import FTTransformerWrapper\n",
    "from models.tab_transformer_wrapper import TabTransformerWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model_state(model_name, X, y, input_dim, num_classes, stratified_fold, batch_size=64, num_epochs=200, n_folds=10, device='cuda', metric='f1_score', cat_dims=[0, 12, 15, 29, 30], cat_idxs=[12, 3, 3, 3, 3], num_idxs=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]):\n",
    "    \"\"\"Get best model out of Cross-Validation, using the specified metric.\"\"\"\n",
    "    best_model_state = None\n",
    "    best_metric = -np.inf\n",
    "    fold_results_list = []\n",
    "    for fold, (train_ids, val_ids) in enumerate(stratified_fold.split(X, y)):\n",
    "        print(f'Fold {fold + 1}/{n_folds}')\n",
    "        # Prepare data loaders\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X[train_ids], y[train_ids]),\n",
    "            batch_size=batch_size, num_workers=0, shuffle=True\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X[val_ids], y[val_ids]),\n",
    "            batch_size=batch_size, num_workers=0, \n",
    "        )\n",
    "        # Initialize model, criterion, and optimizer\n",
    "        if model_name == 'MLP':\n",
    "            model = MLP(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "        elif model_name == 'TabTransformer':\n",
    "            model = TabTransformerWrapper(cat_dims=cat_dims,cat_idxs=cat_idxs,num_idxs=num_idxs,num_classes=num_classes,num_heads=4,num_layers=2,dim_model=64,dropout=0.1).to(device)\n",
    "        elif model_name == 'FTTransformer':\n",
    "            model = FTTransformerWrapper(cat_dims=cat_dims,cat_idxs=cat_idxs,num_idxs=num_idxs,num_classes=num_classes,num_heads=4,num_layers=2,dim_model=64,dim_ff=128,dropout=0.1).to(device)\n",
    "        elif model_name == 'Transformer':\n",
    "            model = Transformer(input_dim=input_dim, num_classes=num_classes, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # Train model\n",
    "        model_state = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=num_epochs)\n",
    "        model.load_state_dict(model_state)\n",
    "        # Evaluate model\n",
    "        result_df = evaluate_model(model, val_loader, device)\n",
    "        metric_result = result_df[metric].values[0]\n",
    "\n",
    "        fold_result = {\n",
    "            'Fold': fold + 1,\n",
    "            'Model Name': model_name,\n",
    "        }\n",
    "        fold_result.update(result_df.to_dict(orient='records')[0])\n",
    "        fold_results_list.append(fold_result)\n",
    "\n",
    "        if metric_result > best_metric:\n",
    "            best_metric = metric_result\n",
    "            best_model_state = model.state_dict()\n",
    "        print(f'Best {metric} for fold {fold + 1}: {best_metric:.4f}')\n",
    "        \n",
    "    return best_model_state, fold_results_list\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=500, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_attentions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "            # Get attention weights\n",
    "            if hasattr(model, 'get_attention_weights') and callable(model.get_attention_weights):\n",
    "                try:\n",
    "                    attention_weights = model.get_attention_weights(batch_x)\n",
    "                    if attention_weights is not None:  # Make sure it's not None\n",
    "                        all_attentions.append(attention_weights.cpu().numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting attention weights: {e}\")\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'attention_weights': None # np.concatenate(all_attentions, axis=0) if len(all_attentions) > 0 else None\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([results])\n",
    "\n",
    "def save_model_state(model_state, output_path):\n",
    "    torch.save(model_state, output_path)\n",
    "    print(f'Model state saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System settings\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# TabTransformer Model Settings\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('./data/3_train_processed.csv')\n",
    "train_ros_df = pd.read_csv('./data/3_train_ros_processed.csv')\n",
    "train_smote_df = pd.read_csv('./data/3_train_smote_processed.csv')\n",
    "test_df = pd.read_csv('./data/3_test_processed.csv')\n",
    "\n",
    "# Preprocess data\n",
    "train_features = train_df.drop(['credit_score'], axis=1)\n",
    "train_ros_features = train_ros_df.drop(['credit_score'], axis=1)\n",
    "train_smote_features = train_smote_df.drop(['credit_score'], axis=1)\n",
    "test_features = test_df.drop(['credit_score'], axis=1)\n",
    "\n",
    "train_labels = train_df['credit_score']\n",
    "train_ros_labels = train_ros_df['credit_score']\n",
    "train_smote_labels = train_smote_df['credit_score']\n",
    "test_labels = test_df['credit_score']\n",
    "\n",
    "X = torch.FloatTensor(train_features.values)\n",
    "X_ros = torch.FloatTensor(train_ros_features.values)\n",
    "X_smote = torch.FloatTensor(train_smote_features.values)\n",
    "X_test = torch.FloatTensor(test_features.values)\n",
    "y = torch.LongTensor(train_labels.values)\n",
    "y_ros = torch.LongTensor(train_ros_labels.values)\n",
    "y_smote = torch.LongTensor(train_smote_labels.values)\n",
    "y_test = torch.LongTensor(test_labels.values)\n",
    "\n",
    "# Transformer Model Settings\n",
    "# 1. Categorical features\n",
    "cat_columns = ['month', 'credit_mix', 'payment_of_min_amount', 'spending_level', 'payment_size']\n",
    "cat_idxs = [0, 12, 15, 29, 30]\n",
    "# 2. Continuous features - Month could have 12 unique values, so we treat it as a categorical feature\n",
    "cat_dims = [12, train_df['credit_mix'].nunique(), train_df['payment_of_min_amount'].nunique(), \n",
    "            train_df['spending_level'].nunique() + 1, train_df['payment_size'].nunique()]\n",
    "# 3. Other columns\n",
    "all_columns = list(train_df.columns)\n",
    "num_idxs = [i for i in range(len(all_columns)) if i not in cat_idxs and i != all_columns.index('credit_score')]  # 排除目标变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - original data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7424, Val Loss: 0.6865\n",
      "Epoch [10/500], Train Loss: 0.6165, Val Loss: 0.6282\n",
      "Epoch [20/500], Train Loss: 0.5765, Val Loss: 0.6123\n",
      "Epoch [30/500], Train Loss: 0.5449, Val Loss: 0.5923\n",
      "Epoch [40/500], Train Loss: 0.5255, Val Loss: 0.5779\n",
      "Epoch [50/500], Train Loss: 0.5070, Val Loss: 0.5667\n",
      "Epoch [60/500], Train Loss: 0.4945, Val Loss: 0.5555\n",
      "Epoch [70/500], Train Loss: 0.4827, Val Loss: 0.5584\n",
      "Epoch [80/500], Train Loss: 0.4723, Val Loss: 0.5476\n",
      "Epoch [90/500], Train Loss: 0.4695, Val Loss: 0.5394\n",
      "Epoch [100/500], Train Loss: 0.4628, Val Loss: 0.5429\n",
      "Epoch [110/500], Train Loss: 0.4570, Val Loss: 0.5402\n",
      "Early stopping at epoch 111\n",
      "Best f1_score for fold 1: 0.7578\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7440, Val Loss: 0.6893\n",
      "Epoch [10/500], Train Loss: 0.6168, Val Loss: 0.6314\n",
      "Epoch [20/500], Train Loss: 0.5735, Val Loss: 0.6141\n",
      "Epoch [30/500], Train Loss: 0.5424, Val Loss: 0.5998\n",
      "Epoch [40/500], Train Loss: 0.5198, Val Loss: 0.5843\n",
      "Epoch [50/500], Train Loss: 0.5049, Val Loss: 0.5713\n",
      "Epoch [60/500], Train Loss: 0.4944, Val Loss: 0.5669\n",
      "Epoch [70/500], Train Loss: 0.4825, Val Loss: 0.5608\n",
      "Early stopping at epoch 76\n",
      "Best f1_score for fold 2: 0.7578\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7459, Val Loss: 0.7024\n",
      "Epoch [10/500], Train Loss: 0.6156, Val Loss: 0.6383\n",
      "Epoch [20/500], Train Loss: 0.5746, Val Loss: 0.6178\n",
      "Epoch [30/500], Train Loss: 0.5407, Val Loss: 0.5965\n",
      "Epoch [40/500], Train Loss: 0.5184, Val Loss: 0.5819\n",
      "Epoch [50/500], Train Loss: 0.5022, Val Loss: 0.5734\n",
      "Epoch [60/500], Train Loss: 0.4885, Val Loss: 0.5636\n",
      "Epoch [70/500], Train Loss: 0.4801, Val Loss: 0.5550\n",
      "Epoch [80/500], Train Loss: 0.4736, Val Loss: 0.5575\n",
      "Epoch [90/500], Train Loss: 0.4678, Val Loss: 0.5447\n",
      "Epoch [100/500], Train Loss: 0.4597, Val Loss: 0.5483\n",
      "Early stopping at epoch 100\n",
      "Best f1_score for fold 3: 0.7578\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7478, Val Loss: 0.6890\n",
      "Epoch [10/500], Train Loss: 0.6181, Val Loss: 0.6360\n",
      "Epoch [20/500], Train Loss: 0.5784, Val Loss: 0.6181\n",
      "Epoch [30/500], Train Loss: 0.5499, Val Loss: 0.5968\n",
      "Epoch [40/500], Train Loss: 0.5296, Val Loss: 0.5873\n",
      "Epoch [50/500], Train Loss: 0.5113, Val Loss: 0.5748\n",
      "Epoch [60/500], Train Loss: 0.4958, Val Loss: 0.5661\n",
      "Epoch [70/500], Train Loss: 0.4860, Val Loss: 0.5601\n",
      "Epoch [80/500], Train Loss: 0.4780, Val Loss: 0.5469\n",
      "Epoch [90/500], Train Loss: 0.4713, Val Loss: 0.5424\n",
      "Epoch [100/500], Train Loss: 0.4659, Val Loss: 0.5373\n",
      "Epoch [110/500], Train Loss: 0.4578, Val Loss: 0.5384\n",
      "Early stopping at epoch 115\n",
      "Best f1_score for fold 4: 0.7650\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7446, Val Loss: 0.6960\n",
      "Epoch [10/500], Train Loss: 0.6163, Val Loss: 0.6341\n",
      "Epoch [20/500], Train Loss: 0.5756, Val Loss: 0.6133\n",
      "Epoch [30/500], Train Loss: 0.5472, Val Loss: 0.6062\n",
      "Epoch [40/500], Train Loss: 0.5231, Val Loss: 0.5961\n",
      "Epoch [50/500], Train Loss: 0.5088, Val Loss: 0.5886\n",
      "Epoch [60/500], Train Loss: 0.4969, Val Loss: 0.5784\n",
      "Epoch [70/500], Train Loss: 0.4857, Val Loss: 0.5753\n",
      "Epoch [80/500], Train Loss: 0.4766, Val Loss: 0.5702\n",
      "Epoch [90/500], Train Loss: 0.4746, Val Loss: 0.5661\n",
      "Early stopping at epoch 96\n",
      "Best f1_score for fold 5: 0.7650\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7480, Val Loss: 0.6807\n",
      "Epoch [10/500], Train Loss: 0.6170, Val Loss: 0.6120\n",
      "Epoch [20/500], Train Loss: 0.5784, Val Loss: 0.5986\n",
      "Epoch [30/500], Train Loss: 0.5470, Val Loss: 0.5784\n",
      "Epoch [40/500], Train Loss: 0.5270, Val Loss: 0.5713\n",
      "Epoch [50/500], Train Loss: 0.5095, Val Loss: 0.5589\n",
      "Epoch [60/500], Train Loss: 0.4978, Val Loss: 0.5539\n",
      "Epoch [70/500], Train Loss: 0.4876, Val Loss: 0.5448\n",
      "Epoch [80/500], Train Loss: 0.4790, Val Loss: 0.5432\n",
      "Epoch [90/500], Train Loss: 0.4680, Val Loss: 0.5364\n",
      "Epoch [100/500], Train Loss: 0.4627, Val Loss: 0.5283\n",
      "Epoch [110/500], Train Loss: 0.4621, Val Loss: 0.5207\n",
      "Epoch [120/500], Train Loss: 0.4566, Val Loss: 0.5194\n",
      "Epoch [130/500], Train Loss: 0.4542, Val Loss: 0.5203\n",
      "Early stopping at epoch 130\n",
      "Best f1_score for fold 6: 0.7679\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7449, Val Loss: 0.6838\n",
      "Epoch [10/500], Train Loss: 0.6163, Val Loss: 0.6280\n",
      "Epoch [20/500], Train Loss: 0.5774, Val Loss: 0.6093\n",
      "Epoch [30/500], Train Loss: 0.5477, Val Loss: 0.5913\n",
      "Epoch [40/500], Train Loss: 0.5275, Val Loss: 0.5858\n",
      "Epoch [50/500], Train Loss: 0.5109, Val Loss: 0.5752\n",
      "Epoch [60/500], Train Loss: 0.4971, Val Loss: 0.5654\n",
      "Epoch [70/500], Train Loss: 0.4855, Val Loss: 0.5635\n",
      "Epoch [80/500], Train Loss: 0.4776, Val Loss: 0.5592\n",
      "Early stopping at epoch 84\n",
      "Best f1_score for fold 7: 0.7679\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7467, Val Loss: 0.6870\n",
      "Epoch [10/500], Train Loss: 0.6148, Val Loss: 0.6345\n",
      "Epoch [20/500], Train Loss: 0.5753, Val Loss: 0.6154\n",
      "Epoch [30/500], Train Loss: 0.5435, Val Loss: 0.5973\n",
      "Epoch [40/500], Train Loss: 0.5205, Val Loss: 0.5821\n",
      "Epoch [50/500], Train Loss: 0.5022, Val Loss: 0.5790\n",
      "Epoch [60/500], Train Loss: 0.4891, Val Loss: 0.5679\n",
      "Epoch [70/500], Train Loss: 0.4812, Val Loss: 0.5633\n",
      "Epoch [80/500], Train Loss: 0.4724, Val Loss: 0.5630\n",
      "Epoch [90/500], Train Loss: 0.4687, Val Loss: 0.5540\n",
      "Epoch [100/500], Train Loss: 0.4609, Val Loss: 0.5562\n",
      "Epoch [110/500], Train Loss: 0.4588, Val Loss: 0.5500\n",
      "Early stopping at epoch 116\n",
      "Best f1_score for fold 8: 0.7679\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7519, Val Loss: 0.6955\n",
      "Epoch [10/500], Train Loss: 0.6196, Val Loss: 0.6348\n",
      "Epoch [20/500], Train Loss: 0.5812, Val Loss: 0.6156\n",
      "Epoch [30/500], Train Loss: 0.5545, Val Loss: 0.5977\n",
      "Epoch [40/500], Train Loss: 0.5300, Val Loss: 0.5860\n",
      "Epoch [50/500], Train Loss: 0.5112, Val Loss: 0.5720\n",
      "Epoch [60/500], Train Loss: 0.5011, Val Loss: 0.5668\n",
      "Epoch [70/500], Train Loss: 0.4906, Val Loss: 0.5629\n",
      "Epoch [80/500], Train Loss: 0.4829, Val Loss: 0.5587\n",
      "Epoch [90/500], Train Loss: 0.4754, Val Loss: 0.5541\n",
      "Early stopping at epoch 97\n",
      "Best f1_score for fold 9: 0.7679\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7461, Val Loss: 0.6999\n",
      "Epoch [10/500], Train Loss: 0.6166, Val Loss: 0.6245\n",
      "Epoch [20/500], Train Loss: 0.5779, Val Loss: 0.6091\n",
      "Epoch [30/500], Train Loss: 0.5497, Val Loss: 0.5960\n",
      "Epoch [40/500], Train Loss: 0.5288, Val Loss: 0.5851\n",
      "Epoch [50/500], Train Loss: 0.5102, Val Loss: 0.5740\n",
      "Epoch [60/500], Train Loss: 0.4993, Val Loss: 0.5703\n",
      "Epoch [70/500], Train Loss: 0.4858, Val Loss: 0.5586\n",
      "Epoch [80/500], Train Loss: 0.4783, Val Loss: 0.5578\n",
      "Early stopping at epoch 84\n",
      "Best f1_score for fold 10: 0.7679\n",
      "MLP - ROS data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7310, Val Loss: 0.6686\n",
      "Epoch [10/500], Train Loss: 0.5707, Val Loss: 0.5728\n",
      "Epoch [20/500], Train Loss: 0.5116, Val Loss: 0.5153\n",
      "Epoch [30/500], Train Loss: 0.4787, Val Loss: 0.4718\n",
      "Epoch [40/500], Train Loss: 0.4562, Val Loss: 0.4496\n",
      "Epoch [50/500], Train Loss: 0.4430, Val Loss: 0.4351\n",
      "Epoch [60/500], Train Loss: 0.4330, Val Loss: 0.4248\n",
      "Epoch [70/500], Train Loss: 0.4248, Val Loss: 0.4131\n",
      "Epoch [80/500], Train Loss: 0.4188, Val Loss: 0.4030\n",
      "Epoch [90/500], Train Loss: 0.4162, Val Loss: 0.4024\n",
      "Early stopping at epoch 94\n",
      "Best f1_score for fold 1: 0.8403\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7336, Val Loss: 0.6813\n",
      "Epoch [10/500], Train Loss: 0.5698, Val Loss: 0.5740\n",
      "Epoch [20/500], Train Loss: 0.5124, Val Loss: 0.5258\n",
      "Epoch [30/500], Train Loss: 0.4811, Val Loss: 0.4904\n",
      "Epoch [40/500], Train Loss: 0.4579, Val Loss: 0.4624\n",
      "Epoch [50/500], Train Loss: 0.4462, Val Loss: 0.4453\n",
      "Epoch [60/500], Train Loss: 0.4367, Val Loss: 0.4363\n",
      "Epoch [70/500], Train Loss: 0.4277, Val Loss: 0.4267\n",
      "Epoch [80/500], Train Loss: 0.4198, Val Loss: 0.4210\n",
      "Early stopping at epoch 88\n",
      "Best f1_score for fold 2: 0.8403\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7328, Val Loss: 0.6711\n",
      "Epoch [10/500], Train Loss: 0.5721, Val Loss: 0.5691\n",
      "Epoch [20/500], Train Loss: 0.5136, Val Loss: 0.5168\n",
      "Epoch [30/500], Train Loss: 0.4785, Val Loss: 0.4759\n",
      "Epoch [40/500], Train Loss: 0.4564, Val Loss: 0.4587\n",
      "Epoch [50/500], Train Loss: 0.4419, Val Loss: 0.4353\n",
      "Epoch [60/500], Train Loss: 0.4320, Val Loss: 0.4285\n",
      "Epoch [70/500], Train Loss: 0.4271, Val Loss: 0.4270\n",
      "Epoch [80/500], Train Loss: 0.4171, Val Loss: 0.4155\n",
      "Epoch [90/500], Train Loss: 0.4157, Val Loss: 0.4083\n",
      "Early stopping at epoch 99\n",
      "Best f1_score for fold 3: 0.8403\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7311, Val Loss: 0.6653\n",
      "Epoch [10/500], Train Loss: 0.5727, Val Loss: 0.5617\n",
      "Epoch [20/500], Train Loss: 0.5143, Val Loss: 0.5037\n",
      "Epoch [30/500], Train Loss: 0.4825, Val Loss: 0.4678\n",
      "Epoch [40/500], Train Loss: 0.4608, Val Loss: 0.4459\n",
      "Epoch [50/500], Train Loss: 0.4457, Val Loss: 0.4336\n",
      "Epoch [60/500], Train Loss: 0.4347, Val Loss: 0.4205\n",
      "Epoch [70/500], Train Loss: 0.4274, Val Loss: 0.4107\n",
      "Epoch [80/500], Train Loss: 0.4198, Val Loss: 0.4016\n",
      "Epoch [90/500], Train Loss: 0.4158, Val Loss: 0.3951\n",
      "Epoch [100/500], Train Loss: 0.4117, Val Loss: 0.3927\n",
      "Epoch [110/500], Train Loss: 0.4054, Val Loss: 0.3919\n",
      "Epoch [120/500], Train Loss: 0.4024, Val Loss: 0.3858\n",
      "Epoch [130/500], Train Loss: 0.4014, Val Loss: 0.3864\n",
      "Epoch [140/500], Train Loss: 0.3974, Val Loss: 0.3840\n",
      "Epoch [150/500], Train Loss: 0.3977, Val Loss: 0.3808\n",
      "Early stopping at epoch 157\n",
      "Best f1_score for fold 4: 0.8516\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7334, Val Loss: 0.6865\n",
      "Epoch [10/500], Train Loss: 0.5715, Val Loss: 0.5748\n",
      "Epoch [20/500], Train Loss: 0.5115, Val Loss: 0.5175\n",
      "Epoch [30/500], Train Loss: 0.4804, Val Loss: 0.4766\n",
      "Epoch [40/500], Train Loss: 0.4589, Val Loss: 0.4505\n",
      "Epoch [50/500], Train Loss: 0.4445, Val Loss: 0.4390\n",
      "Epoch [60/500], Train Loss: 0.4348, Val Loss: 0.4315\n",
      "Epoch [70/500], Train Loss: 0.4254, Val Loss: 0.4244\n",
      "Epoch [80/500], Train Loss: 0.4210, Val Loss: 0.4196\n",
      "Epoch [90/500], Train Loss: 0.4160, Val Loss: 0.4147\n",
      "Epoch [100/500], Train Loss: 0.4135, Val Loss: 0.4141\n",
      "Epoch [110/500], Train Loss: 0.4106, Val Loss: 0.4064\n",
      "Epoch [120/500], Train Loss: 0.4055, Val Loss: 0.4030\n",
      "Epoch [130/500], Train Loss: 0.4052, Val Loss: 0.4003\n",
      "Early stopping at epoch 133\n",
      "Best f1_score for fold 5: 0.8516\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7392, Val Loss: 0.6733\n",
      "Epoch [10/500], Train Loss: 0.5749, Val Loss: 0.5660\n",
      "Epoch [20/500], Train Loss: 0.5133, Val Loss: 0.5104\n",
      "Epoch [30/500], Train Loss: 0.4810, Val Loss: 0.4682\n",
      "Epoch [40/500], Train Loss: 0.4613, Val Loss: 0.4469\n",
      "Epoch [50/500], Train Loss: 0.4461, Val Loss: 0.4293\n",
      "Epoch [60/500], Train Loss: 0.4363, Val Loss: 0.4228\n",
      "Epoch [70/500], Train Loss: 0.4283, Val Loss: 0.4135\n",
      "Epoch [80/500], Train Loss: 0.4212, Val Loss: 0.4063\n",
      "Epoch [90/500], Train Loss: 0.4161, Val Loss: 0.3992\n",
      "Epoch [100/500], Train Loss: 0.4114, Val Loss: 0.3937\n",
      "Epoch [110/500], Train Loss: 0.4085, Val Loss: 0.3952\n",
      "Epoch [120/500], Train Loss: 0.4084, Val Loss: 0.3895\n",
      "Epoch [130/500], Train Loss: 0.4046, Val Loss: 0.3902\n",
      "Early stopping at epoch 133\n",
      "Best f1_score for fold 6: 0.8518\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7362, Val Loss: 0.6694\n",
      "Epoch [10/500], Train Loss: 0.5725, Val Loss: 0.5665\n",
      "Epoch [20/500], Train Loss: 0.5112, Val Loss: 0.5105\n",
      "Epoch [30/500], Train Loss: 0.4743, Val Loss: 0.4731\n",
      "Epoch [40/500], Train Loss: 0.4575, Val Loss: 0.4475\n",
      "Epoch [50/500], Train Loss: 0.4428, Val Loss: 0.4396\n",
      "Epoch [60/500], Train Loss: 0.4326, Val Loss: 0.4258\n",
      "Epoch [70/500], Train Loss: 0.4256, Val Loss: 0.4222\n",
      "Epoch [80/500], Train Loss: 0.4202, Val Loss: 0.4111\n",
      "Epoch [90/500], Train Loss: 0.4141, Val Loss: 0.4098\n",
      "Early stopping at epoch 98\n",
      "Best f1_score for fold 7: 0.8518\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7337, Val Loss: 0.6671\n",
      "Epoch [10/500], Train Loss: 0.5707, Val Loss: 0.5661\n",
      "Epoch [20/500], Train Loss: 0.5093, Val Loss: 0.5050\n",
      "Epoch [30/500], Train Loss: 0.4784, Val Loss: 0.4704\n",
      "Epoch [40/500], Train Loss: 0.4590, Val Loss: 0.4464\n",
      "Epoch [50/500], Train Loss: 0.4485, Val Loss: 0.4353\n",
      "Epoch [60/500], Train Loss: 0.4366, Val Loss: 0.4252\n",
      "Epoch [70/500], Train Loss: 0.4283, Val Loss: 0.4161\n",
      "Epoch [80/500], Train Loss: 0.4228, Val Loss: 0.4077\n",
      "Epoch [90/500], Train Loss: 0.4173, Val Loss: 0.4034\n",
      "Epoch [100/500], Train Loss: 0.4134, Val Loss: 0.4004\n",
      "Early stopping at epoch 102\n",
      "Best f1_score for fold 8: 0.8518\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7341, Val Loss: 0.6772\n",
      "Epoch [10/500], Train Loss: 0.5717, Val Loss: 0.5734\n",
      "Epoch [20/500], Train Loss: 0.5130, Val Loss: 0.5234\n",
      "Epoch [30/500], Train Loss: 0.4809, Val Loss: 0.4850\n",
      "Epoch [40/500], Train Loss: 0.4562, Val Loss: 0.4673\n",
      "Epoch [50/500], Train Loss: 0.4443, Val Loss: 0.4460\n",
      "Epoch [60/500], Train Loss: 0.4333, Val Loss: 0.4382\n",
      "Epoch [70/500], Train Loss: 0.4263, Val Loss: 0.4312\n",
      "Epoch [80/500], Train Loss: 0.4222, Val Loss: 0.4254\n",
      "Epoch [90/500], Train Loss: 0.4164, Val Loss: 0.4233\n",
      "Epoch [100/500], Train Loss: 0.4104, Val Loss: 0.4156\n",
      "Epoch [110/500], Train Loss: 0.4067, Val Loss: 0.4127\n",
      "Epoch [120/500], Train Loss: 0.4037, Val Loss: 0.4067\n",
      "Epoch [130/500], Train Loss: 0.4008, Val Loss: 0.4037\n",
      "Epoch [140/500], Train Loss: 0.3963, Val Loss: 0.4046\n",
      "Early stopping at epoch 146\n",
      "Best f1_score for fold 9: 0.8518\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7300, Val Loss: 0.6840\n",
      "Epoch [10/500], Train Loss: 0.5693, Val Loss: 0.5712\n",
      "Epoch [20/500], Train Loss: 0.5129, Val Loss: 0.5210\n",
      "Epoch [30/500], Train Loss: 0.4808, Val Loss: 0.4884\n",
      "Epoch [40/500], Train Loss: 0.4606, Val Loss: 0.4635\n",
      "Epoch [50/500], Train Loss: 0.4480, Val Loss: 0.4518\n",
      "Epoch [60/500], Train Loss: 0.4389, Val Loss: 0.4439\n",
      "Epoch [70/500], Train Loss: 0.4310, Val Loss: 0.4293\n",
      "Epoch [80/500], Train Loss: 0.4252, Val Loss: 0.4249\n",
      "Epoch [90/500], Train Loss: 0.4202, Val Loss: 0.4192\n",
      "Epoch [100/500], Train Loss: 0.4143, Val Loss: 0.4151\n",
      "Epoch [110/500], Train Loss: 0.4128, Val Loss: 0.4145\n",
      "Epoch [120/500], Train Loss: 0.4096, Val Loss: 0.4076\n",
      "Epoch [130/500], Train Loss: 0.4074, Val Loss: 0.4051\n",
      "Epoch [140/500], Train Loss: 0.4051, Val Loss: 0.4075\n",
      "Epoch [150/500], Train Loss: 0.4015, Val Loss: 0.4045\n",
      "Epoch [160/500], Train Loss: 0.3997, Val Loss: 0.4011\n",
      "Epoch [170/500], Train Loss: 0.3957, Val Loss: 0.3991\n",
      "Epoch [180/500], Train Loss: 0.3951, Val Loss: 0.3968\n",
      "Epoch [190/500], Train Loss: 0.3947, Val Loss: 0.3970\n",
      "Early stopping at epoch 194\n",
      "Best f1_score for fold 10: 0.8518\n",
      "MLP - SMOTE data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.6966, Val Loss: 0.6204\n",
      "Epoch [10/500], Train Loss: 0.5324, Val Loss: 0.5297\n",
      "Epoch [20/500], Train Loss: 0.4851, Val Loss: 0.4884\n",
      "Epoch [30/500], Train Loss: 0.4546, Val Loss: 0.4596\n",
      "Epoch [40/500], Train Loss: 0.4375, Val Loss: 0.4469\n",
      "Epoch [50/500], Train Loss: 0.4236, Val Loss: 0.4338\n",
      "Epoch [60/500], Train Loss: 0.4170, Val Loss: 0.4213\n",
      "Epoch [70/500], Train Loss: 0.4082, Val Loss: 0.4152\n",
      "Epoch [80/500], Train Loss: 0.4004, Val Loss: 0.4124\n",
      "Epoch [90/500], Train Loss: 0.3966, Val Loss: 0.4108\n",
      "Epoch [100/500], Train Loss: 0.3914, Val Loss: 0.4079\n",
      "Epoch [110/500], Train Loss: 0.3878, Val Loss: 0.4066\n",
      "Epoch [120/500], Train Loss: 0.3862, Val Loss: 0.4025\n",
      "Early stopping at epoch 125\n",
      "Best f1_score for fold 1: 0.8435\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6933, Val Loss: 0.6345\n",
      "Epoch [10/500], Train Loss: 0.5287, Val Loss: 0.5325\n",
      "Epoch [20/500], Train Loss: 0.4800, Val Loss: 0.4963\n",
      "Epoch [30/500], Train Loss: 0.4505, Val Loss: 0.4648\n",
      "Epoch [40/500], Train Loss: 0.4316, Val Loss: 0.4551\n",
      "Epoch [50/500], Train Loss: 0.4207, Val Loss: 0.4372\n",
      "Epoch [60/500], Train Loss: 0.4110, Val Loss: 0.4302\n",
      "Epoch [70/500], Train Loss: 0.4044, Val Loss: 0.4249\n",
      "Epoch [80/500], Train Loss: 0.3994, Val Loss: 0.4225\n",
      "Epoch [90/500], Train Loss: 0.3948, Val Loss: 0.4205\n",
      "Epoch [100/500], Train Loss: 0.3901, Val Loss: 0.4200\n",
      "Epoch [110/500], Train Loss: 0.3880, Val Loss: 0.4159\n",
      "Epoch [120/500], Train Loss: 0.3831, Val Loss: 0.4181\n",
      "Epoch [130/500], Train Loss: 0.3797, Val Loss: 0.4097\n",
      "Early stopping at epoch 139\n",
      "Best f1_score for fold 2: 0.8435\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.6959, Val Loss: 0.6316\n",
      "Epoch [10/500], Train Loss: 0.5315, Val Loss: 0.5403\n",
      "Epoch [20/500], Train Loss: 0.4877, Val Loss: 0.5051\n",
      "Epoch [30/500], Train Loss: 0.4584, Val Loss: 0.4743\n",
      "Epoch [40/500], Train Loss: 0.4386, Val Loss: 0.4667\n",
      "Epoch [50/500], Train Loss: 0.4269, Val Loss: 0.4521\n",
      "Epoch [60/500], Train Loss: 0.4166, Val Loss: 0.4432\n",
      "Epoch [70/500], Train Loss: 0.4099, Val Loss: 0.4370\n",
      "Epoch [80/500], Train Loss: 0.4038, Val Loss: 0.4321\n",
      "Epoch [90/500], Train Loss: 0.3962, Val Loss: 0.4307\n",
      "Epoch [100/500], Train Loss: 0.3944, Val Loss: 0.4286\n",
      "Early stopping at epoch 106\n",
      "Best f1_score for fold 3: 0.8435\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.6980, Val Loss: 0.6247\n",
      "Epoch [10/500], Train Loss: 0.5319, Val Loss: 0.5269\n",
      "Epoch [20/500], Train Loss: 0.4901, Val Loss: 0.4911\n",
      "Epoch [30/500], Train Loss: 0.4601, Val Loss: 0.4657\n",
      "Epoch [40/500], Train Loss: 0.4429, Val Loss: 0.4535\n",
      "Epoch [50/500], Train Loss: 0.4283, Val Loss: 0.4358\n",
      "Epoch [60/500], Train Loss: 0.4182, Val Loss: 0.4280\n",
      "Epoch [70/500], Train Loss: 0.4083, Val Loss: 0.4214\n",
      "Epoch [80/500], Train Loss: 0.4040, Val Loss: 0.4198\n",
      "Epoch [90/500], Train Loss: 0.3979, Val Loss: 0.4160\n",
      "Epoch [100/500], Train Loss: 0.3906, Val Loss: 0.4140\n",
      "Epoch [110/500], Train Loss: 0.3906, Val Loss: 0.4164\n",
      "Early stopping at epoch 110\n",
      "Best f1_score for fold 4: 0.8435\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7003, Val Loss: 0.6276\n",
      "Epoch [10/500], Train Loss: 0.5319, Val Loss: 0.5355\n",
      "Epoch [20/500], Train Loss: 0.4848, Val Loss: 0.4989\n",
      "Epoch [30/500], Train Loss: 0.4542, Val Loss: 0.4746\n",
      "Epoch [40/500], Train Loss: 0.4379, Val Loss: 0.4558\n",
      "Epoch [50/500], Train Loss: 0.4229, Val Loss: 0.4441\n",
      "Epoch [60/500], Train Loss: 0.4148, Val Loss: 0.4351\n",
      "Epoch [70/500], Train Loss: 0.4055, Val Loss: 0.4293\n",
      "Epoch [80/500], Train Loss: 0.4018, Val Loss: 0.4233\n",
      "Epoch [90/500], Train Loss: 0.3942, Val Loss: 0.4212\n",
      "Epoch [100/500], Train Loss: 0.3924, Val Loss: 0.4233\n",
      "Epoch [110/500], Train Loss: 0.3884, Val Loss: 0.4181\n",
      "Epoch [120/500], Train Loss: 0.3852, Val Loss: 0.4131\n",
      "Epoch [130/500], Train Loss: 0.3822, Val Loss: 0.4109\n",
      "Epoch [140/500], Train Loss: 0.3818, Val Loss: 0.4087\n",
      "Epoch [150/500], Train Loss: 0.3786, Val Loss: 0.4061\n",
      "Epoch [160/500], Train Loss: 0.3766, Val Loss: 0.4074\n",
      "Early stopping at epoch 166\n",
      "Best f1_score for fold 5: 0.8435\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.6955, Val Loss: 0.6219\n",
      "Epoch [10/500], Train Loss: 0.5296, Val Loss: 0.5223\n",
      "Epoch [20/500], Train Loss: 0.4830, Val Loss: 0.4884\n",
      "Epoch [30/500], Train Loss: 0.4532, Val Loss: 0.4642\n",
      "Epoch [40/500], Train Loss: 0.4351, Val Loss: 0.4444\n",
      "Epoch [50/500], Train Loss: 0.4216, Val Loss: 0.4352\n",
      "Epoch [60/500], Train Loss: 0.4114, Val Loss: 0.4252\n",
      "Epoch [70/500], Train Loss: 0.4033, Val Loss: 0.4191\n",
      "Epoch [80/500], Train Loss: 0.3997, Val Loss: 0.4159\n",
      "Epoch [90/500], Train Loss: 0.3981, Val Loss: 0.4138\n",
      "Epoch [100/500], Train Loss: 0.3927, Val Loss: 0.4063\n",
      "Epoch [110/500], Train Loss: 0.3890, Val Loss: 0.4065\n",
      "Epoch [120/500], Train Loss: 0.3856, Val Loss: 0.4036\n",
      "Epoch [130/500], Train Loss: 0.3840, Val Loss: 0.4049\n",
      "Epoch [140/500], Train Loss: 0.3807, Val Loss: 0.4006\n",
      "Early stopping at epoch 140\n",
      "Best f1_score for fold 6: 0.8460\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7005, Val Loss: 0.6200\n",
      "Epoch [10/500], Train Loss: 0.5286, Val Loss: 0.5218\n",
      "Epoch [20/500], Train Loss: 0.4828, Val Loss: 0.4933\n",
      "Epoch [30/500], Train Loss: 0.4532, Val Loss: 0.4673\n",
      "Epoch [40/500], Train Loss: 0.4383, Val Loss: 0.4545\n",
      "Epoch [50/500], Train Loss: 0.4225, Val Loss: 0.4455\n",
      "Epoch [60/500], Train Loss: 0.4142, Val Loss: 0.4345\n",
      "Epoch [70/500], Train Loss: 0.4060, Val Loss: 0.4333\n",
      "Epoch [80/500], Train Loss: 0.4002, Val Loss: 0.4255\n",
      "Epoch [90/500], Train Loss: 0.3956, Val Loss: 0.4222\n",
      "Early stopping at epoch 99\n",
      "Best f1_score for fold 7: 0.8460\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.6959, Val Loss: 0.6272\n",
      "Epoch [10/500], Train Loss: 0.5258, Val Loss: 0.5373\n",
      "Epoch [20/500], Train Loss: 0.4837, Val Loss: 0.4948\n",
      "Epoch [30/500], Train Loss: 0.4553, Val Loss: 0.4691\n",
      "Epoch [40/500], Train Loss: 0.4357, Val Loss: 0.4514\n",
      "Epoch [50/500], Train Loss: 0.4231, Val Loss: 0.4429\n",
      "Epoch [60/500], Train Loss: 0.4136, Val Loss: 0.4326\n",
      "Epoch [70/500], Train Loss: 0.4062, Val Loss: 0.4251\n",
      "Epoch [80/500], Train Loss: 0.4046, Val Loss: 0.4249\n",
      "Epoch [90/500], Train Loss: 0.3980, Val Loss: 0.4223\n",
      "Epoch [100/500], Train Loss: 0.3950, Val Loss: 0.4187\n",
      "Epoch [110/500], Train Loss: 0.3905, Val Loss: 0.4129\n",
      "Early stopping at epoch 117\n",
      "Best f1_score for fold 8: 0.8460\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.6962, Val Loss: 0.6385\n",
      "Epoch [10/500], Train Loss: 0.5306, Val Loss: 0.5379\n",
      "Epoch [20/500], Train Loss: 0.4861, Val Loss: 0.5004\n",
      "Epoch [30/500], Train Loss: 0.4589, Val Loss: 0.4752\n",
      "Epoch [40/500], Train Loss: 0.4373, Val Loss: 0.4604\n",
      "Epoch [50/500], Train Loss: 0.4251, Val Loss: 0.4506\n",
      "Epoch [60/500], Train Loss: 0.4155, Val Loss: 0.4400\n",
      "Epoch [70/500], Train Loss: 0.4086, Val Loss: 0.4328\n",
      "Epoch [80/500], Train Loss: 0.4010, Val Loss: 0.4239\n",
      "Epoch [90/500], Train Loss: 0.3958, Val Loss: 0.4240\n",
      "Epoch [100/500], Train Loss: 0.3945, Val Loss: 0.4213\n",
      "Early stopping at epoch 100\n",
      "Best f1_score for fold 9: 0.8460\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.6953, Val Loss: 0.6230\n",
      "Epoch [10/500], Train Loss: 0.5325, Val Loss: 0.5262\n",
      "Epoch [20/500], Train Loss: 0.4872, Val Loss: 0.4906\n",
      "Epoch [30/500], Train Loss: 0.4573, Val Loss: 0.4673\n",
      "Epoch [40/500], Train Loss: 0.4386, Val Loss: 0.4509\n",
      "Epoch [50/500], Train Loss: 0.4278, Val Loss: 0.4422\n",
      "Epoch [60/500], Train Loss: 0.4162, Val Loss: 0.4354\n",
      "Epoch [70/500], Train Loss: 0.4098, Val Loss: 0.4238\n",
      "Epoch [80/500], Train Loss: 0.4035, Val Loss: 0.4243\n",
      "Epoch [90/500], Train Loss: 0.3995, Val Loss: 0.4232\n",
      "Epoch [100/500], Train Loss: 0.3985, Val Loss: 0.4153\n",
      "Epoch [110/500], Train Loss: 0.3942, Val Loss: 0.4126\n",
      "Early stopping at epoch 110\n",
      "Best f1_score for fold 10: 0.8460\n",
      "Model state saved to ./models/mlp_best_state.pth\n",
      "Model state saved to ./models/mlp_best_ros_state.pth\n",
      "Model state saved to ./models/mlp_best_smote_state.pth\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "n_folds = 10\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "# MLP\n",
    "stratified_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_ros = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_smote = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print('MLP - original data')\n",
    "mlp_best_state, mlp_fold_results_list = find_best_model_state('MLP', X, y, input_dim=X.shape[1], num_classes=3, stratified_fold=stratified_fold, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "print('MLP - ROS data')\n",
    "mlp_best_ros_state, mlp_fold_results_list_ros = find_best_model_state('MLP', X_ros, y_ros, input_dim=X_ros.shape[1], num_classes=3, stratified_fold=stratified_fold_ros, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "print('MLP - SMOTE data')\n",
    "mlp_best_smote_state, mlp_fold_results_list_smote = find_best_model_state('MLP', X_smote, y_smote, input_dim=X_smote.shape[1], num_classes=3, stratified_fold=stratified_fold_smote, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "save_model_state(mlp_best_state, './models/mlp_best_state.pth')\n",
    "save_model_state(mlp_best_ros_state, './models/mlp_best_ros_state.pth')\n",
    "save_model_state(mlp_best_smote_state, './models/mlp_best_smote_state.pth')\n",
    "# Save csv\n",
    "mlp_fold_results_df = pd.DataFrame(mlp_fold_results_list).to_csv('./models/mlp_fold_results.csv', index=False)\n",
    "mlp_fold_results_df_ros = pd.DataFrame(mlp_fold_results_list_ros).to_csv('./models/mlp_fold_results_ros.csv', index=False)\n",
    "mlp_fold_results_df_smote = pd.DataFrame(mlp_fold_results_list_smote).to_csv('./models/mlp_fold_results_smote.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer - original data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7123, Val Loss: 0.6636\n",
      "Epoch [10/500], Train Loss: 0.6245, Val Loss: 0.6305\n",
      "Epoch [20/500], Train Loss: 0.5775, Val Loss: 0.6051\n",
      "Epoch [30/500], Train Loss: 0.5038, Val Loss: 0.5553\n",
      "Epoch [40/500], Train Loss: 0.4440, Val Loss: 0.5200\n",
      "Epoch [50/500], Train Loss: 0.3999, Val Loss: 0.5144\n",
      "Epoch [60/500], Train Loss: 0.3643, Val Loss: 0.5098\n",
      "Early stopping at epoch 61\n",
      "Best f1_score for fold 1: 0.7963\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7163, Val Loss: 0.6601\n",
      "Epoch [10/500], Train Loss: 0.6266, Val Loss: 0.6287\n",
      "Epoch [20/500], Train Loss: 0.5873, Val Loss: 0.6064\n",
      "Epoch [30/500], Train Loss: 0.5327, Val Loss: 0.5768\n",
      "Epoch [40/500], Train Loss: 0.4707, Val Loss: 0.5597\n",
      "Epoch [50/500], Train Loss: 0.4259, Val Loss: 0.5209\n",
      "Epoch [60/500], Train Loss: 0.3924, Val Loss: 0.5167\n",
      "Early stopping at epoch 63\n",
      "Best f1_score for fold 2: 0.7963\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7145, Val Loss: 0.6797\n",
      "Epoch [10/500], Train Loss: 0.6234, Val Loss: 0.6355\n",
      "Epoch [20/500], Train Loss: 0.5850, Val Loss: 0.6052\n",
      "Epoch [30/500], Train Loss: 0.5252, Val Loss: 0.5686\n",
      "Epoch [40/500], Train Loss: 0.4649, Val Loss: 0.5390\n",
      "Epoch [50/500], Train Loss: 0.4206, Val Loss: 0.5248\n",
      "Early stopping at epoch 58\n",
      "Best f1_score for fold 3: 0.7963\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7140, Val Loss: 0.6629\n",
      "Epoch [10/500], Train Loss: 0.6250, Val Loss: 0.6304\n",
      "Epoch [20/500], Train Loss: 0.5859, Val Loss: 0.6082\n",
      "Epoch [30/500], Train Loss: 0.5217, Val Loss: 0.5643\n",
      "Epoch [40/500], Train Loss: 0.4576, Val Loss: 0.5442\n",
      "Epoch [50/500], Train Loss: 0.4102, Val Loss: 0.5246\n",
      "Epoch [60/500], Train Loss: 0.3758, Val Loss: 0.5309\n",
      "Early stopping at epoch 68\n",
      "Best f1_score for fold 4: 0.7963\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7055, Val Loss: 0.6629\n",
      "Epoch [10/500], Train Loss: 0.6237, Val Loss: 0.6344\n",
      "Epoch [20/500], Train Loss: 0.5832, Val Loss: 0.6124\n",
      "Epoch [30/500], Train Loss: 0.5166, Val Loss: 0.5728\n",
      "Epoch [40/500], Train Loss: 0.4488, Val Loss: 0.5578\n",
      "Epoch [50/500], Train Loss: 0.4041, Val Loss: 0.5256\n",
      "Early stopping at epoch 59\n",
      "Best f1_score for fold 5: 0.7963\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7162, Val Loss: 0.6486\n",
      "Epoch [10/500], Train Loss: 0.6335, Val Loss: 0.6163\n",
      "Epoch [20/500], Train Loss: 0.6041, Val Loss: 0.5988\n",
      "Epoch [30/500], Train Loss: 0.5596, Val Loss: 0.5709\n",
      "Epoch [40/500], Train Loss: 0.5049, Val Loss: 0.5363\n",
      "Epoch [50/500], Train Loss: 0.4522, Val Loss: 0.5045\n",
      "Epoch [60/500], Train Loss: 0.4084, Val Loss: 0.5071\n",
      "Epoch [70/500], Train Loss: 0.3838, Val Loss: 0.5138\n",
      "Early stopping at epoch 75\n",
      "Best f1_score for fold 6: 0.7983\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7138, Val Loss: 0.6707\n",
      "Epoch [10/500], Train Loss: 0.6297, Val Loss: 0.6235\n",
      "Epoch [20/500], Train Loss: 0.5978, Val Loss: 0.6058\n",
      "Epoch [30/500], Train Loss: 0.5475, Val Loss: 0.5807\n",
      "Epoch [40/500], Train Loss: 0.4931, Val Loss: 0.5519\n",
      "Epoch [50/500], Train Loss: 0.4486, Val Loss: 0.5461\n",
      "Epoch [60/500], Train Loss: 0.4161, Val Loss: 0.5237\n",
      "Early stopping at epoch 62\n",
      "Best f1_score for fold 7: 0.7983\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7064, Val Loss: 0.6638\n",
      "Epoch [10/500], Train Loss: 0.6235, Val Loss: 0.6228\n",
      "Epoch [20/500], Train Loss: 0.5833, Val Loss: 0.6054\n",
      "Epoch [30/500], Train Loss: 0.5191, Val Loss: 0.5720\n",
      "Epoch [40/500], Train Loss: 0.4567, Val Loss: 0.5325\n",
      "Epoch [50/500], Train Loss: 0.4144, Val Loss: 0.5321\n",
      "Epoch [60/500], Train Loss: 0.3766, Val Loss: 0.5573\n",
      "Early stopping at epoch 60\n",
      "Best f1_score for fold 8: 0.7983\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7089, Val Loss: 0.6626\n",
      "Epoch [10/500], Train Loss: 0.6208, Val Loss: 0.6305\n",
      "Epoch [20/500], Train Loss: 0.5799, Val Loss: 0.6038\n",
      "Epoch [30/500], Train Loss: 0.5038, Val Loss: 0.5694\n",
      "Epoch [40/500], Train Loss: 0.4416, Val Loss: 0.5565\n",
      "Epoch [50/500], Train Loss: 0.3963, Val Loss: 0.5589\n",
      "Epoch [60/500], Train Loss: 0.3620, Val Loss: 0.5298\n",
      "Early stopping at epoch 62\n",
      "Best f1_score for fold 9: 0.7983\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7170, Val Loss: 0.6544\n",
      "Epoch [10/500], Train Loss: 0.6262, Val Loss: 0.6173\n",
      "Epoch [20/500], Train Loss: 0.5923, Val Loss: 0.6055\n",
      "Epoch [30/500], Train Loss: 0.5306, Val Loss: 0.5597\n",
      "Epoch [40/500], Train Loss: 0.4649, Val Loss: 0.5249\n",
      "Epoch [50/500], Train Loss: 0.4154, Val Loss: 0.5211\n",
      "Epoch [60/500], Train Loss: 0.3771, Val Loss: 0.5159\n",
      "Early stopping at epoch 62\n",
      "Best f1_score for fold 10: 0.7983\n",
      "Transformer - ROS data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.6957, Val Loss: 0.6461\n",
      "Epoch [10/500], Train Loss: 0.5619, Val Loss: 0.5534\n",
      "Epoch [20/500], Train Loss: 0.4372, Val Loss: 0.4269\n",
      "Epoch [30/500], Train Loss: 0.3734, Val Loss: 0.3705\n",
      "Epoch [40/500], Train Loss: 0.3314, Val Loss: 0.3356\n",
      "Epoch [50/500], Train Loss: 0.3017, Val Loss: 0.3285\n",
      "Epoch [60/500], Train Loss: 0.2812, Val Loss: 0.3215\n",
      "Epoch [70/500], Train Loss: 0.2644, Val Loss: 0.3196\n",
      "Early stopping at epoch 78\n",
      "Best f1_score for fold 1: 0.8966\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6978, Val Loss: 0.6542\n",
      "Epoch [10/500], Train Loss: 0.5543, Val Loss: 0.5384\n",
      "Epoch [20/500], Train Loss: 0.4236, Val Loss: 0.4172\n",
      "Epoch [30/500], Train Loss: 0.3574, Val Loss: 0.3706\n",
      "Epoch [40/500], Train Loss: 0.3153, Val Loss: 0.3536\n",
      "Epoch [50/500], Train Loss: 0.2900, Val Loss: 0.3474\n",
      "Epoch [60/500], Train Loss: 0.2682, Val Loss: 0.3480\n",
      "Epoch [70/500], Train Loss: 0.2477, Val Loss: 0.3412\n",
      "Early stopping at epoch 77\n",
      "Best f1_score for fold 2: 0.8966\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7054, Val Loss: 0.6480\n",
      "Epoch [10/500], Train Loss: 0.5727, Val Loss: 0.5556\n",
      "Epoch [20/500], Train Loss: 0.4547, Val Loss: 0.4395\n",
      "Epoch [30/500], Train Loss: 0.3885, Val Loss: 0.3958\n",
      "Epoch [40/500], Train Loss: 0.3445, Val Loss: 0.3607\n",
      "Epoch [50/500], Train Loss: 0.3129, Val Loss: 0.3488\n",
      "Epoch [60/500], Train Loss: 0.2910, Val Loss: 0.3441\n",
      "Epoch [70/500], Train Loss: 0.2724, Val Loss: 0.3457\n",
      "Epoch [80/500], Train Loss: 0.2570, Val Loss: 0.3376\n",
      "Early stopping at epoch 81\n",
      "Best f1_score for fold 3: 0.8966\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7003, Val Loss: 0.6514\n",
      "Epoch [10/500], Train Loss: 0.5740, Val Loss: 0.5523\n",
      "Epoch [20/500], Train Loss: 0.4589, Val Loss: 0.4373\n",
      "Epoch [30/500], Train Loss: 0.3868, Val Loss: 0.3799\n",
      "Epoch [40/500], Train Loss: 0.3474, Val Loss: 0.3500\n",
      "Epoch [50/500], Train Loss: 0.3154, Val Loss: 0.3388\n",
      "Early stopping at epoch 53\n",
      "Best f1_score for fold 4: 0.8966\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.6995, Val Loss: 0.6631\n",
      "Epoch [10/500], Train Loss: 0.5839, Val Loss: 0.5725\n",
      "Epoch [20/500], Train Loss: 0.4655, Val Loss: 0.4476\n",
      "Epoch [30/500], Train Loss: 0.3834, Val Loss: 0.3849\n",
      "Epoch [40/500], Train Loss: 0.3377, Val Loss: 0.3551\n",
      "Epoch [50/500], Train Loss: 0.3069, Val Loss: 0.3420\n",
      "Epoch [60/500], Train Loss: 0.2856, Val Loss: 0.3330\n",
      "Epoch [70/500], Train Loss: 0.2672, Val Loss: 0.3453\n",
      "Epoch [80/500], Train Loss: 0.2525, Val Loss: 0.3246\n",
      "Early stopping at epoch 83\n",
      "Best f1_score for fold 5: 0.8966\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.6985, Val Loss: 0.6480\n",
      "Epoch [10/500], Train Loss: 0.5628, Val Loss: 0.5405\n",
      "Epoch [20/500], Train Loss: 0.4275, Val Loss: 0.4022\n",
      "Epoch [30/500], Train Loss: 0.3599, Val Loss: 0.3499\n",
      "Epoch [40/500], Train Loss: 0.3196, Val Loss: 0.3309\n",
      "Epoch [50/500], Train Loss: 0.2926, Val Loss: 0.3142\n",
      "Epoch [60/500], Train Loss: 0.2712, Val Loss: 0.3135\n",
      "Epoch [70/500], Train Loss: 0.2568, Val Loss: 0.3041\n",
      "Early stopping at epoch 79\n",
      "Best f1_score for fold 6: 0.9045\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7030, Val Loss: 0.6705\n",
      "Epoch [10/500], Train Loss: 0.5979, Val Loss: 0.5854\n",
      "Epoch [20/500], Train Loss: 0.4999, Val Loss: 0.4807\n",
      "Epoch [30/500], Train Loss: 0.4159, Val Loss: 0.4063\n",
      "Epoch [40/500], Train Loss: 0.3638, Val Loss: 0.3674\n",
      "Epoch [50/500], Train Loss: 0.3302, Val Loss: 0.3479\n",
      "Epoch [60/500], Train Loss: 0.3099, Val Loss: 0.3346\n",
      "Epoch [70/500], Train Loss: 0.2888, Val Loss: 0.3361\n",
      "Epoch [80/500], Train Loss: 0.2731, Val Loss: 0.3267\n",
      "Early stopping at epoch 83\n",
      "Best f1_score for fold 7: 0.9045\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7024, Val Loss: 0.6440\n",
      "Epoch [10/500], Train Loss: 0.5599, Val Loss: 0.5321\n",
      "Epoch [20/500], Train Loss: 0.4388, Val Loss: 0.4121\n",
      "Epoch [30/500], Train Loss: 0.3684, Val Loss: 0.3588\n",
      "Epoch [40/500], Train Loss: 0.3288, Val Loss: 0.3361\n",
      "Epoch [50/500], Train Loss: 0.2942, Val Loss: 0.3147\n",
      "Epoch [60/500], Train Loss: 0.2773, Val Loss: 0.3094\n",
      "Epoch [70/500], Train Loss: 0.2585, Val Loss: 0.3081\n",
      "Early stopping at epoch 74\n",
      "Best f1_score for fold 8: 0.9045\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.6986, Val Loss: 0.6552\n",
      "Epoch [10/500], Train Loss: 0.5587, Val Loss: 0.5481\n",
      "Epoch [20/500], Train Loss: 0.4367, Val Loss: 0.4365\n",
      "Epoch [30/500], Train Loss: 0.3710, Val Loss: 0.3819\n",
      "Epoch [40/500], Train Loss: 0.3309, Val Loss: 0.3641\n",
      "Epoch [50/500], Train Loss: 0.3013, Val Loss: 0.3501\n",
      "Epoch [60/500], Train Loss: 0.2837, Val Loss: 0.3424\n",
      "Epoch [70/500], Train Loss: 0.2668, Val Loss: 0.3381\n",
      "Early stopping at epoch 78\n",
      "Best f1_score for fold 9: 0.9045\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.6959, Val Loss: 0.6490\n",
      "Epoch [10/500], Train Loss: 0.5665, Val Loss: 0.5554\n",
      "Epoch [20/500], Train Loss: 0.4451, Val Loss: 0.4207\n",
      "Epoch [30/500], Train Loss: 0.3741, Val Loss: 0.3648\n",
      "Epoch [40/500], Train Loss: 0.3296, Val Loss: 0.3345\n",
      "Epoch [50/500], Train Loss: 0.3011, Val Loss: 0.3407\n",
      "Epoch [60/500], Train Loss: 0.2789, Val Loss: 0.3269\n",
      "Early stopping at epoch 65\n",
      "Best f1_score for fold 10: 0.9045\n",
      "Transformer - SMOTE data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.6701, Val Loss: 0.5957\n",
      "Epoch [10/500], Train Loss: 0.4949, Val Loss: 0.4724\n",
      "Epoch [20/500], Train Loss: 0.3920, Val Loss: 0.3898\n",
      "Epoch [30/500], Train Loss: 0.3384, Val Loss: 0.3582\n",
      "Epoch [40/500], Train Loss: 0.3013, Val Loss: 0.3393\n",
      "Epoch [50/500], Train Loss: 0.2784, Val Loss: 0.3338\n",
      "Epoch [60/500], Train Loss: 0.2596, Val Loss: 0.3400\n",
      "Epoch [70/500], Train Loss: 0.2473, Val Loss: 0.3446\n",
      "Early stopping at epoch 73\n",
      "Best f1_score for fold 1: 0.8843\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6646, Val Loss: 0.5942\n",
      "Epoch [10/500], Train Loss: 0.5094, Val Loss: 0.5039\n",
      "Epoch [20/500], Train Loss: 0.4099, Val Loss: 0.4153\n",
      "Epoch [30/500], Train Loss: 0.3507, Val Loss: 0.3764\n",
      "Epoch [40/500], Train Loss: 0.3114, Val Loss: 0.3599\n",
      "Epoch [50/500], Train Loss: 0.2823, Val Loss: 0.3561\n",
      "Epoch [60/500], Train Loss: 0.2621, Val Loss: 0.3536\n",
      "Early stopping at epoch 67\n",
      "Best f1_score for fold 2: 0.8843\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.6723, Val Loss: 0.6244\n",
      "Epoch [10/500], Train Loss: 0.5034, Val Loss: 0.5023\n",
      "Epoch [20/500], Train Loss: 0.4081, Val Loss: 0.4203\n",
      "Epoch [30/500], Train Loss: 0.3509, Val Loss: 0.3844\n",
      "Epoch [40/500], Train Loss: 0.3160, Val Loss: 0.3613\n",
      "Epoch [50/500], Train Loss: 0.2910, Val Loss: 0.3700\n",
      "Epoch [60/500], Train Loss: 0.2726, Val Loss: 0.3631\n",
      "Epoch [70/500], Train Loss: 0.2573, Val Loss: 0.3591\n",
      "Early stopping at epoch 72\n",
      "Best f1_score for fold 3: 0.8843\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.6750, Val Loss: 0.6052\n",
      "Epoch [10/500], Train Loss: 0.5116, Val Loss: 0.4990\n",
      "Epoch [20/500], Train Loss: 0.4047, Val Loss: 0.4030\n",
      "Epoch [30/500], Train Loss: 0.3443, Val Loss: 0.3643\n",
      "Epoch [40/500], Train Loss: 0.3061, Val Loss: 0.3537\n",
      "Epoch [50/500], Train Loss: 0.2834, Val Loss: 0.3429\n",
      "Early stopping at epoch 59\n",
      "Best f1_score for fold 4: 0.8843\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.6657, Val Loss: 0.5830\n",
      "Epoch [10/500], Train Loss: 0.5041, Val Loss: 0.4927\n",
      "Epoch [20/500], Train Loss: 0.4070, Val Loss: 0.4139\n",
      "Epoch [30/500], Train Loss: 0.3497, Val Loss: 0.3712\n",
      "Epoch [40/500], Train Loss: 0.3143, Val Loss: 0.3642\n",
      "Early stopping at epoch 46\n",
      "Best f1_score for fold 5: 0.8843\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.6658, Val Loss: 0.5962\n",
      "Epoch [10/500], Train Loss: 0.5008, Val Loss: 0.4782\n",
      "Epoch [20/500], Train Loss: 0.4035, Val Loss: 0.3986\n",
      "Epoch [30/500], Train Loss: 0.3457, Val Loss: 0.3624\n",
      "Epoch [40/500], Train Loss: 0.3081, Val Loss: 0.3521\n",
      "Epoch [50/500], Train Loss: 0.2857, Val Loss: 0.3440\n",
      "Epoch [60/500], Train Loss: 0.2616, Val Loss: 0.3366\n",
      "Early stopping at epoch 68\n",
      "Best f1_score for fold 6: 0.8848\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.6658, Val Loss: 0.6007\n",
      "Epoch [10/500], Train Loss: 0.5137, Val Loss: 0.5064\n",
      "Epoch [20/500], Train Loss: 0.4204, Val Loss: 0.4221\n",
      "Epoch [30/500], Train Loss: 0.3613, Val Loss: 0.3790\n",
      "Epoch [40/500], Train Loss: 0.3219, Val Loss: 0.3642\n",
      "Epoch [50/500], Train Loss: 0.2928, Val Loss: 0.3581\n",
      "Epoch [60/500], Train Loss: 0.2741, Val Loss: 0.3625\n",
      "Epoch [70/500], Train Loss: 0.2516, Val Loss: 0.3757\n",
      "Early stopping at epoch 71\n",
      "Best f1_score for fold 7: 0.8848\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.6647, Val Loss: 0.6121\n",
      "Epoch [10/500], Train Loss: 0.5024, Val Loss: 0.4909\n",
      "Epoch [20/500], Train Loss: 0.4070, Val Loss: 0.4294\n",
      "Epoch [30/500], Train Loss: 0.3471, Val Loss: 0.3714\n",
      "Epoch [40/500], Train Loss: 0.3114, Val Loss: 0.3667\n",
      "Epoch [50/500], Train Loss: 0.2825, Val Loss: 0.3619\n",
      "Epoch [60/500], Train Loss: 0.2651, Val Loss: 0.3574\n",
      "Early stopping at epoch 63\n",
      "Best f1_score for fold 8: 0.8848\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.6788, Val Loss: 0.5993\n",
      "Epoch [10/500], Train Loss: 0.5230, Val Loss: 0.5228\n",
      "Epoch [20/500], Train Loss: 0.4530, Val Loss: 0.4555\n",
      "Epoch [30/500], Train Loss: 0.3977, Val Loss: 0.4193\n",
      "Epoch [40/500], Train Loss: 0.3575, Val Loss: 0.3841\n",
      "Epoch [50/500], Train Loss: 0.3283, Val Loss: 0.3655\n",
      "Epoch [60/500], Train Loss: 0.3086, Val Loss: 0.3583\n",
      "Early stopping at epoch 67\n",
      "Best f1_score for fold 9: 0.8848\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.6717, Val Loss: 0.6036\n",
      "Epoch [10/500], Train Loss: 0.5029, Val Loss: 0.4905\n",
      "Epoch [20/500], Train Loss: 0.4055, Val Loss: 0.4042\n",
      "Epoch [30/500], Train Loss: 0.3476, Val Loss: 0.3651\n",
      "Epoch [40/500], Train Loss: 0.3135, Val Loss: 0.3468\n",
      "Epoch [50/500], Train Loss: 0.2924, Val Loss: 0.3468\n",
      "Epoch [60/500], Train Loss: 0.2691, Val Loss: 0.3513\n",
      "Early stopping at epoch 60\n",
      "Best f1_score for fold 10: 0.8848\n",
      "Model state saved to ./models/transformer_best_state.pth\n",
      "Model state saved to ./models/transformer_best_ros_state.pth\n",
      "Model state saved to ./models/transformer_best_smote_state.pth\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "n_folds = 10\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "# Transformer\n",
    "stratified_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_ros = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_smote = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "print('Transformer - original data')\n",
    "transformer_best_state, transformer_fold_results_list = find_best_model_state('Transformer', X, y, input_dim=X.shape[1], num_classes=3, stratified_fold=stratified_fold, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "print('Transformer - ROS data')\n",
    "transformer_best_ros_state, transformer_fold_results_list_ros = find_best_model_state('Transformer', X_ros, y_ros, input_dim=X_ros.shape[1], num_classes=3, stratified_fold=stratified_fold_ros, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "print('Transformer - SMOTE data')\n",
    "transformer_best_smote_state, transformer_fold_results_list_smote = find_best_model_state('Transformer', X_smote, y_smote, input_dim=X_smote.shape[1], num_classes=3, stratified_fold=stratified_fold_smote, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device)\n",
    "save_model_state(transformer_best_state, './models/transformer_best_state.pth')\n",
    "save_model_state(transformer_best_ros_state, './models/transformer_best_ros_state.pth')\n",
    "save_model_state(transformer_best_smote_state, './models/transformer_best_smote_state.pth')\n",
    "# Save csv\n",
    "transformer_fold_results_df = pd.DataFrame(transformer_fold_results_list).to_csv('./models/transformer_fold_results.csv', index=False)\n",
    "transformer_fold_results_df_ros = pd.DataFrame(transformer_fold_results_list_ros).to_csv('./models/transformer_fold_results_ros.csv', index=False)\n",
    "transformer_fold_results_df_smote = pd.DataFrame(transformer_fold_results_list_smote).to_csv('./models/transformer_fold_results_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabTransformer - original data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7177, Val Loss: 0.6738\n",
      "Epoch [10/500], Train Loss: 0.5999, Val Loss: 0.6273\n",
      "Epoch [20/500], Train Loss: 0.2390, Val Loss: 0.7052\n",
      "Early stopping at epoch 24\n",
      "Best f1_score for fold 1: 0.7346\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7190, Val Loss: 0.6679\n",
      "Epoch [10/500], Train Loss: 0.5941, Val Loss: 0.6275\n",
      "Epoch [20/500], Train Loss: 0.2279, Val Loss: 0.7234\n",
      "Early stopping at epoch 24\n",
      "Best f1_score for fold 2: 0.7492\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7197, Val Loss: 0.6839\n",
      "Epoch [10/500], Train Loss: 0.6041, Val Loss: 0.6322\n",
      "Epoch [20/500], Train Loss: 0.2793, Val Loss: 0.7147\n",
      "Early stopping at epoch 24\n",
      "Best f1_score for fold 3: 0.7492\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7250, Val Loss: 0.6813\n",
      "Epoch [10/500], Train Loss: 0.5999, Val Loss: 0.6242\n",
      "Epoch [20/500], Train Loss: 0.2517, Val Loss: 0.7397\n",
      "Early stopping at epoch 24\n",
      "Best f1_score for fold 4: 0.7492\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7248, Val Loss: 0.6810\n",
      "Epoch [10/500], Train Loss: 0.5917, Val Loss: 0.6243\n",
      "Epoch [20/500], Train Loss: 0.2371, Val Loss: 0.7661\n",
      "Early stopping at epoch 24\n",
      "Best f1_score for fold 5: 0.7492\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7199, Val Loss: 0.6602\n",
      "Epoch [10/500], Train Loss: 0.6008, Val Loss: 0.6092\n",
      "Epoch [20/500], Train Loss: 0.2204, Val Loss: 0.7219\n",
      "Early stopping at epoch 23\n",
      "Best f1_score for fold 6: 0.7492\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7224, Val Loss: 0.6714\n",
      "Epoch [10/500], Train Loss: 0.6013, Val Loss: 0.6237\n",
      "Epoch [20/500], Train Loss: 0.2453, Val Loss: 0.7023\n",
      "Early stopping at epoch 25\n",
      "Best f1_score for fold 7: 0.7492\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7218, Val Loss: 0.6805\n",
      "Epoch [10/500], Train Loss: 0.6019, Val Loss: 0.6239\n",
      "Epoch [20/500], Train Loss: 0.2596, Val Loss: 0.7216\n",
      "Early stopping at epoch 22\n",
      "Best f1_score for fold 8: 0.7492\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7180, Val Loss: 0.7011\n",
      "Epoch [10/500], Train Loss: 0.5984, Val Loss: 0.6325\n",
      "Epoch [20/500], Train Loss: 0.2436, Val Loss: 0.7564\n",
      "Early stopping at epoch 23\n",
      "Best f1_score for fold 9: 0.7492\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7250, Val Loss: 0.6741\n",
      "Epoch [10/500], Train Loss: 0.5972, Val Loss: 0.6209\n",
      "Epoch [20/500], Train Loss: 0.2237, Val Loss: 0.7886\n",
      "Early stopping at epoch 23\n",
      "Best f1_score for fold 10: 0.7492\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7051, Val Loss: 0.6691\n",
      "Epoch [10/500], Train Loss: 0.2992, Val Loss: 0.3941\n",
      "Epoch [20/500], Train Loss: 0.0596, Val Loss: 0.5540\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 1: 0.8840\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7114, Val Loss: 0.6684\n",
      "Epoch [10/500], Train Loss: 0.3136, Val Loss: 0.4156\n",
      "Epoch [20/500], Train Loss: 0.0593, Val Loss: 0.5605\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 2: 0.8841\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7078, Val Loss: 0.6645\n",
      "Epoch [10/500], Train Loss: 0.2996, Val Loss: 0.4076\n",
      "Epoch [20/500], Train Loss: 0.0620, Val Loss: 0.5631\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 3: 0.8841\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7064, Val Loss: 0.6653\n",
      "Epoch [10/500], Train Loss: 0.3028, Val Loss: 0.3955\n",
      "Epoch [20/500], Train Loss: 0.0643, Val Loss: 0.5635\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 4: 0.8841\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7017, Val Loss: 0.6799\n",
      "Epoch [10/500], Train Loss: 0.3076, Val Loss: 0.3996\n",
      "Epoch [20/500], Train Loss: 0.0663, Val Loss: 0.5958\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 5: 0.8854\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7098, Val Loss: 0.6752\n",
      "Epoch [10/500], Train Loss: 0.3161, Val Loss: 0.3942\n",
      "Epoch [20/500], Train Loss: 0.0630, Val Loss: 0.5507\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 6: 0.8854\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7054, Val Loss: 0.6622\n",
      "Epoch [10/500], Train Loss: 0.3089, Val Loss: 0.4067\n",
      "Epoch [20/500], Train Loss: 0.0735, Val Loss: 0.5605\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 7: 0.8854\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7045, Val Loss: 0.6620\n",
      "Epoch [10/500], Train Loss: 0.3135, Val Loss: 0.3938\n",
      "Epoch [20/500], Train Loss: 0.0696, Val Loss: 0.5044\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 8: 0.8854\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7054, Val Loss: 0.6694\n",
      "Epoch [10/500], Train Loss: 0.3196, Val Loss: 0.4265\n",
      "Epoch [20/500], Train Loss: 0.0668, Val Loss: 0.5425\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 9: 0.8854\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7047, Val Loss: 0.6899\n",
      "Epoch [10/500], Train Loss: 0.3094, Val Loss: 0.4052\n",
      "Epoch [20/500], Train Loss: 0.0669, Val Loss: 0.5332\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 10: 0.8870\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.6811, Val Loss: 0.6352\n",
      "Epoch [10/500], Train Loss: 0.3433, Val Loss: 0.4347\n",
      "Early stopping at epoch 19\n",
      "Best f1_score for fold 1: 0.8474\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6796, Val Loss: 0.6498\n",
      "Epoch [10/500], Train Loss: 0.3554, Val Loss: 0.4431\n",
      "Epoch [20/500], Train Loss: 0.0929, Val Loss: 0.6982\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 2: 0.8474\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.6869, Val Loss: 0.6499\n",
      "Epoch [10/500], Train Loss: 0.3560, Val Loss: 0.4427\n",
      "Epoch [20/500], Train Loss: 0.0901, Val Loss: 0.6298\n",
      "Early stopping at epoch 22\n",
      "Best f1_score for fold 3: 0.8474\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.6808, Val Loss: 0.6261\n",
      "Epoch [10/500], Train Loss: 0.3497, Val Loss: 0.4444\n",
      "Epoch [20/500], Train Loss: 0.0826, Val Loss: 0.6082\n",
      "Early stopping at epoch 21\n",
      "Best f1_score for fold 4: 0.8488\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.6823, Val Loss: 0.6394\n",
      "Epoch [10/500], Train Loss: 0.3553, Val Loss: 0.4477\n",
      "Epoch [20/500], Train Loss: 0.0951, Val Loss: 0.6491\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 5: 0.8488\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.6822, Val Loss: 0.6557\n",
      "Epoch [10/500], Train Loss: 0.3302, Val Loss: 0.4401\n",
      "Epoch [20/500], Train Loss: 0.0797, Val Loss: 0.6552\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 6: 0.8488\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.6830, Val Loss: 0.6250\n",
      "Epoch [10/500], Train Loss: 0.3492, Val Loss: 0.4298\n",
      "Epoch [20/500], Train Loss: 0.0844, Val Loss: 0.6375\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 7: 0.8488\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.6790, Val Loss: 0.6564\n",
      "Epoch [10/500], Train Loss: 0.3433, Val Loss: 0.4461\n",
      "Epoch [20/500], Train Loss: 0.0877, Val Loss: 0.6498\n",
      "Early stopping at epoch 20\n",
      "Best f1_score for fold 8: 0.8488\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.6816, Val Loss: 0.6502\n",
      "Epoch [10/500], Train Loss: 0.3506, Val Loss: 0.4448\n",
      "Early stopping at epoch 19\n",
      "Best f1_score for fold 9: 0.8488\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.6833, Val Loss: 0.6395\n",
      "Epoch [10/500], Train Loss: 0.3487, Val Loss: 0.4365\n",
      "Epoch [20/500], Train Loss: 0.0869, Val Loss: 0.6458\n",
      "Early stopping at epoch 22\n",
      "Best f1_score for fold 10: 0.8524\n",
      "Model state saved to ./models/tab_transformer_best_state.pth\n",
      "Model state saved to ./models/tab_transformer_best_ros_state.pth\n",
      "Model state saved to ./models/tab_transformer_best_smote_state.pth\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "n_folds = 10\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "# TabTransformer\n",
    "stratified_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_ros = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_smote = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print('TabTransformer - original data')\n",
    "tab_transformer_best_state, tab_transformer_fold_results_list = find_best_model_state('TabTransformer', X, y, input_dim=X.shape[1], num_classes=3, stratified_fold=stratified_fold, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "tab_transformer_best_ros_state, tab_transformer_fold_results_list_ros = find_best_model_state('TabTransformer', X_ros, y_ros, input_dim=X_ros.shape[1], num_classes=3, stratified_fold=stratified_fold_ros, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "tab_transformer_best_smote_state, tab_transformer_fold_results_list_smote = find_best_model_state('TabTransformer', X_smote, y_smote, input_dim=X_smote.shape[1], num_classes=3, stratified_fold=stratified_fold_smote, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "save_model_state(tab_transformer_best_state, './models/tab_transformer_best_state.pth')\n",
    "save_model_state(tab_transformer_best_ros_state, './models/tab_transformer_best_ros_state.pth')\n",
    "save_model_state(tab_transformer_best_smote_state, './models/tab_transformer_best_smote_state.pth')\n",
    "# Save csv\n",
    "tab_transformer_fold_results_df = pd.DataFrame(tab_transformer_fold_results_list).to_csv('./models/tab_transformer_fold_results.csv', index=False)\n",
    "tab_transformer_fold_results_df_ros = pd.DataFrame(tab_transformer_fold_results_list_ros).to_csv('./models/tab_transformer_fold_results_ros.csv', index=False)\n",
    "tab_transformer_fold_results_df_smote = pd.DataFrame(tab_transformer_fold_results_list_smote).to_csv('./models/tab_transformer_fold_results_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTTransformer - original data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7190, Val Loss: 0.6755\n",
      "Epoch [10/500], Train Loss: 0.6264, Val Loss: 0.6249\n",
      "Epoch [20/500], Train Loss: 0.5990, Val Loss: 0.6119\n",
      "Epoch [30/500], Train Loss: 0.5581, Val Loss: 0.5866\n",
      "Epoch [40/500], Train Loss: 0.5139, Val Loss: 0.5537\n",
      "Epoch [50/500], Train Loss: 0.4755, Val Loss: 0.5247\n",
      "Epoch [60/500], Train Loss: 0.4429, Val Loss: 0.5116\n",
      "Epoch [70/500], Train Loss: 0.4179, Val Loss: 0.4994\n",
      "Epoch [80/500], Train Loss: 0.4009, Val Loss: 0.4984\n",
      "Epoch [90/500], Train Loss: 0.3848, Val Loss: 0.4794\n",
      "Early stopping at epoch 99\n",
      "Best f1_score for fold 1: 0.8098\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.7207, Val Loss: 0.6832\n",
      "Epoch [10/500], Train Loss: 0.6259, Val Loss: 0.6269\n",
      "Epoch [20/500], Train Loss: 0.5914, Val Loss: 0.5998\n",
      "Epoch [30/500], Train Loss: 0.5406, Val Loss: 0.5575\n",
      "Epoch [40/500], Train Loss: 0.4913, Val Loss: 0.5257\n",
      "Epoch [50/500], Train Loss: 0.4552, Val Loss: 0.5118\n",
      "Epoch [60/500], Train Loss: 0.4232, Val Loss: 0.5050\n",
      "Epoch [70/500], Train Loss: 0.3991, Val Loss: 0.4839\n",
      "Epoch [80/500], Train Loss: 0.3830, Val Loss: 0.4855\n",
      "Early stopping at epoch 81\n",
      "Best f1_score for fold 2: 0.8098\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7123, Val Loss: 0.6690\n",
      "Epoch [10/500], Train Loss: 0.6281, Val Loss: 0.6364\n",
      "Epoch [20/500], Train Loss: 0.6029, Val Loss: 0.6204\n",
      "Epoch [30/500], Train Loss: 0.5672, Val Loss: 0.5925\n",
      "Epoch [40/500], Train Loss: 0.5231, Val Loss: 0.5518\n",
      "Epoch [50/500], Train Loss: 0.4883, Val Loss: 0.5330\n",
      "Epoch [60/500], Train Loss: 0.4582, Val Loss: 0.5196\n",
      "Epoch [70/500], Train Loss: 0.4331, Val Loss: 0.5107\n",
      "Early stopping at epoch 78\n",
      "Best f1_score for fold 3: 0.8098\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7216, Val Loss: 0.6797\n",
      "Epoch [10/500], Train Loss: 0.6255, Val Loss: 0.6328\n",
      "Epoch [20/500], Train Loss: 0.6075, Val Loss: 0.6237\n",
      "Epoch [30/500], Train Loss: 0.5765, Val Loss: 0.5999\n",
      "Epoch [40/500], Train Loss: 0.5330, Val Loss: 0.5794\n",
      "Epoch [50/500], Train Loss: 0.4927, Val Loss: 0.5451\n",
      "Epoch [60/500], Train Loss: 0.4621, Val Loss: 0.5254\n",
      "Epoch [70/500], Train Loss: 0.4347, Val Loss: 0.5182\n",
      "Epoch [80/500], Train Loss: 0.4142, Val Loss: 0.5161\n",
      "Early stopping at epoch 81\n",
      "Best f1_score for fold 4: 0.8098\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7124, Val Loss: 0.6834\n",
      "Epoch [10/500], Train Loss: 0.6261, Val Loss: 0.6332\n",
      "Epoch [20/500], Train Loss: 0.6021, Val Loss: 0.6134\n",
      "Epoch [30/500], Train Loss: 0.5631, Val Loss: 0.5814\n",
      "Epoch [40/500], Train Loss: 0.5216, Val Loss: 0.5588\n",
      "Epoch [50/500], Train Loss: 0.4843, Val Loss: 0.5345\n",
      "Epoch [60/500], Train Loss: 0.4515, Val Loss: 0.5120\n",
      "Epoch [70/500], Train Loss: 0.4294, Val Loss: 0.5039\n",
      "Epoch [80/500], Train Loss: 0.4048, Val Loss: 0.4979\n",
      "Epoch [90/500], Train Loss: 0.3919, Val Loss: 0.4849\n",
      "Epoch [100/500], Train Loss: 0.3763, Val Loss: 0.4935\n",
      "Epoch [110/500], Train Loss: 0.3627, Val Loss: 0.4874\n",
      "Early stopping at epoch 115\n",
      "Best f1_score for fold 5: 0.8098\n",
      "Fold 6/10\n",
      "Epoch [1/500], Train Loss: 0.7167, Val Loss: 0.6630\n",
      "Epoch [10/500], Train Loss: 0.6269, Val Loss: 0.6141\n",
      "Epoch [20/500], Train Loss: 0.6006, Val Loss: 0.5988\n",
      "Epoch [30/500], Train Loss: 0.5671, Val Loss: 0.5720\n",
      "Epoch [40/500], Train Loss: 0.5284, Val Loss: 0.5460\n",
      "Epoch [50/500], Train Loss: 0.4922, Val Loss: 0.5310\n",
      "Epoch [60/500], Train Loss: 0.4619, Val Loss: 0.5076\n",
      "Epoch [70/500], Train Loss: 0.4333, Val Loss: 0.5057\n",
      "Epoch [80/500], Train Loss: 0.4157, Val Loss: 0.5059\n",
      "Epoch [90/500], Train Loss: 0.3968, Val Loss: 0.4881\n",
      "Epoch [100/500], Train Loss: 0.3831, Val Loss: 0.4838\n",
      "Early stopping at epoch 105\n",
      "Best f1_score for fold 6: 0.8098\n",
      "Fold 7/10\n",
      "Epoch [1/500], Train Loss: 0.7135, Val Loss: 0.6697\n",
      "Epoch [10/500], Train Loss: 0.6299, Val Loss: 0.6234\n",
      "Epoch [20/500], Train Loss: 0.6066, Val Loss: 0.6104\n",
      "Epoch [30/500], Train Loss: 0.5684, Val Loss: 0.5796\n",
      "Epoch [40/500], Train Loss: 0.5234, Val Loss: 0.5472\n",
      "Epoch [50/500], Train Loss: 0.4824, Val Loss: 0.5185\n",
      "Epoch [60/500], Train Loss: 0.4514, Val Loss: 0.5013\n",
      "Epoch [70/500], Train Loss: 0.4269, Val Loss: 0.5032\n",
      "Epoch [80/500], Train Loss: 0.4015, Val Loss: 0.4901\n",
      "Epoch [90/500], Train Loss: 0.3877, Val Loss: 0.4841\n",
      "Epoch [100/500], Train Loss: 0.3678, Val Loss: 0.4871\n",
      "Early stopping at epoch 100\n",
      "Best f1_score for fold 7: 0.8098\n",
      "Fold 8/10\n",
      "Epoch [1/500], Train Loss: 0.7153, Val Loss: 0.6680\n",
      "Epoch [10/500], Train Loss: 0.6266, Val Loss: 0.6247\n",
      "Epoch [20/500], Train Loss: 0.6035, Val Loss: 0.6137\n",
      "Epoch [30/500], Train Loss: 0.5682, Val Loss: 0.5911\n",
      "Epoch [40/500], Train Loss: 0.5245, Val Loss: 0.5660\n",
      "Epoch [50/500], Train Loss: 0.4822, Val Loss: 0.5299\n",
      "Epoch [60/500], Train Loss: 0.4539, Val Loss: 0.5106\n",
      "Epoch [70/500], Train Loss: 0.4280, Val Loss: 0.5083\n",
      "Epoch [80/500], Train Loss: 0.4030, Val Loss: 0.4935\n",
      "Epoch [90/500], Train Loss: 0.3926, Val Loss: 0.4875\n",
      "Epoch [100/500], Train Loss: 0.3751, Val Loss: 0.4786\n",
      "Early stopping at epoch 109\n",
      "Best f1_score for fold 8: 0.8098\n",
      "Fold 9/10\n",
      "Epoch [1/500], Train Loss: 0.7213, Val Loss: 0.6872\n",
      "Epoch [10/500], Train Loss: 0.6257, Val Loss: 0.6354\n",
      "Epoch [20/500], Train Loss: 0.5980, Val Loss: 0.6181\n",
      "Epoch [30/500], Train Loss: 0.5547, Val Loss: 0.5932\n",
      "Epoch [40/500], Train Loss: 0.5138, Val Loss: 0.5581\n",
      "Epoch [50/500], Train Loss: 0.4770, Val Loss: 0.5337\n",
      "Epoch [60/500], Train Loss: 0.4454, Val Loss: 0.5253\n",
      "Epoch [70/500], Train Loss: 0.4221, Val Loss: 0.5157\n",
      "Epoch [80/500], Train Loss: 0.3998, Val Loss: 0.5055\n",
      "Epoch [90/500], Train Loss: 0.3820, Val Loss: 0.5053\n",
      "Epoch [100/500], Train Loss: 0.3681, Val Loss: 0.5094\n",
      "Early stopping at epoch 102\n",
      "Best f1_score for fold 9: 0.8098\n",
      "Fold 10/10\n",
      "Epoch [1/500], Train Loss: 0.7232, Val Loss: 0.6908\n",
      "Epoch [10/500], Train Loss: 0.6308, Val Loss: 0.6280\n",
      "Epoch [20/500], Train Loss: 0.6104, Val Loss: 0.6125\n",
      "Epoch [30/500], Train Loss: 0.5823, Val Loss: 0.5982\n",
      "Epoch [40/500], Train Loss: 0.5454, Val Loss: 0.5801\n",
      "Epoch [50/500], Train Loss: 0.5094, Val Loss: 0.5610\n",
      "Epoch [60/500], Train Loss: 0.4755, Val Loss: 0.5418\n",
      "Epoch [70/500], Train Loss: 0.4483, Val Loss: 0.5298\n",
      "Epoch [80/500], Train Loss: 0.4229, Val Loss: 0.5133\n",
      "Epoch [90/500], Train Loss: 0.4024, Val Loss: 0.5228\n",
      "Early stopping at epoch 90\n",
      "Best f1_score for fold 10: 0.8098\n",
      "FTTransformer - ROS data\n",
      "Fold 1/10\n",
      "Epoch [1/500], Train Loss: 0.7102, Val Loss: 0.6551\n",
      "Epoch [10/500], Train Loss: 0.6003, Val Loss: 0.5904\n",
      "Epoch [20/500], Train Loss: 0.5066, Val Loss: 0.4789\n",
      "Epoch [30/500], Train Loss: 0.4400, Val Loss: 0.4084\n",
      "Epoch [40/500], Train Loss: 0.3992, Val Loss: 0.3702\n",
      "Epoch [50/500], Train Loss: 0.3707, Val Loss: 0.3589\n",
      "Epoch [60/500], Train Loss: 0.3477, Val Loss: 0.3412\n",
      "Epoch [70/500], Train Loss: 0.3320, Val Loss: 0.3412\n",
      "Epoch [80/500], Train Loss: 0.3162, Val Loss: 0.3221\n",
      "Epoch [90/500], Train Loss: 0.3051, Val Loss: 0.3296\n",
      "Epoch [100/500], Train Loss: 0.2928, Val Loss: 0.3241\n",
      "Early stopping at epoch 107\n",
      "Best f1_score for fold 1: 0.8896\n",
      "Fold 2/10\n",
      "Epoch [1/500], Train Loss: 0.6940, Val Loss: 0.6524\n",
      "Epoch [10/500], Train Loss: 0.5986, Val Loss: 0.5923\n",
      "Epoch [20/500], Train Loss: 0.5085, Val Loss: 0.5003\n",
      "Epoch [30/500], Train Loss: 0.4378, Val Loss: 0.4228\n",
      "Epoch [40/500], Train Loss: 0.3957, Val Loss: 0.3923\n",
      "Epoch [50/500], Train Loss: 0.3635, Val Loss: 0.3667\n",
      "Epoch [60/500], Train Loss: 0.3432, Val Loss: 0.3495\n",
      "Epoch [70/500], Train Loss: 0.3267, Val Loss: 0.3396\n",
      "Epoch [80/500], Train Loss: 0.3116, Val Loss: 0.3375\n",
      "Early stopping at epoch 84\n",
      "Best f1_score for fold 2: 0.8896\n",
      "Fold 3/10\n",
      "Epoch [1/500], Train Loss: 0.7070, Val Loss: 0.6480\n",
      "Epoch [10/500], Train Loss: 0.6012, Val Loss: 0.5879\n",
      "Epoch [20/500], Train Loss: 0.5106, Val Loss: 0.4966\n",
      "Epoch [30/500], Train Loss: 0.4384, Val Loss: 0.4314\n",
      "Epoch [40/500], Train Loss: 0.3953, Val Loss: 0.3839\n",
      "Epoch [50/500], Train Loss: 0.3654, Val Loss: 0.3627\n",
      "Epoch [60/500], Train Loss: 0.3444, Val Loss: 0.3506\n",
      "Epoch [70/500], Train Loss: 0.3226, Val Loss: 0.3456\n",
      "Epoch [80/500], Train Loss: 0.3127, Val Loss: 0.3397\n",
      "Epoch [90/500], Train Loss: 0.2979, Val Loss: 0.3382\n",
      "Epoch [100/500], Train Loss: 0.2836, Val Loss: 0.3329\n",
      "Early stopping at epoch 105\n",
      "Best f1_score for fold 3: 0.8896\n",
      "Fold 4/10\n",
      "Epoch [1/500], Train Loss: 0.7064, Val Loss: 0.6491\n",
      "Epoch [10/500], Train Loss: 0.5964, Val Loss: 0.5832\n",
      "Epoch [20/500], Train Loss: 0.5146, Val Loss: 0.4874\n",
      "Epoch [30/500], Train Loss: 0.4437, Val Loss: 0.4035\n",
      "Epoch [40/500], Train Loss: 0.4014, Val Loss: 0.3661\n",
      "Epoch [50/500], Train Loss: 0.3709, Val Loss: 0.3567\n",
      "Epoch [60/500], Train Loss: 0.3474, Val Loss: 0.3306\n",
      "Epoch [70/500], Train Loss: 0.3291, Val Loss: 0.3219\n",
      "Epoch [80/500], Train Loss: 0.3123, Val Loss: 0.3127\n",
      "Epoch [90/500], Train Loss: 0.3015, Val Loss: 0.3063\n",
      "Epoch [100/500], Train Loss: 0.2928, Val Loss: 0.2996\n",
      "Early stopping at epoch 102\n",
      "Best f1_score for fold 4: 0.8946\n",
      "Fold 5/10\n",
      "Epoch [1/500], Train Loss: 0.7006, Val Loss: 0.6608\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "n_folds = 10\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "# FTTransformer\n",
    "stratified_fold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_ros = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "stratified_fold_smote = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print('FTTransformer - original data')\n",
    "ft_transformer_best_state, ft_transformer_fold_results_list = find_best_model_state('FTTransformer', X, y, input_dim=X.shape[1], num_classes=3, stratified_fold=stratified_fold, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "print('FTTransformer - ROS data')\n",
    "ft_transformer_best_ros_state, ft_transformer_fold_results_list_ros = find_best_model_state('FTTransformer', X_ros, y_ros, input_dim=X_ros.shape[1], num_classes=3, stratified_fold=stratified_fold_ros, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "print('FTTransformer - SMOTE data')\n",
    "ft_transformer_best_smote_state, ft_transformer_fold_results_list_smote = find_best_model_state('FTTransformer', X_smote, y_smote, input_dim=X_smote.shape[1], num_classes=3, stratified_fold=stratified_fold_smote, batch_size=batch_size, num_epochs=num_epochs, n_folds=n_folds, device=device, cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs)\n",
    "save_model_state(ft_transformer_best_state, './models/ft_transformer_best_state.pth')\n",
    "save_model_state(ft_transformer_best_ros_state, './models/ft_transformer_best_ros_state.pth')\n",
    "save_model_state(ft_transformer_best_smote_state, './models/ft_transformer_best_smote_state.pth')\n",
    "# Save csv\n",
    "ft_transformer_fold_results_df = pd.DataFrame(ft_transformer_fold_results_list).to_csv('./models/ft_transformer_fold_results.csv', index=False)\n",
    "ft_transformer_fold_results_df_ros = pd.DataFrame(ft_transformer_fold_results_list_ros).to_csv('./models/ft_transformer_fold_results_ros.csv', index=False)\n",
    "ft_transformer_fold_results_df_smote = pd.DataFrame(ft_transformer_fold_results_list_smote).to_csv('./models/ft_transformer_fold_results_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Fold",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "roc_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "attention_weights",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e6e596b5-150c-4b43-877c-832bdc285e58",
       "rows": [
        [
         "0",
         "1",
         "TabTransformer",
         "0.8090014064697609",
         "0.7944424402099344",
         "0.8081123137581265",
         "0.8005803527011549",
         "0.9243691599757448",
         "[[[6.5461518e-03 9.0716593e-03 2.0918595e-03 ... 1.4397101e-03\n   4.4229357e-03 3.3023365e-02]\n  [3.3903611e-04 2.2887934e-06 1.4345066e-04 ... 1.2638579e-03\n   6.2846002e-04 3.9247092e-04]\n  [3.7654368e-03 5.4482627e-03 3.1341094e-04 ... 1.4212193e-02\n   6.1501446e-04 7.0777750e-03]\n  ...\n  [2.4294192e-02 1.0019088e-02 5.2534901e-03 ... 5.0908560e-03\n   3.1752989e-03 1.8167241e-02]\n  [4.1783233e-03 7.6695024e-03 5.5611534e-03 ... 5.5213356e-03\n   3.5111722e-02 1.9667726e-03]\n  [1.4603158e-02 1.5550227e-02 3.8727659e-03 ... 2.4467376e-03\n   4.1816672e-04 3.1706817e-02]]\n\n [[1.3456484e-02 9.4998911e-02 4.3330421e-03 ... 4.9144076e-03\n   3.3720573e-03 2.5901048e-02]\n  [1.1168535e-02 5.6665604e-06 4.8688253e-05 ... 6.9056422e-04\n   3.1675174e-04 8.5023986e-03]\n  [9.1284059e-04 2.0650527e-02 8.8881049e-04 ... 6.2337919e-04\n   3.4436083e-03 1.4320461e-03]\n  ...\n  [3.4473937e-02 8.4739417e-02 1.2728471e-02 ... 3.2725781e-03\n   6.2386463e-03 1.6450835e-02]\n  [5.9764395e-03 1.0337351e-03 2.0826144e-02 ... 2.6758155e-03\n   4.0749800e-03 3.1643279e-03]\n  [2.5768096e-02 7.4899860e-02 5.9566917e-03 ... 6.1328900e-03\n   2.3238137e-03 4.0884234e-02]]\n\n [[2.3961710e-03 6.7652698e-04 2.6067675e-03 ... 1.6149019e-03\n   6.1447907e-04 2.5597429e-03]\n  [1.5497073e-05 8.1664912e-04 2.6517532e-06 ... 1.4327676e-05\n   4.1334453e-05 1.0314359e-05]\n  [9.1966828e-03 7.3405183e-03 8.6377542e-03 ... 1.3891842e-02\n   6.0210582e-03 1.8610438e-02]\n  ...\n  [7.6516396e-03 8.7353820e-04 8.2481466e-03 ... 6.0509052e-03\n   6.4012426e-04 7.8945728e-03]\n  [3.4103715e-03 7.6583311e-02 1.6991274e-03 ... 1.9079716e-03\n   5.8933767e-03 2.7913076e-03]\n  [7.8079919e-03 7.5241219e-04 9.1946637e-03 ... 6.8541891e-03\n   4.7141183e-04 9.0389019e-03]]\n\n ...\n\n [[1.5671031e-03 5.5112150e-03 1.3374268e-03 ... 2.4480079e-03\n   1.7458029e-02 5.4630437e-03]\n  [8.7698596e-04 2.0019975e-03 2.8072945e-03 ... 9.0867328e-03\n   1.4150337e-03 7.6284497e-03]\n  [2.2472329e-03 8.0133928e-03 5.5235885e-03 ... 8.7722078e-02\n   1.1106228e-04 3.6438915e-03]\n  ...\n  [4.6558040e-03 9.0208147e-03 7.9285697e-04 ... 2.4044728e-03\n   2.5512292e-03 7.1238965e-04]\n  [1.5873050e-02 2.0012332e-02 1.0249342e-02 ... 2.5576069e-03\n   1.6901039e-03 8.7079052e-03]\n  [7.2098803e-04 1.8218066e-01 1.5372502e-02 ... 1.5278585e-02\n   5.1843899e-04 7.0398767e-03]]\n\n [[2.0141301e-03 7.4410819e-02 8.8674884e-04 ... 1.9962704e-04\n   7.8114886e-03 4.8424670e-04]\n  [2.0598851e-03 2.0339678e-03 1.3191226e-03 ... 1.6289098e-04\n   2.1456584e-04 3.8518687e-04]\n  [7.8003369e-02 2.3613133e-02 1.6193315e-03 ... 2.1578581e-03\n   1.1295387e-02 2.0634311e-03]\n  ...\n  [3.1585651e-03 2.0754942e-01 1.2644938e-03 ... 4.1432809e-03\n   3.9574830e-03 2.9366401e-03]\n  [6.4335582e-03 1.2559591e-01 1.0568718e-03 ... 2.9574640e-04\n   6.4030889e-04 4.4121206e-04]\n  [4.8472569e-04 6.1184140e-03 3.3355318e-05 ... 2.0350206e-04\n   2.3982264e-04 6.0183120e-05]]\n\n [[1.0323783e-02 1.3613012e-01 6.6412743e-03 ... 5.5039688e-03\n   1.9224875e-02 7.5731045e-03]\n  [4.1998257e-03 2.7227479e-05 1.7249002e-03 ... 6.3374517e-03\n   2.9932085e-02 6.8553942e-03]\n  [1.3430173e-02 2.1498062e-02 5.0423397e-03 ... 1.3980752e-02\n   8.8700339e-02 1.5747890e-02]\n  ...\n  [2.1302920e-02 2.3419080e-02 8.6433683e-03 ... 1.2935850e-02\n   4.2106900e-02 1.5118029e-02]\n  [8.0757542e-04 1.2118854e-03 2.1785131e-04 ... 5.2551576e-04\n   3.4628254e-03 4.9847976e-04]\n  [2.0596109e-02 2.1999795e-02 1.0045091e-02 ... 1.3580642e-02\n   3.9093148e-02 1.5987650e-02]]]"
        ],
        [
         "1",
         "2",
         "TabTransformer",
         "0.8082981715893108",
         "0.796922734312111",
         "0.8097081460857494",
         "0.8025608232529372",
         "0.9250417071699285",
         "[[[0.01070856 0.01570933 0.02962457 ... 0.03110477 0.00348366 0.00405451]\n  [0.04287575 0.0044206  0.00786578 ... 0.00511246 0.00550356 0.00867067]\n  [0.00953528 0.01573977 0.01220571 ... 0.01583681 0.01394436 0.00954169]\n  ...\n  [0.00670771 0.00416731 0.0131838  ... 0.00201458 0.00134343 0.00761978]\n  [0.02598053 0.00373009 0.03573281 ... 0.01266937 0.0073155  0.03511029]\n  [0.00776875 0.00172203 0.00687345 ... 0.00310022 0.00761625 0.02466499]]\n\n [[0.00121751 0.00125462 0.01915438 ... 0.01753582 0.01379372 0.00127214]\n  [0.00120204 0.00132146 0.01281856 ... 0.0184348  0.0171802  0.00136497]\n  [0.00469496 0.00495986 0.00681842 ... 0.03107028 0.0298307  0.00560918]\n  ...\n  [0.01201966 0.01107104 0.052991   ... 0.00254265 0.00184883 0.02872701]\n  [0.01262969 0.01536755 0.0585617  ... 0.02125615 0.00890802 0.02310703]\n  [0.00125277 0.00248394 0.00543253 ... 0.02244146 0.06093549 0.00159945]]\n\n [[0.00269789 0.01902491 0.00436947 ... 0.00148171 0.00259095 0.00167721]\n  [0.02060416 0.00101839 0.0096877  ... 0.01915441 0.00480231 0.01757038]\n  [0.0276798  0.00473747 0.0011263  ... 0.00048659 0.00186146 0.00098846]\n  ...\n  [0.01873847 0.01881361 0.0290086  ... 0.01324849 0.00602605 0.00114318]\n  [0.00683481 0.01846575 0.00590314 ... 0.0177448  0.00318526 0.03411015]\n  [0.00180862 0.03503298 0.01749496 ... 0.00128908 0.00562998 0.01456647]]\n\n ...\n\n [[0.00610409 0.07079532 0.0030155  ... 0.00909766 0.00802412 0.00654619]\n  [0.0146417  0.01201545 0.00061953 ... 0.01797599 0.00479901 0.02105506]\n  [0.01353115 0.04993349 0.00364605 ... 0.00503029 0.00558575 0.0426151 ]\n  ...\n  [0.0111072  0.13190123 0.01152124 ... 0.0093407  0.00183493 0.00334079]\n  [0.00547292 0.1248326  0.0119741  ... 0.0021663  0.0015731  0.00653007]\n  [0.01482741 0.00310997 0.00112356 ... 0.00132336 0.00781462 0.01385746]]\n\n [[0.00285498 0.00166874 0.04204653 ... 0.00082192 0.00181487 0.00292047]\n  [0.00471243 0.00368747 0.03777037 ... 0.00115561 0.00296558 0.00435333]\n  [0.00353962 0.00331584 0.00277366 ... 0.00109429 0.03067622 0.00532437]\n  ...\n  [0.00044647 0.00028373 0.0063658  ... 0.00239256 0.00245524 0.00205356]\n  [0.00121612 0.00110678 0.0305978  ... 0.00140857 0.00354365 0.00065925]\n  [0.00137862 0.00076449 0.00089192 ... 0.00353694 0.00292656 0.00204666]]\n\n [[0.00282003 0.02035179 0.04425331 ... 0.00733792 0.00072332 0.00223338]\n  [0.01489977 0.0024467  0.00444974 ... 0.00208795 0.02638913 0.01454937]\n  [0.01284704 0.00411659 0.01042774 ... 0.00541074 0.00763904 0.00915773]\n  ...\n  [0.0028864  0.01824872 0.00783207 ... 0.0022792  0.00287785 0.00439738]\n  [0.00333166 0.00059044 0.00059965 ... 0.00149528 0.00038477 0.01753263]\n  [0.00430142 0.00326236 0.00013279 ... 0.00319488 0.00094713 0.00601246]]]"
        ],
        [
         "2",
         "3",
         "TabTransformer",
         "0.7866385372714486",
         "0.7687045009280191",
         "0.7881168274424151",
         "0.7771928178285208",
         "0.9155472826907145",
         "[[[8.27945332e-05 7.37429038e-02 7.97927263e-04 ... 4.81862633e-04\n   4.76921923e-05 2.85031681e-04]\n  [3.07855589e-05 4.49885894e-03 1.97201825e-05 ... 1.36288931e-04\n   4.84000938e-03 3.03817185e-04]\n  [2.36932799e-04 1.55027676e-02 2.27647746e-04 ... 5.03482064e-04\n   3.25875031e-03 4.53935354e-04]\n  ...\n  [9.71674399e-06 4.56924923e-03 7.39657989e-05 ... 5.16227374e-05\n   8.35718587e-03 1.24834754e-04]\n  [3.86411884e-05 3.06846388e-03 3.34094511e-04 ... 3.87070293e-04\n   1.81618426e-03 8.72259901e-04]\n  [9.49601781e-06 1.77058145e-01 1.72448374e-04 ... 5.63021276e-05\n   8.45256203e-04 7.35367621e-06]]\n\n [[9.16018053e-06 2.44901003e-06 1.64327852e-03 ... 7.86383953e-06\n   2.36613763e-04 1.10341098e-04]\n  [1.80728421e-05 6.94292282e-07 8.09955411e-03 ... 1.07478320e-06\n   1.77007387e-04 6.64532272e-05]\n  [2.00160639e-03 2.83872680e-04 4.02717665e-03 ... 1.98900211e-03\n   4.26894845e-03 2.10521067e-03]\n  ...\n  [4.40392829e-03 9.23998246e-04 7.97807984e-03 ... 1.98036694e-04\n   7.41680805e-03 9.63999797e-03]\n  [2.47976929e-03 9.77697875e-03 2.51001120e-02 ... 1.27332984e-02\n   2.04946846e-02 1.17094128e-03]\n  [1.02854343e-02 1.13059133e-02 1.02075329e-02 ... 3.09504848e-03\n   1.12606073e-03 8.47815711e-04]]\n\n [[3.06909671e-04 1.80234257e-02 8.21556523e-03 ... 3.81692243e-03\n   2.58542597e-03 2.89256545e-03]\n  [7.30628904e-04 1.96428164e-06 2.57315161e-03 ... 3.91520560e-04\n   2.45582651e-05 1.20251614e-03]\n  [7.17497105e-03 2.64464237e-04 2.51001865e-03 ... 3.26300063e-03\n   1.19433796e-03 7.55421643e-04]\n  ...\n  [2.48557376e-03 2.54508364e-03 1.92336098e-03 ... 6.88798632e-03\n   2.43920437e-03 9.57211033e-02]\n  [8.66331719e-03 5.02318423e-03 7.22848170e-04 ... 7.40831485e-03\n   1.32856565e-03 4.63156076e-03]\n  [2.40177568e-03 8.38423986e-03 5.50574437e-03 ... 7.29838200e-03\n   4.23240056e-03 1.01523891e-01]]\n\n ...\n\n [[1.10570816e-02 2.54434831e-02 1.54347727e-02 ... 6.48703938e-03\n   5.95463905e-03 4.05946700e-03]\n  [8.45105315e-05 3.09274878e-06 1.67734176e-01 ... 9.48797259e-03\n   6.24248246e-03 4.77322028e-05]\n  [6.13418408e-03 4.62258272e-02 3.84846493e-03 ... 7.14596547e-03\n   3.28084966e-03 3.86714819e-03]\n  ...\n  [8.44763033e-03 2.04901658e-02 7.36189038e-02 ... 6.28957199e-03\n   1.08625367e-02 1.62955970e-02]\n  [4.77027002e-04 3.45512815e-02 1.89599872e-03 ... 1.71804707e-03\n   4.06885752e-03 3.86620406e-04]\n  [9.98239149e-04 1.02291293e-02 8.59568454e-03 ... 7.06220511e-04\n   3.08899558e-03 3.30966635e-04]]\n\n [[2.15540167e-05 9.95889539e-04 3.12717236e-03 ... 4.96053428e-04\n   1.68986700e-03 2.10493108e-05]\n  [4.27451311e-03 1.05290803e-06 9.98117961e-03 ... 1.36502704e-05\n   9.64476130e-05 9.37084405e-05]\n  [1.03818243e-02 2.29129964e-03 2.47664750e-03 ... 5.23945596e-03\n   3.00189108e-03 4.64579382e-04]\n  ...\n  [1.22675495e-02 5.42348833e-04 1.67040003e-03 ... 1.33884314e-03\n   1.92789163e-03 5.88989758e-04]\n  [3.08016641e-03 9.77751799e-04 6.70721056e-04 ... 9.45243635e-04\n   1.00421673e-03 1.48458377e-04]\n  [5.77470614e-03 4.30543348e-03 9.24037769e-03 ... 2.44382629e-03\n   3.06247710e-03 2.65736785e-02]]\n\n [[1.90256687e-04 2.41087720e-01 2.28672568e-03 ... 7.70317158e-03\n   3.87422740e-02 6.17873302e-05]\n  [1.71389070e-03 6.13709759e-08 4.25466755e-03 ... 7.92654348e-04\n   3.22568594e-05 1.51368016e-02]\n  [2.35387757e-02 2.53032730e-03 2.68311938e-03 ... 1.73339918e-01\n   2.36517008e-04 1.17074940e-02]\n  ...\n  [7.56329857e-03 1.47336824e-02 9.32714716e-03 ... 6.48663789e-02\n   8.55942890e-02 6.72289077e-03]\n  [1.34630082e-03 4.18946693e-05 2.64889747e-02 ... 1.11651599e-01\n   9.10598974e-05 5.72202774e-03]\n  [8.62082280e-03 3.17874551e-02 1.19093368e-02 ... 2.19395999e-02\n   7.98191977e-05 5.39151765e-03]]]"
        ],
        [
         "3",
         "4",
         "TabTransformer",
         "0.8081575246132208",
         "0.7932287289731786",
         "0.8102841532108188",
         "0.8005644901401388",
         "0.9236162859983281",
         "[[[4.6162889e-03 4.7474368e-03 1.3187103e-02 ... 2.3410497e-03\n   1.3585285e-03 1.2061538e-03]\n  [6.8632606e-03 1.9489359e-02 6.2905448e-03 ... 9.8768324e-03\n   2.7398835e-03 1.2540498e-03]\n  [6.5493240e-04 4.0134482e-04 3.3073564e-04 ... 4.9529289e-04\n   7.0357535e-05 9.9517861e-03]\n  ...\n  [5.4403918e-04 7.7595911e-03 1.3833619e-03 ... 4.1628148e-02\n   4.0608129e-04 2.1765074e-03]\n  [8.6338669e-03 4.6549030e-03 3.5989243e-03 ... 2.7321749e-03\n   8.6277322e-04 1.6506445e-02]\n  [2.2865140e-03 1.3830632e-03 2.3240282e-03 ... 2.9970526e-03\n   6.4369931e-05 1.1403330e-03]]\n\n [[1.2756893e-02 2.7035762e-02 6.0767047e-03 ... 1.3229360e-02\n   1.4349006e-03 2.7465255e-03]\n  [1.2351106e-03 9.0384595e-02 4.1215192e-03 ... 5.6260182e-03\n   1.8887992e-03 3.0120127e-02]\n  [1.9340874e-03 6.4351112e-02 5.7755825e-03 ... 2.1044940e-03\n   1.3502518e-04 7.7803805e-04]\n  ...\n  [4.5146029e-05 8.8698030e-02 4.8538335e-04 ... 4.0810453e-04\n   9.5513889e-05 2.9050241e-05]\n  [4.3810918e-03 2.1222527e-01 8.2878984e-04 ... 9.7208340e-03\n   9.7440812e-04 7.1900821e-04]\n  [1.2019366e-03 2.1020372e-01 7.2684370e-02 ... 1.6593653e-03\n   3.4413531e-03 8.7913941e-05]]\n\n [[1.5951030e-02 6.0224812e-04 5.8529088e-03 ... 2.2914439e-02\n   5.0497579e-04 4.4622403e-03]\n  [4.5789829e-03 8.5658650e-04 4.0366207e-03 ... 1.0171512e-02\n   1.5420201e-03 1.1796436e-01]\n  [2.4536543e-03 5.2291504e-04 1.8618173e-03 ... 1.3352347e-03\n   1.8461065e-04 2.4234056e-03]\n  ...\n  [3.9257627e-04 9.1991073e-04 5.0574343e-04 ... 7.4702376e-03\n   1.8553976e-04 2.3538435e-03]\n  [3.4317984e-03 3.9439171e-04 1.4216960e-03 ... 7.4759079e-03\n   4.6578899e-04 1.4807948e-01]\n  [2.7747862e-03 1.5499692e-01 1.2318257e-02 ... 5.9366818e-03\n   1.7441084e-03 5.4047927e-03]]\n\n ...\n\n [[6.6855974e-03 6.4961901e-03 2.9526216e-03 ... 1.7049138e-02\n   1.3564612e-03 4.7796378e-03]\n  [4.0412124e-05 6.6671532e-04 1.0834725e-03 ... 6.5989880e-04\n   5.5978315e-05 8.7662647e-04]\n  [3.0328648e-03 2.9046053e-02 5.2403519e-03 ... 5.6135148e-02\n   6.0897198e-04 2.2559799e-02]\n  ...\n  [1.2701450e-02 2.6712053e-02 1.1336235e-02 ... 3.9217744e-02\n   2.9698282e-03 6.4785846e-02]\n  [3.3753889e-03 9.4133671e-03 1.3106193e-03 ... 2.3456466e-01\n   8.5971440e-04 2.8448062e-02]\n  [2.9921508e-04 2.7408084e-04 2.6260398e-04 ... 2.9211182e-02\n   7.4299530e-04 8.7758002e-04]]\n\n [[4.2750137e-03 1.2143725e-02 4.0890910e-03 ... 1.2302618e-02\n   4.5946467e-04 3.3541255e-02]\n  [6.8685161e-03 1.4292897e-03 6.2165774e-02 ... 2.6293600e-03\n   2.4082889e-03 1.5597725e-04]\n  [6.3668932e-03 4.2234559e-04 4.2924760e-03 ... 6.3389135e-03\n   4.3290938e-04 7.9432614e-03]\n  ...\n  [1.2094845e-04 2.2664040e-03 3.3224229e-04 ... 2.7266808e-02\n   1.2037277e-04 1.0559149e-04]\n  [2.9288961e-03 1.5005509e-04 8.0015702e-04 ... 1.1103156e-02\n   6.9741352e-04 1.5759428e-03]\n  [8.4047010e-03 1.0529178e-03 3.2858990e-02 ... 4.3906933e-03\n   6.8801916e-03 1.2524456e-03]]\n\n [[9.4047328e-03 3.9447106e-02 1.3730933e-03 ... 2.2519948e-02\n   8.1727549e-02 5.1618684e-03]\n  [1.1217272e-02 3.4233086e-02 4.7624372e-03 ... 1.8971810e-03\n   1.0398406e-02 1.9745412e-03]\n  [2.3889858e-03 4.5080245e-03 2.4320476e-03 ... 3.9473684e-03\n   5.1013415e-04 1.3608308e-02]\n  ...\n  [4.5291772e-03 3.7336140e-03 8.0195675e-04 ... 1.4493500e-03\n   5.1165558e-03 1.5945248e-04]\n  [4.6493551e-03 1.7288053e-02 4.8973612e-03 ... 1.7375865e-03\n   4.8358641e-03 3.8237986e-04]\n  [1.6664286e-04 5.8792444e-04 2.2501704e-04 ... 1.1014368e-04\n   1.5420085e-03 4.8457240e-03]]]"
        ],
        [
         "4",
         "5",
         "TabTransformer",
         "0.7947960618846694",
         "0.7785515559246923",
         "0.7969744105603853",
         "0.7862459901278546",
         "0.9163187894137254",
         "[[[2.10054242e-03 4.61391360e-03 2.10227501e-02 ... 2.31889673e-04\n   2.01506098e-03 2.41828820e-04]\n  [6.99038245e-03 1.62529014e-02 5.74108399e-02 ... 8.22873693e-03\n   4.53105904e-02 6.28993195e-03]\n  [9.29818489e-03 2.73419898e-02 1.88250542e-02 ... 7.10989209e-03\n   6.04671473e-03 4.70315386e-03]\n  ...\n  [1.41717831e-03 1.22021709e-03 4.22648806e-03 ... 3.23898246e-04\n   1.34346052e-03 2.72229838e-04]\n  [1.42757101e-02 9.43063945e-03 2.27405541e-02 ... 5.24042966e-03\n   5.89594990e-03 9.52317286e-03]\n  [1.18120178e-03 9.04942211e-03 1.53575679e-02 ... 7.30011612e-04\n   1.74987386e-03 5.06125099e-04]]\n\n [[3.08767660e-03 1.08271390e-02 1.47398759e-03 ... 2.96435843e-04\n   2.00744695e-03 5.73959667e-04]\n  [9.70598124e-03 5.81715722e-03 2.74222866e-02 ... 6.76234160e-03\n   4.19530179e-03 1.26091531e-02]\n  [1.11762283e-03 3.51129612e-03 7.47509161e-03 ... 3.32282143e-05\n   1.74409314e-03 1.05413390e-04]\n  ...\n  [2.43519014e-03 2.75162468e-03 1.69372244e-03 ... 1.71663851e-04\n   2.23413133e-03 3.14499019e-04]\n  [7.02082878e-03 9.07830335e-03 1.31854499e-02 ... 2.36628740e-03\n   4.03199438e-03 5.84147125e-03]\n  [2.89885420e-03 8.88211373e-03 8.00126116e-04 ... 2.95660051e-04\n   1.89086702e-03 2.40255962e-04]]\n\n [[2.99303792e-03 6.52776100e-03 3.01927347e-02 ... 8.76170176e-04\n   1.31164994e-02 2.46260781e-03]\n  [1.44408438e-02 9.40477103e-03 6.53997362e-02 ... 7.90262781e-03\n   5.95645048e-03 1.46510284e-02]\n  [9.25764348e-03 7.49092828e-03 6.99968031e-03 ... 7.76705856e-04\n   3.02376645e-03 1.88456790e-03]\n  ...\n  [1.84597429e-02 2.84063201e-02 2.30906103e-02 ... 6.00387901e-03\n   2.42506322e-02 6.78740069e-03]\n  [2.70110299e-03 4.66409400e-02 1.04734935e-02 ... 3.41568328e-03\n   1.58691569e-03 4.80403844e-03]\n  [1.62867066e-02 1.18150897e-02 1.50580853e-02 ... 1.30588673e-02\n   6.36133924e-03 6.18484756e-03]]\n\n ...\n\n [[5.59758104e-04 4.72250730e-02 2.72000907e-03 ... 1.25442259e-02\n   1.04662497e-02 2.02226196e-03]\n  [4.88721160e-03 9.39834164e-04 1.54699879e-02 ... 2.09409930e-02\n   1.45297665e-02 1.23147061e-03]\n  [1.04432525e-02 5.79015492e-03 4.35261754e-03 ... 2.86354702e-02\n   2.37954929e-02 4.20381967e-03]\n  ...\n  [3.22455098e-03 1.99939217e-03 1.19911786e-03 ... 5.19501269e-02\n   2.33271029e-02 1.14067539e-03]\n  [3.72332928e-04 5.42950025e-03 4.52756532e-04 ... 5.31394705e-02\n   1.28505398e-02 1.80183255e-04]\n  [4.01556911e-03 8.30071978e-03 3.27167623e-02 ... 1.66743360e-02\n   1.62394736e-02 1.43981841e-03]]\n\n [[1.40622156e-02 4.62794444e-03 5.98135579e-04 ... 6.78213639e-03\n   4.97695431e-03 4.41628834e-03]\n  [2.37524300e-03 2.12833453e-02 8.86087865e-03 ... 1.56769808e-02\n   7.66686816e-03 1.43420780e-02]\n  [4.27876227e-03 9.32619441e-03 1.13155553e-03 ... 1.46276923e-03\n   1.72907952e-03 1.22566195e-03]\n  ...\n  [9.17064026e-03 2.64033995e-04 1.65953315e-04 ... 1.42725427e-02\n   7.84139708e-03 7.75793334e-03]\n  [3.67911370e-03 1.69499498e-02 2.47583142e-03 ... 4.21768660e-03\n   2.85219843e-03 6.50665537e-03]\n  [1.36006409e-02 4.74261935e-04 6.58108795e-04 ... 9.50990431e-03\n   1.08474670e-02 8.42944905e-03]]\n\n [[4.29491997e-02 5.33162989e-03 6.55906647e-03 ... 4.38857544e-03\n   2.63755699e-03 6.58379751e-04]\n  [3.90593223e-02 1.75141376e-02 8.46267492e-03 ... 1.02590658e-02\n   3.85357365e-02 1.02214105e-02]\n  [1.65603179e-02 8.86464491e-03 2.61770748e-02 ... 3.10732098e-03\n   2.70834416e-02 1.32412242e-03]\n  ...\n  [4.76299319e-03 8.44700541e-03 8.70342890e-04 ... 5.43134883e-02\n   1.65600993e-03 7.94291962e-03]\n  [3.88779002e-03 2.34162398e-02 3.51607706e-03 ... 2.36507459e-03\n   1.97881497e-02 4.11923788e-03]\n  [7.09053874e-03 5.25993714e-03 5.76015981e-03 ... 1.74506400e-02\n   5.29588573e-03 1.84698636e-03]]]"
        ],
        [
         "5",
         "6",
         "TabTransformer",
         "0.8046413502109705",
         "0.7940145499576651",
         "0.7957228507171825",
         "0.7945341319959986",
         "0.9227282413507036",
         "[[[2.66887539e-04 3.84187093e-04 2.24945674e-04 ... 2.90700817e-04\n   2.18671863e-04 3.15861602e-04]\n  [4.39798023e-04 4.62374138e-03 2.37421896e-02 ... 1.04181271e-03\n   3.03682219e-03 2.62975483e-03]\n  [1.17716137e-02 1.34452945e-03 3.69424839e-03 ... 4.08352772e-03\n   7.77512044e-03 8.29676259e-03]\n  ...\n  [4.08333912e-03 1.38335628e-03 5.24677662e-03 ... 3.28995334e-03\n   9.30966064e-03 5.92146488e-03]\n  [1.55405607e-04 2.19023041e-03 2.13836116e-04 ... 2.60042085e-04\n   5.96916769e-04 4.91665211e-04]\n  [1.12753049e-04 1.83139602e-03 1.91652700e-01 ... 1.23000564e-03\n   2.17115390e-03 1.00156236e-02]]\n\n [[6.67364523e-02 4.05185332e-04 2.88026612e-02 ... 1.35112507e-02\n   1.31685641e-02 1.30860116e-02]\n  [6.80919387e-04 1.90373394e-03 8.51665251e-03 ... 6.29818183e-04\n   5.46351483e-04 8.27624055e-04]\n  [6.25179429e-03 7.73172826e-04 2.83953035e-04 ... 1.31550152e-02\n   1.32486159e-02 4.88905050e-03]\n  ...\n  [3.16590294e-02 4.41836519e-03 7.88658485e-03 ... 2.78283581e-02\n   2.24954337e-02 2.18257047e-02]\n  [4.18157503e-02 3.39276041e-03 7.54963467e-03 ... 2.51516830e-02\n   2.04707105e-02 2.16184836e-02]\n  [4.75662537e-02 1.48831459e-03 3.99671169e-03 ... 3.60504128e-02\n   3.15262042e-02 1.79828145e-02]]\n\n [[3.21409553e-02 9.75392759e-03 1.08962087e-02 ... 1.86064485e-02\n   1.76730398e-02 1.50769018e-02]\n  [3.79252136e-02 1.39749469e-03 9.72064794e-04 ... 7.64965452e-03\n   8.56141653e-03 4.20883065e-03]\n  [2.42890529e-02 2.79642991e-03 7.42737018e-03 ... 6.01245388e-02\n   5.13943769e-02 9.63462517e-03]\n  ...\n  [4.29637618e-02 1.44102890e-02 7.52365915e-03 ... 2.46356353e-02\n   2.19268054e-02 1.71524491e-02]\n  [5.28071448e-02 1.38183767e-02 7.91124627e-03 ... 2.48912554e-02\n   2.32497286e-02 1.82191301e-02]\n  [4.97924611e-02 2.65173567e-03 9.09852516e-03 ... 6.93342611e-02\n   6.00241125e-02 1.09518953e-02]]\n\n ...\n\n [[3.81810851e-02 5.28036151e-03 1.84693560e-02 ... 1.49609027e-02\n   1.35899959e-02 1.47276474e-02]\n  [3.07115298e-02 1.49653375e-03 2.51660147e-03 ... 2.94462545e-03\n   2.77649541e-03 3.26758483e-03]\n  [4.14062105e-03 6.75024174e-04 3.69473710e-04 ... 8.25547799e-03\n   7.11571798e-03 6.13874523e-03]\n  ...\n  [3.27930562e-02 1.51941236e-02 1.13574704e-02 ... 2.29565632e-02\n   1.85861737e-02 2.22962983e-02]\n  [3.87188010e-02 1.35164792e-02 1.13796592e-02 ... 2.21591499e-02\n   1.80525910e-02 2.25139875e-02]\n  [3.07612605e-02 6.34531165e-03 8.27148929e-03 ... 2.61426661e-02\n   2.21304987e-02 2.05222275e-02]]\n\n [[1.05455786e-01 3.97201686e-04 1.21657364e-02 ... 1.54479332e-02\n   1.59266610e-02 1.34884343e-02]\n  [4.83674370e-03 2.21134000e-03 1.02997720e-02 ... 1.70109887e-03\n   1.59381679e-03 2.82629882e-03]\n  [4.84515913e-03 1.40682911e-03 2.71811150e-03 ... 3.75831909e-02\n   3.24013494e-02 1.41686480e-02]\n  ...\n  [3.42528187e-02 5.19647403e-03 9.70696472e-03 ... 3.03261038e-02\n   2.56564841e-02 2.41096132e-02]\n  [4.58838642e-02 4.47208667e-03 9.67178959e-03 ... 3.11710145e-02\n   2.66840085e-02 2.61086151e-02]\n  [3.53045389e-02 1.40065257e-03 7.06971204e-03 ... 4.17207405e-02\n   3.65364179e-02 2.14301664e-02]]\n\n [[3.41097824e-04 1.22299418e-03 4.51416057e-03 ... 3.14417272e-03\n   3.58625548e-03 1.57915975e-03]\n  [4.15306597e-04 1.56407547e-03 7.25145277e-04 ... 2.51514488e-04\n   1.05584727e-03 1.84936437e-03]\n  [1.32082240e-03 8.15694686e-03 1.98155385e-03 ... 6.45316811e-03\n   3.12728249e-03 4.17160196e-03]\n  ...\n  [7.41760829e-04 6.66775799e-04 3.85023654e-03 ... 3.42990010e-04\n   1.45749049e-03 2.15487307e-04]\n  [3.17151658e-03 3.05663352e-03 4.52717720e-03 ... 6.20998791e-04\n   2.36989022e-03 2.98776291e-03]\n  [1.32127409e-03 1.00645758e-02 3.60661116e-03 ... 6.34623040e-03\n   4.28348454e-03 1.82243586e-02]]]"
        ],
        [
         "6",
         "7",
         "TabTransformer",
         "0.7969057665260197",
         "0.7774224799901751",
         "0.7971729723020817",
         "0.7860803822855451",
         "0.9194936469967053",
         "[[[4.31115227e-03 6.89097680e-03 6.79004006e-03 ... 1.22764725e-02\n   6.16026390e-03 9.85502498e-04]\n  [4.07518819e-03 1.52878594e-02 1.51834805e-02 ... 2.88509578e-02\n   1.60896070e-02 3.97106260e-03]\n  [5.65011753e-03 1.61668826e-02 1.72490608e-02 ... 5.97769283e-02\n   1.94385331e-02 1.83251279e-03]\n  ...\n  [4.50444315e-03 2.69559072e-03 2.23867479e-03 ... 7.35998061e-03\n   1.73530704e-03 2.46610609e-03]\n  [6.49963645e-03 1.71566233e-02 1.85931642e-02 ... 5.24620824e-02\n   2.14269552e-02 1.23720826e-03]\n  [1.16818072e-02 5.28061111e-03 6.06506458e-03 ... 2.04958431e-02\n   5.44614531e-03 2.25567631e-03]]\n\n [[4.62785317e-03 2.09419325e-01 1.37199750e-02 ... 6.73715072e-03\n   4.91829636e-03 8.63212976e-04]\n  [1.87277107e-03 8.89853388e-03 1.96627295e-03 ... 7.85319018e-04\n   5.64557733e-04 7.01124370e-02]\n  [4.84851422e-03 1.81565061e-01 7.71315070e-03 ... 1.61429811e-02\n   1.10261878e-02 2.62279203e-03]\n  ...\n  [2.47982843e-03 6.54013008e-02 4.85201553e-03 ... 5.38498303e-03\n   1.95530010e-03 2.37225331e-02]\n  [8.27473868e-03 1.20792232e-01 1.69555582e-02 ... 7.34768715e-03\n   1.54313231e-02 2.54593557e-03]\n  [1.73939310e-03 5.03075123e-03 1.47049525e-03 ... 1.45325065e-02\n   1.72123918e-03 5.09957643e-03]]\n\n [[1.92601141e-02 9.77563262e-02 1.55117512e-02 ... 1.62879918e-02\n   1.55108678e-03 2.75629479e-03]\n  [1.49758169e-04 1.00845832e-03 1.00518379e-03 ... 3.85963498e-03\n   2.27289600e-03 4.07546584e-04]\n  [6.47224253e-03 1.82286911e-02 2.56660245e-02 ... 2.39932165e-02\n   8.50227254e-04 2.34476829e-04]\n  ...\n  [8.01624637e-03 1.60623272e-03 3.80227948e-03 ... 5.23678772e-03\n   1.32097667e-02 4.24832152e-03]\n  [9.76271927e-03 5.80257475e-02 1.47735700e-02 ... 9.28643905e-03\n   1.05008669e-02 1.90835167e-02]\n  [1.01672513e-02 4.49633133e-03 6.49992423e-03 ... 1.88147239e-02\n   1.22949919e-02 3.25593073e-03]]\n\n ...\n\n [[3.93137708e-03 1.20690316e-01 2.04808591e-03 ... 5.99172153e-03\n   5.28890733e-03 1.00381635e-02]\n  [1.17686344e-04 3.95018506e-05 1.26054452e-04 ... 3.67658940e-04\n   3.27117654e-04 3.92226204e-02]\n  [5.75997084e-02 1.13041680e-02 1.12598459e-03 ... 2.59767240e-03\n   2.07249587e-03 6.43669488e-03]\n  ...\n  [9.99504980e-03 8.72970000e-03 9.05361958e-03 ... 7.22230505e-03\n   8.21323227e-03 8.11870396e-03]\n  [8.52789439e-04 3.22872261e-03 1.03874947e-03 ... 9.89948749e-04\n   3.59914103e-03 4.69647115e-03]\n  [5.74131636e-03 5.25961183e-02 1.05790049e-02 ... 1.40897091e-02\n   7.47750420e-03 2.30698730e-03]]\n\n [[5.68439113e-03 1.88733757e-01 6.23921398e-03 ... 2.30027344e-02\n   4.68660938e-03 8.20209179e-03]\n  [2.50977930e-03 2.22742576e-02 2.65799789e-03 ... 7.13714957e-02\n   2.68796342e-03 9.06116366e-02]\n  [8.98155384e-03 9.63192731e-02 1.51805840e-02 ... 2.22187638e-02\n   1.86054744e-02 4.47008992e-04]\n  ...\n  [6.60102069e-03 3.21972673e-03 8.01053736e-03 ... 1.62324123e-02\n   4.35558753e-03 5.43860123e-02]\n  [1.07770422e-02 5.83454110e-02 1.73088219e-02 ... 2.86351591e-02\n   2.26226095e-02 6.47094217e-04]\n  [2.56810361e-03 8.90737399e-02 1.68772147e-03 ... 1.15614468e-02\n   2.48728646e-03 6.98544085e-02]]\n\n [[7.56253861e-03 5.92411216e-03 3.72437201e-03 ... 3.23511008e-03\n   8.01292434e-03 1.04326727e-02]\n  [1.59337316e-02 1.34881691e-03 6.34494983e-03 ... 9.86720552e-04\n   2.68839896e-02 5.84330894e-02]\n  [9.88545455e-03 3.41520808e-03 7.79415760e-03 ... 5.82353259e-03\n   2.62246846e-04 3.85298452e-04]\n  ...\n  [2.85466146e-02 6.80214446e-03 9.89033189e-03 ... 4.59190877e-03\n   1.92318000e-02 1.80261545e-02]\n  [3.81412171e-03 7.19206873e-03 4.17131465e-03 ... 2.31670099e-03\n   2.86433790e-02 4.81734313e-02]\n  [3.95896751e-03 7.82393198e-03 3.51545215e-03 ... 2.30895355e-03\n   2.85846312e-02 4.69058044e-02]]]"
        ],
        [
         "7",
         "8",
         "TabTransformer",
         "0.7857645238430159",
         "0.7661613459109553",
         "0.7881081026169481",
         "0.7758892043717088",
         "0.9171603483564921",
         "[[[1.61447540e-01 6.13470050e-03 2.14674277e-03 ... 7.23559817e-04\n   8.18545558e-03 7.07107596e-03]\n  [2.14193971e-03 1.29825734e-02 1.57280844e-02 ... 2.05680598e-02\n   2.17639212e-03 2.44874135e-03]\n  [2.17379318e-04 8.70586256e-04 4.18770406e-03 ... 9.62134730e-03\n   1.30131972e-04 6.88289583e-04]\n  ...\n  [1.11310137e-03 1.09100025e-02 8.84125102e-03 ... 1.32990964e-02\n   1.11979456e-03 1.17679674e-03]\n  [1.63951322e-01 6.75940048e-03 6.28780457e-04 ... 2.40397966e-03\n   4.78082616e-03 4.01147231e-02]\n  [6.55153475e-04 2.88562593e-03 1.28819386e-03 ... 2.35821269e-02\n   9.63415718e-04 1.90286082e-03]]\n\n [[8.78083184e-02 6.20525097e-04 4.54260549e-03 ... 2.36861844e-04\n   2.17645839e-02 2.02053133e-02]\n  [5.85427927e-03 1.00713912e-02 1.54081071e-02 ... 2.60505415e-02\n   3.98220448e-03 5.59509965e-03]\n  [9.23073199e-03 6.30995072e-03 1.34286163e-02 ... 4.25743349e-02\n   3.05914111e-03 3.46647273e-03]\n  ...\n  [3.23473220e-03 1.00459000e-02 6.86449790e-03 ... 2.06676237e-02\n   9.86549817e-03 1.31979175e-02]\n  [7.65672047e-03 5.67611167e-03 2.26534028e-02 ... 4.03082464e-03\n   4.79513500e-03 7.47249927e-03]\n  [7.03291781e-03 1.20435497e-02 2.66555827e-02 ... 5.75070269e-03\n   6.07105624e-03 9.48992744e-03]]\n\n [[7.51415193e-02 1.71991228e-03 2.25148397e-03 ... 2.59052118e-04\n   6.20673457e-03 9.88062285e-03]\n  [3.35266045e-03 1.18236628e-03 2.28290819e-03 ... 4.06686077e-03\n   6.65631320e-04 7.28092913e-04]\n  [2.41388078e-03 1.88514765e-03 1.41582417e-03 ... 6.69329707e-03\n   1.17936125e-03 1.01967156e-03]\n  ...\n  [4.15036827e-03 1.01821581e-02 9.46420990e-03 ... 5.97976474e-03\n   1.11459773e-02 1.36520537e-02]\n  [8.30611773e-03 6.43162755e-04 1.03719579e-03 ... 3.72185465e-03\n   3.56399600e-04 3.47500551e-04]\n  [2.35529151e-03 2.02635629e-03 3.04722507e-03 ... 5.92843443e-03\n   1.86233188e-03 2.06922460e-03]]\n\n ...\n\n [[6.46916404e-02 1.57084805e-03 9.16796038e-04 ... 1.55509019e-03\n   3.20966123e-04 1.51411898e-03]\n  [2.05127988e-03 7.53885880e-03 5.25352871e-03 ... 1.99514199e-02\n   5.27780410e-03 5.35468757e-03]\n  [4.62091994e-04 7.17843068e-04 9.47674643e-03 ... 4.45809960e-03\n   5.48045291e-03 1.41024846e-03]\n  ...\n  [1.16664125e-03 1.03854882e-02 2.82918988e-03 ... 4.90163406e-03\n   4.26553236e-03 6.60727220e-03]\n  [1.29365278e-02 3.60990781e-03 5.02069155e-03 ... 3.86907486e-03\n   3.79988505e-03 1.27644334e-02]\n  [1.58739835e-03 7.96464458e-03 6.94884034e-03 ... 5.50782308e-03\n   4.30906331e-03 2.23264680e-03]]\n\n [[1.14435107e-02 7.15093985e-02 1.36250975e-02 ... 3.05408752e-03\n   6.74035819e-03 8.03070050e-03]\n  [1.77343860e-02 1.30234323e-02 1.45307053e-02 ... 4.58309650e-02\n   1.43524725e-02 8.77692364e-03]\n  [7.43267708e-04 4.34237830e-02 9.46472865e-03 ... 7.42260180e-03\n   1.27408523e-02 9.87011660e-03]\n  ...\n  [1.13348756e-02 9.54684615e-02 2.39747972e-03 ... 1.65939424e-02\n   3.10024852e-03 3.33199836e-03]\n  [1.85641311e-02 1.28994053e-02 3.77329905e-03 ... 7.83975236e-03\n   1.85315441e-02 2.22449750e-02]\n  [4.18317597e-03 9.68369842e-03 4.45501553e-03 ... 1.19730001e-02\n   9.71117895e-03 3.31617519e-03]]\n\n [[9.22504216e-02 1.85504124e-01 1.42514182e-04 ... 7.78655522e-05\n   2.17225887e-02 7.43277487e-04]\n  [2.15240307e-02 7.78180081e-04 6.56398805e-03 ... 1.76608004e-03\n   3.32513265e-02 2.70507648e-03]\n  [5.27488068e-04 1.64635107e-02 4.95494157e-03 ... 9.64682922e-03\n   3.45423556e-04 4.67536552e-03]\n  ...\n  [1.68691191e-03 3.19454726e-03 9.06562805e-03 ... 1.18650068e-02\n   1.44183135e-03 6.46015164e-03]\n  [1.20489009e-01 1.93976283e-01 9.64620922e-05 ... 2.96970218e-04\n   1.09155150e-02 6.29107584e-04]\n  [8.68254632e-04 9.37055051e-02 2.03135423e-03 ... 2.19951998e-02\n   1.50711590e-03 1.75757259e-02]]]"
        ],
        [
         "8",
         "9",
         "TabTransformer",
         "0.8063018708679139",
         "0.7908908489650975",
         "0.8101570012043067",
         "0.7993992292749702",
         "0.9218112884162893",
         "[[[7.05700964e-02 1.86881423e-02 2.37011015e-02 ... 5.44812642e-02\n   8.82758796e-02 1.55806728e-02]\n  [1.03946239e-01 2.03556684e-03 3.30755342e-04 ... 2.69552553e-03\n   1.09298572e-01 2.44121589e-02]\n  [4.32801694e-02 3.87885317e-04 1.07213011e-04 ... 2.04675854e-03\n   7.54263252e-02 1.27975801e-02]\n  ...\n  [1.46629766e-03 9.80114681e-04 4.75007284e-04 ... 7.58237168e-02\n   1.47293322e-03 1.27580846e-02]\n  [6.63860887e-02 2.00287420e-02 7.48042166e-02 ... 1.32855684e-01\n   7.40457252e-02 8.06203764e-03]\n  [8.52064416e-02 8.18615500e-03 1.03690277e-03 ... 2.32953019e-02\n   9.59817097e-02 2.88011078e-02]]\n\n [[6.83464035e-02 4.98052081e-03 1.76626816e-03 ... 8.51338543e-03\n   1.07677549e-01 2.46560527e-03]\n  [1.39640346e-01 5.81794768e-04 1.39977988e-02 ... 1.82835793e-03\n   9.78263393e-02 6.44623593e-04]\n  [5.74529730e-03 6.38889521e-03 3.10875080e-03 ... 5.61735779e-03\n   7.63125392e-03 2.49770377e-03]\n  ...\n  [3.97977274e-05 5.69834257e-04 1.27130165e-03 ... 5.62496483e-04\n   8.06629541e-05 1.62677636e-04]\n  [7.44500086e-02 9.26176924e-03 1.85278000e-03 ... 3.76132037e-03\n   1.25244662e-01 3.85703007e-03]\n  [1.64211560e-02 4.27947892e-03 1.02559123e-02 ... 9.16530378e-03\n   1.70097686e-02 1.25787524e-03]]\n\n [[1.05038751e-04 1.69538297e-02 6.66967942e-04 ... 1.69845000e-02\n   6.15763784e-05 3.60512920e-02]\n  [4.70716768e-05 1.41369430e-02 1.81401335e-03 ... 3.10268311e-04\n   1.74262357e-04 3.11919097e-02]\n  [4.46518324e-02 2.25899909e-02 1.40828046e-03 ... 3.65989795e-03\n   5.36303036e-02 6.60189567e-03]\n  ...\n  [6.65237676e-05 1.91989355e-02 3.30864481e-04 ... 5.81858726e-03\n   4.77465765e-05 8.78906995e-03]\n  [5.71149249e-05 1.17416289e-02 4.98674868e-04 ... 1.65648572e-02\n   2.41668986e-05 3.66761908e-02]\n  [1.20445511e-04 1.05445027e-01 8.75301752e-03 ... 6.89975510e-04\n   3.99753859e-04 3.34195532e-02]]\n\n ...\n\n [[4.24648188e-02 5.29332040e-03 2.42797658e-03 ... 3.83972116e-02\n   9.81495008e-02 1.00370422e-02]\n  [1.14870362e-03 8.80148378e-04 5.89183941e-02 ... 2.37434730e-03\n   8.83579196e-04 4.91787773e-03]\n  [7.98643683e-04 1.95488539e-02 1.51809864e-03 ... 1.02273542e-02\n   3.48748523e-03 1.10786350e-03]\n  ...\n  [3.70060094e-04 1.92700587e-02 2.81603378e-03 ... 6.79703604e-04\n   4.39408061e-04 1.73061257e-04]\n  [3.99496742e-02 3.31806242e-02 2.45978031e-03 ... 3.85204218e-02\n   1.00339159e-01 2.23875567e-02]\n  [2.95660701e-02 1.83962826e-02 1.44207925e-02 ... 1.33088287e-02\n   4.06450108e-02 1.34938927e-02]]\n\n [[7.11690858e-02 1.42098796e-02 4.04993538e-03 ... 6.07035272e-02\n   7.28678480e-02 1.07608754e-02]\n  [1.23796575e-02 4.37265821e-03 6.24780580e-02 ... 1.27446149e-02\n   4.97602904e-03 1.33807342e-02]\n  [1.14725821e-03 3.42754796e-02 2.66121421e-03 ... 6.29957020e-02\n   1.90237677e-03 2.88788765e-03]\n  ...\n  [4.19351418e-04 1.31812354e-03 8.45126575e-04 ... 1.82248093e-03\n   1.79415380e-04 1.37927718e-02]\n  [6.43077567e-02 8.85227323e-03 3.60289938e-03 ... 2.05635652e-02\n   7.00076297e-02 1.17332740e-02]\n  [9.14141983e-02 7.84890354e-02 1.54422550e-03 ... 1.34275248e-03\n   2.12873314e-02 3.44409496e-02]]\n\n [[2.35157160e-04 5.70995826e-03 3.72020039e-03 ... 2.76625832e-03\n   7.63890799e-04 7.30896695e-03]\n  [8.04961310e-04 2.44860440e-01 4.67688311e-03 ... 7.18144923e-02\n   3.80587880e-03 2.34823627e-03]\n  [3.94264869e-02 4.41544782e-03 2.35671108e-03 ... 6.10412359e-02\n   4.11690511e-02 7.64024258e-03]\n  ...\n  [5.25663234e-03 7.69246742e-03 1.29302836e-03 ... 4.04917784e-02\n   8.37588962e-03 5.96802961e-03]\n  [1.54396621e-04 4.18441225e-04 3.37170321e-04 ... 3.23953247e-03\n   6.05040055e-04 9.76987481e-02]\n  [4.29016724e-03 3.39320824e-02 1.82844773e-02 ... 6.68218881e-02\n   1.20734060e-02 7.64176063e-03]]]"
        ],
        [
         "9",
         "10",
         "TabTransformer",
         "0.8055985370656914",
         "0.7951731108789032",
         "0.7993450260247705",
         "0.7971822379908192",
         "0.9253383443928099",
         "[[[1.70498528e-03 1.80307105e-02 1.10902982e-02 ... 7.54064228e-03\n   1.12385619e-02 2.88680522e-03]\n  [7.35149300e-03 5.39894076e-03 6.72612526e-03 ... 1.18994585e-03\n   6.73835399e-03 3.01802251e-03]\n  [1.95960654e-03 3.49321924e-02 6.65797107e-03 ... 4.96167038e-03\n   7.50651723e-03 2.36782711e-02]\n  ...\n  [8.76343576e-04 2.09721088e-01 9.50080284e-04 ... 3.83588579e-03\n   6.23266969e-04 9.70784808e-04]\n  [1.50580257e-02 2.51045302e-02 2.00426616e-02 ... 5.61798457e-03\n   1.05661657e-02 4.08938108e-03]\n  [5.28642966e-04 2.57735491e-01 6.99272146e-04 ... 1.08763343e-03\n   2.45527155e-03 1.76699297e-03]]\n\n [[4.22374671e-03 3.27452668e-04 2.30358186e-04 ... 9.34624579e-04\n   1.81072988e-02 1.01316713e-04]\n  [8.29742756e-03 3.46450089e-03 4.67314292e-03 ... 3.34179332e-03\n   3.42870466e-02 2.60611856e-03]\n  [1.45153683e-02 6.93556038e-04 2.01232778e-03 ... 6.17754739e-03\n   3.06350505e-03 4.52321023e-03]\n  ...\n  [2.58041313e-03 7.87405297e-04 1.28623564e-03 ... 1.22086829e-04\n   1.88275240e-02 9.65489162e-05]\n  [5.37534725e-05 6.61070590e-06 6.79202776e-06 ... 4.59652692e-05\n   5.33910097e-05 4.82327414e-05]\n  [1.51996582e-03 3.58455349e-04 4.75628767e-04 ... 3.14760196e-04\n   1.13571938e-02 2.56927364e-04]]\n\n [[3.82992788e-04 2.78668329e-02 2.82637239e-03 ... 2.75470503e-03\n   5.61557477e-03 3.46744724e-04]\n  [2.40903115e-03 8.01295340e-02 1.18627351e-04 ... 3.66828113e-04\n   1.76753572e-04 5.26627002e-04]\n  [3.62529396e-03 1.51100531e-02 5.91472257e-03 ... 6.57482538e-03\n   1.33989751e-02 1.88945364e-02]\n  ...\n  [1.45312530e-04 2.07340643e-01 1.06799568e-03 ... 5.06859389e-04\n   2.14232295e-03 3.60388425e-04]\n  [5.08690439e-03 2.37155497e-01 5.48411440e-03 ... 1.27385533e-03\n   3.66981793e-03 7.71555235e-04]\n  [1.74845773e-04 1.07000545e-01 7.01641256e-04 ... 2.21713941e-04\n   1.54376309e-03 2.33754588e-04]]\n\n ...\n\n [[4.70507098e-03 1.81343174e-03 2.23464984e-03 ... 2.33434979e-03\n   5.33547737e-02 9.86490864e-04]\n  [1.95003487e-03 2.55352166e-03 2.53800699e-03 ... 2.85010831e-03\n   4.57474180e-02 1.60955712e-02]\n  [7.80241215e-04 1.76834618e-03 2.02773395e-03 ... 2.54552951e-03\n   2.97306059e-03 6.86266646e-03]\n  ...\n  [2.95743637e-04 2.23997538e-03 2.45846296e-03 ... 1.46012811e-03\n   1.34888701e-02 5.78145683e-03]\n  [3.07100738e-04 2.65097653e-04 1.91939791e-04 ... 5.15213411e-04\n   5.06655732e-03 8.60473738e-05]\n  [5.58559492e-04 1.96348550e-03 1.94173562e-03 ... 1.04354788e-03\n   1.65138170e-02 1.32460007e-03]]\n\n [[1.29837904e-03 5.94566390e-03 3.62287345e-03 ... 5.79914637e-03\n   7.75338034e-04 2.00726185e-03]\n  [3.61366547e-04 2.04071126e-04 1.85989644e-04 ... 3.24966881e-04\n   2.13888697e-02 2.21542455e-03]\n  [3.20354244e-03 5.64108137e-03 6.22653589e-03 ... 7.96967745e-03\n   5.60914874e-02 1.88569562e-03]\n  ...\n  [4.85668570e-04 1.37089097e-04 3.26796406e-04 ... 3.86583945e-03\n   5.21117589e-03 1.15713489e-03]\n  [1.60437021e-02 2.67319730e-03 7.31173083e-02 ... 2.06718184e-02\n   1.34201590e-02 5.99853834e-03]\n  [1.07382995e-03 1.88106881e-03 1.13891228e-03 ... 1.81168120e-03\n   7.86134601e-03 1.29257319e-02]]\n\n [[1.14755367e-03 1.29661784e-01 2.99025886e-03 ... 5.03917783e-03\n   6.63206950e-02 2.03142292e-03]\n  [2.94188590e-04 1.46344095e-01 2.33680755e-03 ... 2.02417083e-04\n   1.70675557e-04 5.00882452e-04]\n  [1.99262542e-03 8.12422426e-04 5.49186626e-03 ... 2.63837050e-03\n   4.25375719e-03 2.18606903e-03]\n  ...\n  [2.88463780e-04 4.05534357e-01 1.10338640e-03 ... 8.00570007e-04\n   2.30180146e-03 1.34481204e-04]\n  [8.66428763e-03 3.08494240e-01 2.27833912e-02 ... 1.19190998e-02\n   6.28063548e-03 3.09954048e-03]\n  [1.70239931e-04 4.12101865e-01 1.29173830e-04 ... 3.02110799e-04\n   2.09734635e-03 5.54947241e-04]]]"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>attention_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.809001</td>\n",
       "      <td>0.794442</td>\n",
       "      <td>0.808112</td>\n",
       "      <td>0.800580</td>\n",
       "      <td>0.924369</td>\n",
       "      <td>[[[0.006546152, 0.009071659, 0.0020918595, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.808298</td>\n",
       "      <td>0.796923</td>\n",
       "      <td>0.809708</td>\n",
       "      <td>0.802561</td>\n",
       "      <td>0.925042</td>\n",
       "      <td>[[[0.010708559, 0.015709331, 0.029624566, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.786639</td>\n",
       "      <td>0.768705</td>\n",
       "      <td>0.788117</td>\n",
       "      <td>0.777193</td>\n",
       "      <td>0.915547</td>\n",
       "      <td>[[[8.279453e-05, 0.073742904, 0.00079792726, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.808158</td>\n",
       "      <td>0.793229</td>\n",
       "      <td>0.810284</td>\n",
       "      <td>0.800564</td>\n",
       "      <td>0.923616</td>\n",
       "      <td>[[[0.004616289, 0.004747437, 0.013187103, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.794796</td>\n",
       "      <td>0.778552</td>\n",
       "      <td>0.796974</td>\n",
       "      <td>0.786246</td>\n",
       "      <td>0.916319</td>\n",
       "      <td>[[[0.0021005424, 0.0046139136, 0.02102275, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.804641</td>\n",
       "      <td>0.794015</td>\n",
       "      <td>0.795723</td>\n",
       "      <td>0.794534</td>\n",
       "      <td>0.922728</td>\n",
       "      <td>[[[0.00026688754, 0.0003841871, 0.00022494567,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.796906</td>\n",
       "      <td>0.777422</td>\n",
       "      <td>0.797173</td>\n",
       "      <td>0.786080</td>\n",
       "      <td>0.919494</td>\n",
       "      <td>[[[0.0043111523, 0.006890977, 0.00679004, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.785765</td>\n",
       "      <td>0.766161</td>\n",
       "      <td>0.788108</td>\n",
       "      <td>0.775889</td>\n",
       "      <td>0.917160</td>\n",
       "      <td>[[[0.16144754, 0.0061347005, 0.0021467428, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.806302</td>\n",
       "      <td>0.790891</td>\n",
       "      <td>0.810157</td>\n",
       "      <td>0.799399</td>\n",
       "      <td>0.921811</td>\n",
       "      <td>[[[0.0705701, 0.018688142, 0.023701102, 0.0021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.805599</td>\n",
       "      <td>0.795173</td>\n",
       "      <td>0.799345</td>\n",
       "      <td>0.797182</td>\n",
       "      <td>0.925338</td>\n",
       "      <td>[[[0.0017049853, 0.01803071, 0.011090298, 0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold      Model Name  accuracy  precision    recall  f1_score   roc_auc  \\\n",
       "0     1  TabTransformer  0.809001   0.794442  0.808112  0.800580  0.924369   \n",
       "1     2  TabTransformer  0.808298   0.796923  0.809708  0.802561  0.925042   \n",
       "2     3  TabTransformer  0.786639   0.768705  0.788117  0.777193  0.915547   \n",
       "3     4  TabTransformer  0.808158   0.793229  0.810284  0.800564  0.923616   \n",
       "4     5  TabTransformer  0.794796   0.778552  0.796974  0.786246  0.916319   \n",
       "5     6  TabTransformer  0.804641   0.794015  0.795723  0.794534  0.922728   \n",
       "6     7  TabTransformer  0.796906   0.777422  0.797173  0.786080  0.919494   \n",
       "7     8  TabTransformer  0.785765   0.766161  0.788108  0.775889  0.917160   \n",
       "8     9  TabTransformer  0.806302   0.790891  0.810157  0.799399  0.921811   \n",
       "9    10  TabTransformer  0.805599   0.795173  0.799345  0.797182  0.925338   \n",
       "\n",
       "                                   attention_weights  \n",
       "0  [[[0.006546152, 0.009071659, 0.0020918595, 0.0...  \n",
       "1  [[[0.010708559, 0.015709331, 0.029624566, 0.02...  \n",
       "2  [[[8.279453e-05, 0.073742904, 0.00079792726, 0...  \n",
       "3  [[[0.004616289, 0.004747437, 0.013187103, 0.00...  \n",
       "4  [[[0.0021005424, 0.0046139136, 0.02102275, 0.0...  \n",
       "5  [[[0.00026688754, 0.0003841871, 0.00022494567,...  \n",
       "6  [[[0.0043111523, 0.006890977, 0.00679004, 0.00...  \n",
       "7  [[[0.16144754, 0.0061347005, 0.0021467428, 0.0...  \n",
       "8  [[[0.0705701, 0.018688142, 0.023701102, 0.0021...  \n",
       "9  [[[0.0017049853, 0.01803071, 0.011090298, 0.00...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_fold_results_df = pd.concat([pd.DataFrame(mlp_fold_results_list), pd.DataFrame(mlp_fold_results_list_ros), pd.DataFrame(mlp_fold_results_list_smote), pd.DataFrame(transformer_fold_results_list), pd.DataFrame(transformer_fold_results_list_ros), pd.DataFrame(transformer_fold_results_list_smote), pd.DataFrame(tab_transformer_fold_results_list), pd.DataFrame(tab_transformer_fold_results_list_ros), pd.DataFrame(tab_transformer_fold_results_list_smote), pd.DataFrame(ft_transformer_fold_results_list), pd.DataFrame(ft_transformer_fold_results_list_ros), pd.DataFrame(ft_transformer_fold_results_list_smote)], ignore_index=True)\n",
    "final_fold_results_df.to_csv('./data/5_final_fold_results.csv', index=False)\n",
    "final_fold_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TabTransformerWrapper:\n\tsize mismatch for model.mlp.mlp.0.weight: copying a param with shape torch.Size([722, 361]) from checkpoint, the shape in current model is torch.Size([1444, 361]).\n\tsize mismatch for model.mlp.mlp.0.bias: copying a param with shape torch.Size([722]) from checkpoint, the shape in current model is torch.Size([1444]).\n\tsize mismatch for model.mlp.mlp.2.weight: copying a param with shape torch.Size([361, 722]) from checkpoint, the shape in current model is torch.Size([722, 1444]).\n\tsize mismatch for model.mlp.mlp.2.bias: copying a param with shape torch.Size([361]) from checkpoint, the shape in current model is torch.Size([722]).\n\tsize mismatch for model.mlp.mlp.4.weight: copying a param with shape torch.Size([3, 361]) from checkpoint, the shape in current model is torch.Size([3, 722]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m tab_transformer_best_roc_model_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/tab_transformer_best_ros_state.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m tab_transformer_best_roc_model \u001b[38;5;241m=\u001b[39m TabTransformerWrapper(cat_dims\u001b[38;5;241m=\u001b[39mcat_dims, cat_idxs\u001b[38;5;241m=\u001b[39mcat_idxs, num_idxs\u001b[38;5;241m=\u001b[39mnum_idxs, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 31\u001b[0m tab_transformer_best_roc_model\u001b[38;5;241m.\u001b[39mload_state_dict(tab_transformer_best_roc_model_state)\n\u001b[1;32m     32\u001b[0m tab_transformer_best_model_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/tab_transformer_best_state.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m tab_transformer_best_model \u001b[38;5;241m=\u001b[39m TabTransformerWrapper(cat_dims\u001b[38;5;241m=\u001b[39mcat_dims,cat_idxs\u001b[38;5;241m=\u001b[39mcat_idxs,num_idxs\u001b[38;5;241m=\u001b[39mnum_idxs,num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/cs/student/projects1/dsml/2024/zecaox01/miniconda3/envs/finance/lib/python3.12/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TabTransformerWrapper:\n\tsize mismatch for model.mlp.mlp.0.weight: copying a param with shape torch.Size([722, 361]) from checkpoint, the shape in current model is torch.Size([1444, 361]).\n\tsize mismatch for model.mlp.mlp.0.bias: copying a param with shape torch.Size([722]) from checkpoint, the shape in current model is torch.Size([1444]).\n\tsize mismatch for model.mlp.mlp.2.weight: copying a param with shape torch.Size([361, 722]) from checkpoint, the shape in current model is torch.Size([722, 1444]).\n\tsize mismatch for model.mlp.mlp.2.bias: copying a param with shape torch.Size([361]) from checkpoint, the shape in current model is torch.Size([722]).\n\tsize mismatch for model.mlp.mlp.4.weight: copying a param with shape torch.Size([3, 361]) from checkpoint, the shape in current model is torch.Size([3, 722])."
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "mlp_best_model_state = torch.load('./models/mlp_best_state.pth')\n",
    "mlp_best_model = MLP(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "mlp_best_model.load_state_dict(mlp_best_model_state)\n",
    "mlp_best_roc_model_state = torch.load('./models/mlp_best_ros_state.pth')\n",
    "mlp_best_roc_model = MLP(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "mlp_best_roc_model.load_state_dict(mlp_best_roc_model_state)\n",
    "mlp_best_smote_model_state = torch.load('./models/mlp_best_smote_state.pth')\n",
    "mlp_best_smote_model = MLP(input_dim=X.shape[1], num_classes=3).to(device)\n",
    "mlp_best_smote_model.load_state_dict(mlp_best_smote_model_state)\n",
    "transformer_best_model_state = torch.load('./models/transformer_best_state.pth')\n",
    "transformer_best_model = Transformer(input_dim=X.shape[1], num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "transformer_best_model.load_state_dict(transformer_best_model_state)\n",
    "transformer_best_roc_model_state = torch.load('./models/transformer_best_ros_state.pth')\n",
    "transformer_best_roc_model = Transformer(input_dim=X.shape[1], num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "transformer_best_roc_model.load_state_dict(transformer_best_roc_model_state)\n",
    "transformer_best_smote_model_state = torch.load('./models/transformer_best_smote_state.pth')\n",
    "transformer_best_smote_model = Transformer(input_dim=X.shape[1], num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "transformer_best_smote_model.load_state_dict(transformer_best_smote_model_state)\n",
    "ft_transformer_best_model_state = torch.load('./models/ft_transformer_best_state.pth')\n",
    "ft_transformer_best_model = FTTransformerWrapper(cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs, num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "ft_transformer_best_model.load_state_dict(ft_transformer_best_model_state)\n",
    "ft_transformer_best_roc_model_state = torch.load('./models/ft_transformer_best_ros_state.pth')\n",
    "ft_transformer_best_roc_model = FTTransformerWrapper(cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs, num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "ft_transformer_best_roc_model.load_state_dict(ft_transformer_best_roc_model_state)\n",
    "ft_transformer_best_smote_model_state = torch.load('./models/ft_transformer_best_smote_state.pth')\n",
    "ft_transformer_best_smote_model = FTTransformerWrapper(cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs, num_classes=3, num_heads=4, num_layers=2, dim_model=64, dim_ff=128, dropout=0.1).to(device)\n",
    "ft_transformer_best_smote_model.load_state_dict(ft_transformer_best_smote_model_state)\n",
    "tab_transformer_best_model_state = torch.load('./models/tab_transformer_best_state.pth')\n",
    "tab_transformer_best_model = TabTransformerWrapper(cat_dims=cat_dims,cat_idxs=cat_idxs,num_idxs=num_idxs,num_classes=3,num_heads=4,num_layers=2, dim_model=64, dropout=0.1).to(device)\n",
    "tab_transformer_best_model.load_state_dict(tab_transformer_best_model_state)\n",
    "tab_transformer_best_roc_model_state = torch.load('./models/tab_transformer_best_ros_state.pth')\n",
    "tab_transformer_best_roc_model = TabTransformerWrapper(cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs, num_classes=3, num_heads=4, num_layers=2, dim_model=64, dropout=0.1).to(device)\n",
    "tab_transformer_best_roc_model.load_state_dict(tab_transformer_best_roc_model_state)\n",
    "tab_transformer_best_smote_model_state = torch.load('./models/tab_transformer_best_smote_state.pth')\n",
    "tab_transformer_best_smote_model = TabTransformerWrapper(cat_dims=cat_dims, cat_idxs=cat_idxs, num_idxs=num_idxs, num_classes=3, num_heads=4, num_layers=2, dim_model=64, dropout=0.1).to(device)\n",
    "tab_transformer_best_smote_model.load_state_dict(tab_transformer_best_smote_model_state)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, device, name='MLP'):\n",
    "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(X_test, y_test),\n",
    "        batch_size=128, num_workers=0\n",
    "    )\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_y.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "\n",
    "    results = {\n",
    "        'name': name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([results])\n",
    "\n",
    "# Evaluate best model\n",
    "result_mlp_df = evaluate_model(mlp_best_model, X_test, y_test, device, name='MLP')\n",
    "result_mlp_roc_df = evaluate_model(mlp_best_roc_model, X_test, y_test, device, name='MLP_ROS')\n",
    "result_mlp_smote_df = evaluate_model(mlp_best_smote_model, X_test, y_test, device, name='MLP_SMOTE')\n",
    "result_transformer_df = evaluate_model(transformer_best_model, X_test, y_test, device, name='Transformer')\n",
    "result_transformer_roc_df = evaluate_model(transformer_best_roc_model, X_test, y_test, device, name='Transformer_ROS')\n",
    "result_transformer_smote_df = evaluate_model(transformer_best_smote_model, X_test, y_test, device, name='Transformer_SMOTE')\n",
    "result_ft_transformer_df = evaluate_model(ft_transformer_best_model, X_test, y_test, device, name='FTTransformer')\n",
    "result_ft_transformer_roc_df = evaluate_model(ft_transformer_best_roc_model, X_test, y_test, device, name='FTTransformer_ROS')\n",
    "result_ft_transformer_smote_df = evaluate_model(ft_transformer_best_smote_model, X_test, y_test, device, name='FTTransformer_SMOTE')\n",
    "result_tab_transformer_df = evaluate_model(tab_transformer_best_model, X_test, y_test, device, name='TabTransformer')\n",
    "result_tab_transformer_roc_df = evaluate_model(tab_transformer_best_roc_model, X_test, y_test, device, name='TabTransformer_ROS')\n",
    "result_tab_transformer_smote_df = evaluate_model(tab_transformer_best_smote_model, X_test, y_test, device, name='TabTransformer_SMOTE')\n",
    "\n",
    "# Combine results\n",
    "result_df = pd.concat([\n",
    "    result_mlp_df, \n",
    "    result_mlp_roc_df, \n",
    "    result_mlp_smote_df, \n",
    "    result_transformer_df, \n",
    "    result_transformer_roc_df, \n",
    "    result_transformer_smote_df,\n",
    "    result_tab_transformer_df,\n",
    "    result_tab_transformer_roc_df,\n",
    "    result_tab_transformer_smote_df,\n",
    "    result_ft_transformer_df,\n",
    "    result_ft_transformer_roc_df,\n",
    "    result_ft_transformer_smote_df\n",
    "], ignore_index=True)\n",
    "result_df.to_csv('./data/5_test_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "roc_auc",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "60f4a364-5586-4565-99d3-4018198b0042",
       "rows": [
        [
         "0",
         "MLP",
         "0.7682700421940928",
         "0.751039661921313",
         "0.7692312489905838",
         "0.7589198717664285",
         "0.9039418597437701"
        ],
        [
         "0",
         "MLP_ROS",
         "0.7617440225035161",
         "0.7446064020832462",
         "0.8039007785301644",
         "0.7602983562123359",
         "0.9063958820179389"
        ],
        [
         "0",
         "MLP_SMOTE",
         "0.7590998593530239",
         "0.7383375568133239",
         "0.7688561632702168",
         "0.7509445349263345",
         "0.8974711605693511"
        ],
        [
         "0",
         "TabTransformer",
         "0.7844725738396624",
         "0.7757071684583785",
         "0.7779072267435335",
         "0.7750741307304351",
         "0.913639293565958"
        ],
        [
         "0",
         "TabTransformer_ROS",
         "0.7932489451476793",
         "0.7770442371927698",
         "0.8283475539888724",
         "0.7923439597531571",
         "0.9172983862182941"
        ],
        [
         "0",
         "TabTransformer_SMOTE",
         "0.7917862165963432",
         "0.7773410456426827",
         "0.8077276172879784",
         "0.7888416882111069",
         "0.9153775081550041"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.768270</td>\n",
       "      <td>0.751040</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.758920</td>\n",
       "      <td>0.903942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP_ROS</td>\n",
       "      <td>0.761744</td>\n",
       "      <td>0.744606</td>\n",
       "      <td>0.803901</td>\n",
       "      <td>0.760298</td>\n",
       "      <td>0.906396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP_SMOTE</td>\n",
       "      <td>0.759100</td>\n",
       "      <td>0.738338</td>\n",
       "      <td>0.768856</td>\n",
       "      <td>0.750945</td>\n",
       "      <td>0.897471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.784473</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.777907</td>\n",
       "      <td>0.775074</td>\n",
       "      <td>0.913639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TabTransformer_ROS</td>\n",
       "      <td>0.793249</td>\n",
       "      <td>0.777044</td>\n",
       "      <td>0.828348</td>\n",
       "      <td>0.792344</td>\n",
       "      <td>0.917298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TabTransformer_SMOTE</td>\n",
       "      <td>0.791786</td>\n",
       "      <td>0.777341</td>\n",
       "      <td>0.807728</td>\n",
       "      <td>0.788842</td>\n",
       "      <td>0.915378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  accuracy  precision    recall  f1_score   roc_auc\n",
       "0                   MLP  0.768270   0.751040  0.769231  0.758920  0.903942\n",
       "0               MLP_ROS  0.761744   0.744606  0.803901  0.760298  0.906396\n",
       "0             MLP_SMOTE  0.759100   0.738338  0.768856  0.750945  0.897471\n",
       "0        TabTransformer  0.784473   0.775707  0.777907  0.775074  0.913639\n",
       "0    TabTransformer_ROS  0.793249   0.777044  0.828348  0.792344  0.917298\n",
       "0  TabTransformer_SMOTE  0.791786   0.777341  0.807728  0.788842  0.915378"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
