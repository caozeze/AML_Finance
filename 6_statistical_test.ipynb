{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fold",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "macro precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "macro recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "macro f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "roc auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "class 0.0 precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "class 0.0 recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "class 1.0 precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "class 1.0 recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "class 2.0 precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "class 2.0 recall",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f4cc8f34-05fc-4a4c-ac6a-18d3cc1c3c7b",
       "rows": [
        [
         "0",
         "1",
         "Logistic Regression",
         "0.6624472573839663",
         "0.64265968734587",
         "0.6358442026280047",
         "0.6348406160605374",
         "0.8153042054160613",
         "0.694526191877575",
         "0.5402930402930403",
         "0.6879310344827586",
         "0.7473909553117474",
         "0.5455218356772761",
         "0.6198486122792263"
        ],
        [
         "1",
         "2",
         "Logistic Regression",
         "0.6488045007032349",
         "0.6291919184532336",
         "0.6246453520069338",
         "0.6233776338150566",
         "0.8114370853489715",
         "0.6722063037249284",
         "0.5370879120879121",
         "0.6747886623570363",
         "0.7262510034787263",
         "0.5405807892777365",
         "0.6105971404541631"
        ],
        [
         "2",
         "3",
         "Logistic Regression",
         "0.6541490857946554",
         "0.6334190130320932",
         "0.6313158476835762",
         "0.6292594081079647",
         "0.8111612729357113",
         "0.670601461495222",
         "0.5464956481905634",
         "0.6833249623304872",
         "0.7281241637677281",
         "0.5463306152705708",
         "0.619327731092437"
        ],
        [
         "3",
         "4",
         "Logistic Regression",
         "0.6552742616033755",
         "0.6355645330569529",
         "0.6317745986018397",
         "0.6310679924676311",
         "0.8094589528599069",
         "0.6638888888888889",
         "0.5474118185982593",
         "0.682591295647824",
         "0.7302649183837303",
         "0.5602134146341463",
         "0.6176470588235294"
        ],
        [
         "4",
         "5",
         "Logistic Regression",
         "0.6524613220815753",
         "0.6336417451375692",
         "0.6310764306085274",
         "0.6285847472638589",
         "0.8088031489065005",
         "0.6714614499424626",
         "0.5345854328905176",
         "0.6788029925187032",
         "0.7283917580947283",
         "0.5506607929515418",
         "0.6302521008403361"
        ],
        [
         "5",
         "6",
         "Logistic Regression",
         "0.6672292545710268",
         "0.6480717421616728",
         "0.6367896557404052",
         "0.6391029230586284",
         "0.8183416047543739",
         "0.697594501718213",
         "0.5576923076923077",
         "0.6883720930232559",
         "0.7526766595289079",
         "0.5582486317435497",
         "0.6"
        ],
        [
         "6",
         "7",
         "Logistic Regression",
         "0.6603375527426161",
         "0.6409181141439012",
         "0.6361183011611277",
         "0.6344453834584339",
         "0.8166647483599664",
         "0.6833527357392316",
         "0.5375457875457875",
         "0.6865560782371875",
         "0.742237687366167",
         "0.5528455284552846",
         "0.6285714285714286"
        ],
        [
         "7",
         "8",
         "Logistic Regression",
         "0.6512871008580673",
         "0.6318823309834635",
         "0.6239444092357936",
         "0.6239396008128658",
         "0.8147608515918888",
         "0.6668621700879765",
         "0.5206043956043956",
         "0.6766935681095623",
         "0.7406316916488223",
         "0.5520912547528517",
         "0.6105971404541631"
        ],
        [
         "8",
         "9",
         "Logistic Regression",
         "0.6510057673371782",
         "0.6299064962313945",
         "0.6278265164655488",
         "0.6251172760880129",
         "0.8100226721278485",
         "0.6807538549400343",
         "0.5457875457875457",
         "0.6803216888665494",
         "0.7245717344753747",
         "0.5286439448875997",
         "0.6131202691337259"
        ],
        [
         "9",
         "10",
         "Logistic Regression",
         "0.6512871008580673",
         "0.6320620842738606",
         "0.6281455088975129",
         "0.6262659284461125",
         "0.8092712814707772",
         "0.6657010428736964",
         "0.5260989010989011",
         "0.6781466798810704",
         "0.7326017130620985",
         "0.5523385300668151",
         "0.6257359125315392"
        ],
        [
         "10",
         "1",
         "Logistic Regression ROS",
         "0.6977698483496878",
         "0.6956710476806937",
         "0.6977691113633616",
         "0.6946812197430982",
         "0.8301140107480149",
         "0.7258382642998028",
         "0.6895074946466809",
         "0.6469886702444841",
         "0.5806796895905807",
         "0.7141862084977942",
         "0.8231201498528231"
        ],
        [
         "11",
         "2",
         "Logistic Regression ROS",
         "0.6990187332738627",
         "0.6974001729166783",
         "0.6990172402563481",
         "0.6960259080505499",
         "0.8254301787020332",
         "0.7289104947097512",
         "0.6822805139186295",
         "0.6507842556969518",
         "0.5884399250735884",
         "0.7125057683433318",
         "0.8263312817768264"
        ],
        [
         "12",
         "3",
         "Logistic Regression ROS",
         "0.7011596788581623",
         "0.6989710448456692",
         "0.7011590215101146",
         "0.6976639585938288",
         "0.8272666902202883",
         "0.7305524239007892",
         "0.6937901498929336",
         "0.6510574018126888",
         "0.5766657746855767",
         "0.7153033088235294",
         "0.833021139951833"
        ],
        [
         "13",
         "4",
         "Logistic Regression ROS",
         "0.70035682426405",
         "0.6981119672336309",
         "0.7003559281502548",
         "0.6975936354791465",
         "0.8253481728736514",
         "0.7252530933633295",
         "0.6903104925053534",
         "0.6459189665296535",
         "0.5887075194005887",
         "0.7231638418079096",
         "0.822049772544822"
        ],
        [
         "14",
         "5",
         "Logistic Regression ROS",
         "0.6900981266726137",
         "0.6884996075789359",
         "0.6900964982197623",
         "0.6869127787541184",
         "0.8219358428215711",
         "0.726693688477128",
         "0.6718415417558886",
         "0.6345926800472255",
         "0.5753278030505753",
         "0.7042124542124543",
         "0.8231201498528231"
        ],
        [
         "15",
         "6",
         "Logistic Regression ROS",
         "0.7008653760371131",
         "0.6997287068545028",
         "0.7008671937568199",
         "0.698160699730094",
         "0.8290010433337563",
         "0.7383855981416957",
         "0.6804923735616805",
         "0.6470759383183008",
         "0.5952890792291221",
         "0.713724584103512",
         "0.8268201284796574"
        ],
        [
         "16",
         "7",
         "Logistic Regression ROS",
         "0.704612365063788",
         "0.7031875379848378",
         "0.7046132039559647",
         "0.7014531782125789",
         "0.8294683718542254",
         "0.7334839073969509",
         "0.6952100615466952",
         "0.6643504531722054",
         "0.5885974304068522",
         "0.711728253385357",
         "0.8300321199143469"
        ],
        [
         "17",
         "8",
         "Logistic Regression ROS",
         "0.7043447229904541",
         "0.703222244684237",
         "0.7043458722572297",
         "0.7014204741173055",
         "0.8326585753137813",
         "0.7382857142857143",
         "0.6914637409686915",
         "0.6607036374478235",
         "0.5931477516059958",
         "0.7106773823191733",
         "0.8284261241970021"
        ],
        [
         "18",
         "9",
         "Logistic Regression ROS",
         "0.6952448924971005",
         "0.6932699505766201",
         "0.6952454924872081",
         "0.6920495808615524",
         "0.825965397196161",
         "0.7305508233957978",
         "0.6885202033716885",
         "0.6399164677804295",
         "0.5741434689507494",
         "0.7093425605536332",
         "0.8230728051391863"
        ],
        [
         "19",
         "10",
         "Logistic Regression ROS",
         "0.6979213132304398",
         "0.6966099945065958",
         "0.6979224862702719",
         "0.6950381172467873",
         "0.823793579459549",
         "0.7313518148042297",
         "0.6847738827936848",
         "0.6513353115727003",
         "0.5875267665952891",
         "0.7071428571428572",
         "0.8214668094218416"
        ],
        [
         "20",
         "1",
         "Logistic Regression SMOTE",
         "0.7255129348795718",
         "0.7252779822641736",
         "0.7255091980058589",
         "0.7215642249772665",
         "0.8624971680364415",
         "0.7509556012937372",
         "0.6836188436830836",
         "0.6998175182481752",
         "0.6157345464276157",
         "0.7250608272506083",
         "0.8771742039068772"
        ],
        [
         "21",
         "2",
         "Logistic Regression SMOTE",
         "0.7260481712756467",
         "0.7260555669422839",
         "0.726043383897392",
         "0.7220140200725619",
         "0.8589909973026444",
         "0.7514208794495961",
         "0.6723768736616702",
         "0.7011736382786639",
         "0.6234947819106235",
         "0.7255721830985915",
         "0.8822584961198823"
        ],
        [
         "22",
         "3",
         "Logistic Regression SMOTE",
         "0.7157894736842105",
         "0.7153603238122749",
         "0.7157853387341163",
         "0.7112171181166503",
         "0.8542449100016198",
         "0.7375405485107638",
         "0.6694325481798715",
         "0.6920703486578217",
         "0.6002140754616002",
         "0.7164700742682394",
         "0.8777093925608777"
        ],
        [
         "23",
         "4",
         "Logistic Regression SMOTE",
         "0.7285459411239964",
         "0.7276493640041974",
         "0.7285420292130492",
         "0.7244000036880878",
         "0.8602899807983496",
         "0.7451208855228663",
         "0.6846895074946466",
         "0.7042124542124543",
         "0.6173401123896174",
         "0.7336147522772717",
         "0.8835964677548835"
        ],
        [
         "24",
         "5",
         "Logistic Regression SMOTE",
         "0.7163247100802854",
         "0.7158706104246368",
         "0.7163201215080708",
         "0.7118027090544223",
         "0.8577293608127068",
         "0.7439353099730458",
         "0.6648822269807281",
         "0.6847694174757282",
         "0.6039603960396039",
         "0.7189071038251366",
         "0.8801177415038801"
        ],
        [
         "25",
         "6",
         "Logistic Regression SMOTE",
         "0.7213846016593809",
         "0.7218147714563431",
         "0.7213902556700487",
         "0.7175720715488659",
         "0.855851596754336",
         "0.7456033959975743",
         "0.658014450093658",
         "0.7007125890736342",
         "0.6316916488222698",
         "0.7191283292978208",
         "0.8744646680942184"
        ],
        [
         "26",
         "7",
         "Logistic Regression SMOTE",
         "0.7324471406905165",
         "0.7324502392045008",
         "0.732451083814325",
         "0.7284785279655651",
         "0.8657385769103091",
         "0.7551379917792131",
         "0.6882526090446882",
         "0.7121906507791017",
         "0.6239293361884368",
         "0.7300220750551877",
         "0.8851713062098501"
        ],
        [
         "27",
         "8",
         "Logistic Regression SMOTE",
         "0.7220090998304933",
         "0.7223734275375643",
         "0.7220142843035967",
         "0.7178167296658157",
         "0.8579526140361761",
         "0.7443744374437443",
         "0.6639015252876639",
         "0.7041868932038835",
         "0.6212526766595289",
         "0.7185589519650655",
         "0.8808886509635975"
        ],
        [
         "28",
         "9",
         "Logistic Regression SMOTE",
         "0.7185297528771523",
         "0.718314938596203",
         "0.7185341732853764",
         "0.7143766613445971",
         "0.8559151230790008",
         "0.7438262421898244",
         "0.668985817500669",
         "0.6921212121212121",
         "0.6113490364025695",
         "0.7189973614775725",
         "0.8752676659528907"
        ],
        [
         "29",
         "10",
         "Logistic Regression SMOTE",
         "0.723347310197163",
         "0.7238113825508424",
         "0.7233519933103328",
         "0.7190311707036073",
         "0.8592036206276686",
         "0.7499252168710739",
         "0.6708589777896709",
         "0.7030581039755351",
         "0.6153640256959315",
         "0.7184508268059182",
         "0.8838329764453962"
        ],
        [
         "30",
         "1",
         "XGBoost",
         "0.7812939521800282",
         "0.7673492968096957",
         "0.7706485440270384",
         "0.7689217119733837",
         "0.9120215632006418",
         "0.7789669613773849",
         "0.7664835164835165",
         "0.8026245313336904",
         "0.801980198019802",
         "0.7204563977180114",
         "0.7434819175777965"
        ],
        [
         "31",
         "2",
         "XGBoost",
         "0.7731364275668073",
         "0.7594143621266606",
         "0.7654159686173442",
         "0.7623027108581312",
         "0.909070768363665",
         "0.7630004601932812",
         "0.7591575091575091",
         "0.7977285018929151",
         "0.7894032646507894",
         "0.7175141242937854",
         "0.7476871320437343"
        ],
        [
         "32",
         "3",
         "XGBoost",
         "0.7745428973277074",
         "0.7588738122503852",
         "0.7636660936059769",
         "0.7611750676437875",
         "0.9081225744207824",
         "0.7761332099907493",
         "0.7686669720568026",
         "0.7974137931034483",
         "0.7920792079207921",
         "0.7030744336569579",
         "0.7302521008403361"
        ],
        [
         "33",
         "4",
         "XGBoost",
         "0.7794655414908579",
         "0.7629647183582922",
         "0.7697135411813161",
         "0.7662033344148723",
         "0.9084164689002584",
         "0.776044056906838",
         "0.7746220797068255",
         "0.8060227889310906",
         "0.7950227455177951",
         "0.7068273092369478",
         "0.7394957983193278"
        ],
        [
         "34",
         "5",
         "XGBoost",
         "0.7781997187060479",
         "0.760890247452739",
         "0.7713917932480068",
         "0.7657963317883746",
         "0.907566452029844",
         "0.7809479981592269",
         "0.777370590929913",
         "0.8050847457627118",
         "0.7880652930157881",
         "0.6966379984362784",
         "0.7487394957983193"
        ],
        [
         "35",
         "6",
         "XGBoost",
         "0.7825597749648383",
         "0.7692866245919113",
         "0.7687732667327074",
         "0.768864128684314",
         "0.91434261138494",
         "0.7914081145584726",
         "0.7591575091575091",
         "0.7984189723320159",
         "0.8110278372591007",
         "0.7180327868852459",
         "0.7361344537815127"
        ],
        [
         "36",
         "7",
         "XGBoost",
         "0.7763713080168776",
         "0.7596496291514742",
         "0.7679603796715524",
         "0.7635840542421688",
         "0.9095768711141198",
         "0.7734806629834254",
         "0.7692307692307693",
         "0.8036442752243677",
         "0.7909528907922913",
         "0.7018239492466296",
         "0.7436974789915967"
        ],
        [
         "37",
         "8",
         "XGBoost",
         "0.7828105218736813",
         "0.7679693308539756",
         "0.7728224751835636",
         "0.7702332794843078",
         "0.91313288708458",
         "0.7845433255269321",
         "0.766941391941392",
         "0.8045052292839904",
         "0.8029978586723768",
         "0.714859437751004",
         "0.7485281749369218"
        ],
        [
         "38",
         "9",
         "XGBoost",
         "0.7749331832887889",
         "0.7562932063185405",
         "0.7662924581799061",
         "0.7609478499904924",
         "0.9082100698726888",
         "0.7802400738688827",
         "0.7738095238095238",
         "0.8029475982532751",
         "0.7874732334047109",
         "0.6856919468334637",
         "0.7375946173254836"
        ],
        [
         "39",
         "10",
         "XGBoost",
         "0.7826698551132367",
         "0.7703893204680409",
         "0.7759778084853769",
         "0.7730941441450278",
         "0.910388887172274",
         "0.7759412304866851",
         "0.7738095238095238",
         "0.803677663601947",
         "0.7955032119914347",
         "0.7315490673154906",
         "0.7586206896551724"
        ],
        [
         "40",
         "1",
         "XGBoost ROS",
         "0.8354148082069581",
         "0.8357542129056702",
         "0.8354183868818041",
         "0.8317884770389586",
         "0.9386973722818596",
         "0.8365728900255754",
         "0.8755353319057816",
         "0.8387728459530026",
         "0.6877174203906877",
         "0.8319169027384324",
         "0.943002408348943"
        ],
        [
         "41",
         "2",
         "XGBoost ROS",
         "0.832114183764496",
         "0.8322607312524642",
         "0.8321177942205357",
         "0.8284234394240819",
         "0.9365109892563526",
         "0.8326947637292464",
         "0.8725910064239829",
         "0.8335509138381201",
         "0.6834359111586834",
         "0.830536516190026",
         "0.9403264650789404"
        ],
        [
         "42",
         "3",
         "XGBoost ROS",
         "0.8301516503122213",
         "0.8310544435689482",
         "0.8301548628154093",
         "0.8262190247248569",
         "0.936594191916656",
         "0.8340206185567011",
         "0.8661670235546038",
         "0.8379629629629629",
         "0.6780840246186781",
         "0.8211797491871807",
         "0.9462135402729462"
        ],
        [
         "43",
         "4",
         "XGBoost ROS",
         "0.8375557537912578",
         "0.8377690443646936",
         "0.8375596906296336",
         "0.8339259903079034",
         "0.9374043191033414",
         "0.837954718901043",
         "0.8816916488222698",
         "0.8398042414355628",
         "0.6887877976986888",
         "0.8355481727574751",
         "0.9421996253679422"
        ],
        [
         "44",
         "5",
         "XGBoost ROS",
         "0.8320249776984835",
         "0.8323862909052492",
         "0.8320287871139103",
         "0.8278142291921234",
         "0.9362775560095032",
         "0.8340990301174068",
         "0.8747323340471093",
         "0.8351028533510285",
         "0.6735349210596735",
         "0.8279569892473119",
         "0.9478191062349478"
        ],
        [
         "45",
         "6",
         "XGBoost ROS",
         "0.8391471139263093",
         "0.8392197189462832",
         "0.839143888200962",
         "0.8360019188190697",
         "0.9408653783830816",
         "0.8487285936689154",
         "0.8753010436178753",
         "0.836741214057508",
         "0.701017130620985",
         "0.832189349112426",
         "0.9411134903640256"
        ],
        [
         "46",
         "7",
         "XGBoost ROS",
         "0.8319207779462932",
         "0.8321001391536512",
         "0.8319173849788474",
         "0.8283137052914946",
         "0.9369091630178582",
         "0.8378865979381444",
         "0.8699491570778699",
         "0.831979200519987",
         "0.6852248394004282",
         "0.8264346190028222",
         "0.940578158458244"
        ],
        [
         "47",
         "8",
         "XGBoost ROS",
         "0.8369167633151932",
         "0.8377824282461738",
         "0.8369127655864145",
         "0.8332183880977483",
         "0.9408284157239062",
         "0.8418497700562085",
         "0.8817233074658817",
         "0.8445834705301284",
         "0.6865631691648822",
         "0.8269140441521842",
         "0.9424518201284796"
        ],
        [
         "48",
         "9",
         "XGBoost ROS",
         "0.8241591578196092",
         "0.8247102466338889",
         "0.8241550245944208",
         "0.8199922523672986",
         "0.9338954379107356",
         "0.8309067688378033",
         "0.8704843457318705",
         "0.8277352843365481",
         "0.666220556745182",
         "0.8154886867273151",
         "0.9357601713062098"
        ],
        [
         "49",
         "10",
         "XGBoost ROS",
         "0.8289767151396199",
         "0.8286932261629224",
         "0.8289738235065477",
         "0.8254368222036431",
         "0.9367141571890704",
         "0.8322130299896587",
         "0.8613861386138614",
         "0.8247422680412371",
         "0.6852248394004282",
         "0.8291243804578712",
         "0.9403104925053534"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 60
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>model name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro precision</th>\n",
       "      <th>macro recall</th>\n",
       "      <th>macro f1</th>\n",
       "      <th>roc auc</th>\n",
       "      <th>class 0.0 precision</th>\n",
       "      <th>class 0.0 recall</th>\n",
       "      <th>class 1.0 precision</th>\n",
       "      <th>class 1.0 recall</th>\n",
       "      <th>class 2.0 precision</th>\n",
       "      <th>class 2.0 recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.662447</td>\n",
       "      <td>0.642660</td>\n",
       "      <td>0.635844</td>\n",
       "      <td>0.634841</td>\n",
       "      <td>0.815304</td>\n",
       "      <td>0.694526</td>\n",
       "      <td>0.540293</td>\n",
       "      <td>0.687931</td>\n",
       "      <td>0.747391</td>\n",
       "      <td>0.545522</td>\n",
       "      <td>0.619849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.648805</td>\n",
       "      <td>0.629192</td>\n",
       "      <td>0.624645</td>\n",
       "      <td>0.623378</td>\n",
       "      <td>0.811437</td>\n",
       "      <td>0.672206</td>\n",
       "      <td>0.537088</td>\n",
       "      <td>0.674789</td>\n",
       "      <td>0.726251</td>\n",
       "      <td>0.540581</td>\n",
       "      <td>0.610597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.654149</td>\n",
       "      <td>0.633419</td>\n",
       "      <td>0.631316</td>\n",
       "      <td>0.629259</td>\n",
       "      <td>0.811161</td>\n",
       "      <td>0.670601</td>\n",
       "      <td>0.546496</td>\n",
       "      <td>0.683325</td>\n",
       "      <td>0.728124</td>\n",
       "      <td>0.546331</td>\n",
       "      <td>0.619328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.655274</td>\n",
       "      <td>0.635565</td>\n",
       "      <td>0.631775</td>\n",
       "      <td>0.631068</td>\n",
       "      <td>0.809459</td>\n",
       "      <td>0.663889</td>\n",
       "      <td>0.547412</td>\n",
       "      <td>0.682591</td>\n",
       "      <td>0.730265</td>\n",
       "      <td>0.560213</td>\n",
       "      <td>0.617647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.652461</td>\n",
       "      <td>0.633642</td>\n",
       "      <td>0.631076</td>\n",
       "      <td>0.628585</td>\n",
       "      <td>0.808803</td>\n",
       "      <td>0.671461</td>\n",
       "      <td>0.534585</td>\n",
       "      <td>0.678803</td>\n",
       "      <td>0.728392</td>\n",
       "      <td>0.550661</td>\n",
       "      <td>0.630252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.667229</td>\n",
       "      <td>0.648072</td>\n",
       "      <td>0.636790</td>\n",
       "      <td>0.639103</td>\n",
       "      <td>0.818342</td>\n",
       "      <td>0.697595</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.688372</td>\n",
       "      <td>0.752677</td>\n",
       "      <td>0.558249</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.660338</td>\n",
       "      <td>0.640918</td>\n",
       "      <td>0.636118</td>\n",
       "      <td>0.634445</td>\n",
       "      <td>0.816665</td>\n",
       "      <td>0.683353</td>\n",
       "      <td>0.537546</td>\n",
       "      <td>0.686556</td>\n",
       "      <td>0.742238</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>0.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.651287</td>\n",
       "      <td>0.631882</td>\n",
       "      <td>0.623944</td>\n",
       "      <td>0.623940</td>\n",
       "      <td>0.814761</td>\n",
       "      <td>0.666862</td>\n",
       "      <td>0.520604</td>\n",
       "      <td>0.676694</td>\n",
       "      <td>0.740632</td>\n",
       "      <td>0.552091</td>\n",
       "      <td>0.610597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.651006</td>\n",
       "      <td>0.629906</td>\n",
       "      <td>0.627827</td>\n",
       "      <td>0.625117</td>\n",
       "      <td>0.810023</td>\n",
       "      <td>0.680754</td>\n",
       "      <td>0.545788</td>\n",
       "      <td>0.680322</td>\n",
       "      <td>0.724572</td>\n",
       "      <td>0.528644</td>\n",
       "      <td>0.613120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.651287</td>\n",
       "      <td>0.632062</td>\n",
       "      <td>0.628146</td>\n",
       "      <td>0.626266</td>\n",
       "      <td>0.809271</td>\n",
       "      <td>0.665701</td>\n",
       "      <td>0.526099</td>\n",
       "      <td>0.678147</td>\n",
       "      <td>0.732602</td>\n",
       "      <td>0.552339</td>\n",
       "      <td>0.625736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.697770</td>\n",
       "      <td>0.695671</td>\n",
       "      <td>0.697769</td>\n",
       "      <td>0.694681</td>\n",
       "      <td>0.830114</td>\n",
       "      <td>0.725838</td>\n",
       "      <td>0.689507</td>\n",
       "      <td>0.646989</td>\n",
       "      <td>0.580680</td>\n",
       "      <td>0.714186</td>\n",
       "      <td>0.823120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.699019</td>\n",
       "      <td>0.697400</td>\n",
       "      <td>0.699017</td>\n",
       "      <td>0.696026</td>\n",
       "      <td>0.825430</td>\n",
       "      <td>0.728910</td>\n",
       "      <td>0.682281</td>\n",
       "      <td>0.650784</td>\n",
       "      <td>0.588440</td>\n",
       "      <td>0.712506</td>\n",
       "      <td>0.826331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.701160</td>\n",
       "      <td>0.698971</td>\n",
       "      <td>0.701159</td>\n",
       "      <td>0.697664</td>\n",
       "      <td>0.827267</td>\n",
       "      <td>0.730552</td>\n",
       "      <td>0.693790</td>\n",
       "      <td>0.651057</td>\n",
       "      <td>0.576666</td>\n",
       "      <td>0.715303</td>\n",
       "      <td>0.833021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.700357</td>\n",
       "      <td>0.698112</td>\n",
       "      <td>0.700356</td>\n",
       "      <td>0.697594</td>\n",
       "      <td>0.825348</td>\n",
       "      <td>0.725253</td>\n",
       "      <td>0.690310</td>\n",
       "      <td>0.645919</td>\n",
       "      <td>0.588708</td>\n",
       "      <td>0.723164</td>\n",
       "      <td>0.822050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.690098</td>\n",
       "      <td>0.688500</td>\n",
       "      <td>0.690096</td>\n",
       "      <td>0.686913</td>\n",
       "      <td>0.821936</td>\n",
       "      <td>0.726694</td>\n",
       "      <td>0.671842</td>\n",
       "      <td>0.634593</td>\n",
       "      <td>0.575328</td>\n",
       "      <td>0.704212</td>\n",
       "      <td>0.823120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.700865</td>\n",
       "      <td>0.699729</td>\n",
       "      <td>0.700867</td>\n",
       "      <td>0.698161</td>\n",
       "      <td>0.829001</td>\n",
       "      <td>0.738386</td>\n",
       "      <td>0.680492</td>\n",
       "      <td>0.647076</td>\n",
       "      <td>0.595289</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.826820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.704612</td>\n",
       "      <td>0.703188</td>\n",
       "      <td>0.704613</td>\n",
       "      <td>0.701453</td>\n",
       "      <td>0.829468</td>\n",
       "      <td>0.733484</td>\n",
       "      <td>0.695210</td>\n",
       "      <td>0.664350</td>\n",
       "      <td>0.588597</td>\n",
       "      <td>0.711728</td>\n",
       "      <td>0.830032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.704345</td>\n",
       "      <td>0.703222</td>\n",
       "      <td>0.704346</td>\n",
       "      <td>0.701420</td>\n",
       "      <td>0.832659</td>\n",
       "      <td>0.738286</td>\n",
       "      <td>0.691464</td>\n",
       "      <td>0.660704</td>\n",
       "      <td>0.593148</td>\n",
       "      <td>0.710677</td>\n",
       "      <td>0.828426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.695245</td>\n",
       "      <td>0.693270</td>\n",
       "      <td>0.695245</td>\n",
       "      <td>0.692050</td>\n",
       "      <td>0.825965</td>\n",
       "      <td>0.730551</td>\n",
       "      <td>0.688520</td>\n",
       "      <td>0.639916</td>\n",
       "      <td>0.574143</td>\n",
       "      <td>0.709343</td>\n",
       "      <td>0.823073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.697921</td>\n",
       "      <td>0.696610</td>\n",
       "      <td>0.697922</td>\n",
       "      <td>0.695038</td>\n",
       "      <td>0.823794</td>\n",
       "      <td>0.731352</td>\n",
       "      <td>0.684774</td>\n",
       "      <td>0.651335</td>\n",
       "      <td>0.587527</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.821467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.725513</td>\n",
       "      <td>0.725278</td>\n",
       "      <td>0.725509</td>\n",
       "      <td>0.721564</td>\n",
       "      <td>0.862497</td>\n",
       "      <td>0.750956</td>\n",
       "      <td>0.683619</td>\n",
       "      <td>0.699818</td>\n",
       "      <td>0.615735</td>\n",
       "      <td>0.725061</td>\n",
       "      <td>0.877174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.726048</td>\n",
       "      <td>0.726056</td>\n",
       "      <td>0.726043</td>\n",
       "      <td>0.722014</td>\n",
       "      <td>0.858991</td>\n",
       "      <td>0.751421</td>\n",
       "      <td>0.672377</td>\n",
       "      <td>0.701174</td>\n",
       "      <td>0.623495</td>\n",
       "      <td>0.725572</td>\n",
       "      <td>0.882258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.715360</td>\n",
       "      <td>0.715785</td>\n",
       "      <td>0.711217</td>\n",
       "      <td>0.854245</td>\n",
       "      <td>0.737541</td>\n",
       "      <td>0.669433</td>\n",
       "      <td>0.692070</td>\n",
       "      <td>0.600214</td>\n",
       "      <td>0.716470</td>\n",
       "      <td>0.877709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.728546</td>\n",
       "      <td>0.727649</td>\n",
       "      <td>0.728542</td>\n",
       "      <td>0.724400</td>\n",
       "      <td>0.860290</td>\n",
       "      <td>0.745121</td>\n",
       "      <td>0.684690</td>\n",
       "      <td>0.704212</td>\n",
       "      <td>0.617340</td>\n",
       "      <td>0.733615</td>\n",
       "      <td>0.883596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.716325</td>\n",
       "      <td>0.715871</td>\n",
       "      <td>0.716320</td>\n",
       "      <td>0.711803</td>\n",
       "      <td>0.857729</td>\n",
       "      <td>0.743935</td>\n",
       "      <td>0.664882</td>\n",
       "      <td>0.684769</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>0.718907</td>\n",
       "      <td>0.880118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.721385</td>\n",
       "      <td>0.721815</td>\n",
       "      <td>0.721390</td>\n",
       "      <td>0.717572</td>\n",
       "      <td>0.855852</td>\n",
       "      <td>0.745603</td>\n",
       "      <td>0.658014</td>\n",
       "      <td>0.700713</td>\n",
       "      <td>0.631692</td>\n",
       "      <td>0.719128</td>\n",
       "      <td>0.874465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.732447</td>\n",
       "      <td>0.732450</td>\n",
       "      <td>0.732451</td>\n",
       "      <td>0.728479</td>\n",
       "      <td>0.865739</td>\n",
       "      <td>0.755138</td>\n",
       "      <td>0.688253</td>\n",
       "      <td>0.712191</td>\n",
       "      <td>0.623929</td>\n",
       "      <td>0.730022</td>\n",
       "      <td>0.885171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.722009</td>\n",
       "      <td>0.722373</td>\n",
       "      <td>0.722014</td>\n",
       "      <td>0.717817</td>\n",
       "      <td>0.857953</td>\n",
       "      <td>0.744374</td>\n",
       "      <td>0.663902</td>\n",
       "      <td>0.704187</td>\n",
       "      <td>0.621253</td>\n",
       "      <td>0.718559</td>\n",
       "      <td>0.880889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.718530</td>\n",
       "      <td>0.718315</td>\n",
       "      <td>0.718534</td>\n",
       "      <td>0.714377</td>\n",
       "      <td>0.855915</td>\n",
       "      <td>0.743826</td>\n",
       "      <td>0.668986</td>\n",
       "      <td>0.692121</td>\n",
       "      <td>0.611349</td>\n",
       "      <td>0.718997</td>\n",
       "      <td>0.875268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.723347</td>\n",
       "      <td>0.723811</td>\n",
       "      <td>0.723352</td>\n",
       "      <td>0.719031</td>\n",
       "      <td>0.859204</td>\n",
       "      <td>0.749925</td>\n",
       "      <td>0.670859</td>\n",
       "      <td>0.703058</td>\n",
       "      <td>0.615364</td>\n",
       "      <td>0.718451</td>\n",
       "      <td>0.883833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.781294</td>\n",
       "      <td>0.767349</td>\n",
       "      <td>0.770649</td>\n",
       "      <td>0.768922</td>\n",
       "      <td>0.912022</td>\n",
       "      <td>0.778967</td>\n",
       "      <td>0.766484</td>\n",
       "      <td>0.802625</td>\n",
       "      <td>0.801980</td>\n",
       "      <td>0.720456</td>\n",
       "      <td>0.743482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.773136</td>\n",
       "      <td>0.759414</td>\n",
       "      <td>0.765416</td>\n",
       "      <td>0.762303</td>\n",
       "      <td>0.909071</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>0.759158</td>\n",
       "      <td>0.797729</td>\n",
       "      <td>0.789403</td>\n",
       "      <td>0.717514</td>\n",
       "      <td>0.747687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.774543</td>\n",
       "      <td>0.758874</td>\n",
       "      <td>0.763666</td>\n",
       "      <td>0.761175</td>\n",
       "      <td>0.908123</td>\n",
       "      <td>0.776133</td>\n",
       "      <td>0.768667</td>\n",
       "      <td>0.797414</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.703074</td>\n",
       "      <td>0.730252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.779466</td>\n",
       "      <td>0.762965</td>\n",
       "      <td>0.769714</td>\n",
       "      <td>0.766203</td>\n",
       "      <td>0.908416</td>\n",
       "      <td>0.776044</td>\n",
       "      <td>0.774622</td>\n",
       "      <td>0.806023</td>\n",
       "      <td>0.795023</td>\n",
       "      <td>0.706827</td>\n",
       "      <td>0.739496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.778200</td>\n",
       "      <td>0.760890</td>\n",
       "      <td>0.771392</td>\n",
       "      <td>0.765796</td>\n",
       "      <td>0.907566</td>\n",
       "      <td>0.780948</td>\n",
       "      <td>0.777371</td>\n",
       "      <td>0.805085</td>\n",
       "      <td>0.788065</td>\n",
       "      <td>0.696638</td>\n",
       "      <td>0.748739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.782560</td>\n",
       "      <td>0.769287</td>\n",
       "      <td>0.768773</td>\n",
       "      <td>0.768864</td>\n",
       "      <td>0.914343</td>\n",
       "      <td>0.791408</td>\n",
       "      <td>0.759158</td>\n",
       "      <td>0.798419</td>\n",
       "      <td>0.811028</td>\n",
       "      <td>0.718033</td>\n",
       "      <td>0.736134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.776371</td>\n",
       "      <td>0.759650</td>\n",
       "      <td>0.767960</td>\n",
       "      <td>0.763584</td>\n",
       "      <td>0.909577</td>\n",
       "      <td>0.773481</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.803644</td>\n",
       "      <td>0.790953</td>\n",
       "      <td>0.701824</td>\n",
       "      <td>0.743697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.782811</td>\n",
       "      <td>0.767969</td>\n",
       "      <td>0.772822</td>\n",
       "      <td>0.770233</td>\n",
       "      <td>0.913133</td>\n",
       "      <td>0.784543</td>\n",
       "      <td>0.766941</td>\n",
       "      <td>0.804505</td>\n",
       "      <td>0.802998</td>\n",
       "      <td>0.714859</td>\n",
       "      <td>0.748528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.774933</td>\n",
       "      <td>0.756293</td>\n",
       "      <td>0.766292</td>\n",
       "      <td>0.760948</td>\n",
       "      <td>0.908210</td>\n",
       "      <td>0.780240</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.802948</td>\n",
       "      <td>0.787473</td>\n",
       "      <td>0.685692</td>\n",
       "      <td>0.737595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.782670</td>\n",
       "      <td>0.770389</td>\n",
       "      <td>0.775978</td>\n",
       "      <td>0.773094</td>\n",
       "      <td>0.910389</td>\n",
       "      <td>0.775941</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.803678</td>\n",
       "      <td>0.795503</td>\n",
       "      <td>0.731549</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.835415</td>\n",
       "      <td>0.835754</td>\n",
       "      <td>0.835418</td>\n",
       "      <td>0.831788</td>\n",
       "      <td>0.938697</td>\n",
       "      <td>0.836573</td>\n",
       "      <td>0.875535</td>\n",
       "      <td>0.838773</td>\n",
       "      <td>0.687717</td>\n",
       "      <td>0.831917</td>\n",
       "      <td>0.943002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.832114</td>\n",
       "      <td>0.832261</td>\n",
       "      <td>0.832118</td>\n",
       "      <td>0.828423</td>\n",
       "      <td>0.936511</td>\n",
       "      <td>0.832695</td>\n",
       "      <td>0.872591</td>\n",
       "      <td>0.833551</td>\n",
       "      <td>0.683436</td>\n",
       "      <td>0.830537</td>\n",
       "      <td>0.940326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.830152</td>\n",
       "      <td>0.831054</td>\n",
       "      <td>0.830155</td>\n",
       "      <td>0.826219</td>\n",
       "      <td>0.936594</td>\n",
       "      <td>0.834021</td>\n",
       "      <td>0.866167</td>\n",
       "      <td>0.837963</td>\n",
       "      <td>0.678084</td>\n",
       "      <td>0.821180</td>\n",
       "      <td>0.946214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.837556</td>\n",
       "      <td>0.837769</td>\n",
       "      <td>0.837560</td>\n",
       "      <td>0.833926</td>\n",
       "      <td>0.937404</td>\n",
       "      <td>0.837955</td>\n",
       "      <td>0.881692</td>\n",
       "      <td>0.839804</td>\n",
       "      <td>0.688788</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.832025</td>\n",
       "      <td>0.832386</td>\n",
       "      <td>0.832029</td>\n",
       "      <td>0.827814</td>\n",
       "      <td>0.936278</td>\n",
       "      <td>0.834099</td>\n",
       "      <td>0.874732</td>\n",
       "      <td>0.835103</td>\n",
       "      <td>0.673535</td>\n",
       "      <td>0.827957</td>\n",
       "      <td>0.947819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>6</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.839147</td>\n",
       "      <td>0.839220</td>\n",
       "      <td>0.839144</td>\n",
       "      <td>0.836002</td>\n",
       "      <td>0.940865</td>\n",
       "      <td>0.848729</td>\n",
       "      <td>0.875301</td>\n",
       "      <td>0.836741</td>\n",
       "      <td>0.701017</td>\n",
       "      <td>0.832189</td>\n",
       "      <td>0.941113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>7</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.831921</td>\n",
       "      <td>0.832100</td>\n",
       "      <td>0.831917</td>\n",
       "      <td>0.828314</td>\n",
       "      <td>0.936909</td>\n",
       "      <td>0.837887</td>\n",
       "      <td>0.869949</td>\n",
       "      <td>0.831979</td>\n",
       "      <td>0.685225</td>\n",
       "      <td>0.826435</td>\n",
       "      <td>0.940578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>8</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.836917</td>\n",
       "      <td>0.837782</td>\n",
       "      <td>0.836913</td>\n",
       "      <td>0.833218</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.841850</td>\n",
       "      <td>0.881723</td>\n",
       "      <td>0.844583</td>\n",
       "      <td>0.686563</td>\n",
       "      <td>0.826914</td>\n",
       "      <td>0.942452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>9</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.824159</td>\n",
       "      <td>0.824710</td>\n",
       "      <td>0.824155</td>\n",
       "      <td>0.819992</td>\n",
       "      <td>0.933895</td>\n",
       "      <td>0.830907</td>\n",
       "      <td>0.870484</td>\n",
       "      <td>0.827735</td>\n",
       "      <td>0.666221</td>\n",
       "      <td>0.815489</td>\n",
       "      <td>0.935760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.828977</td>\n",
       "      <td>0.828693</td>\n",
       "      <td>0.828974</td>\n",
       "      <td>0.825437</td>\n",
       "      <td>0.936714</td>\n",
       "      <td>0.832213</td>\n",
       "      <td>0.861386</td>\n",
       "      <td>0.824742</td>\n",
       "      <td>0.685225</td>\n",
       "      <td>0.829124</td>\n",
       "      <td>0.940310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.843800</td>\n",
       "      <td>0.842987</td>\n",
       "      <td>0.843801</td>\n",
       "      <td>0.842196</td>\n",
       "      <td>0.951366</td>\n",
       "      <td>0.848621</td>\n",
       "      <td>0.856799</td>\n",
       "      <td>0.827310</td>\n",
       "      <td>0.747391</td>\n",
       "      <td>0.853028</td>\n",
       "      <td>0.927214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.844603</td>\n",
       "      <td>0.843706</td>\n",
       "      <td>0.844604</td>\n",
       "      <td>0.843036</td>\n",
       "      <td>0.951004</td>\n",
       "      <td>0.844709</td>\n",
       "      <td>0.854657</td>\n",
       "      <td>0.828065</td>\n",
       "      <td>0.750067</td>\n",
       "      <td>0.858344</td>\n",
       "      <td>0.929088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.838715</td>\n",
       "      <td>0.838310</td>\n",
       "      <td>0.838716</td>\n",
       "      <td>0.836868</td>\n",
       "      <td>0.948943</td>\n",
       "      <td>0.844291</td>\n",
       "      <td>0.849036</td>\n",
       "      <td>0.829922</td>\n",
       "      <td>0.737758</td>\n",
       "      <td>0.840717</td>\n",
       "      <td>0.929355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.842730</td>\n",
       "      <td>0.841988</td>\n",
       "      <td>0.842731</td>\n",
       "      <td>0.841197</td>\n",
       "      <td>0.951243</td>\n",
       "      <td>0.845033</td>\n",
       "      <td>0.853854</td>\n",
       "      <td>0.828402</td>\n",
       "      <td>0.749264</td>\n",
       "      <td>0.852528</td>\n",
       "      <td>0.925074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.838448</td>\n",
       "      <td>0.837719</td>\n",
       "      <td>0.838449</td>\n",
       "      <td>0.836665</td>\n",
       "      <td>0.950855</td>\n",
       "      <td>0.840699</td>\n",
       "      <td>0.850375</td>\n",
       "      <td>0.825554</td>\n",
       "      <td>0.738293</td>\n",
       "      <td>0.846906</td>\n",
       "      <td>0.926679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>6</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.845660</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.845660</td>\n",
       "      <td>0.844127</td>\n",
       "      <td>0.952043</td>\n",
       "      <td>0.849650</td>\n",
       "      <td>0.845330</td>\n",
       "      <td>0.834513</td>\n",
       "      <td>0.757227</td>\n",
       "      <td>0.851256</td>\n",
       "      <td>0.934422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.839682</td>\n",
       "      <td>0.838790</td>\n",
       "      <td>0.839681</td>\n",
       "      <td>0.837972</td>\n",
       "      <td>0.949182</td>\n",
       "      <td>0.846256</td>\n",
       "      <td>0.849880</td>\n",
       "      <td>0.821227</td>\n",
       "      <td>0.741435</td>\n",
       "      <td>0.848886</td>\n",
       "      <td>0.927730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>8</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.844143</td>\n",
       "      <td>0.843637</td>\n",
       "      <td>0.844142</td>\n",
       "      <td>0.842649</td>\n",
       "      <td>0.951832</td>\n",
       "      <td>0.848017</td>\n",
       "      <td>0.852556</td>\n",
       "      <td>0.833531</td>\n",
       "      <td>0.753212</td>\n",
       "      <td>0.849362</td>\n",
       "      <td>0.926660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>9</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.835222</td>\n",
       "      <td>0.834425</td>\n",
       "      <td>0.835221</td>\n",
       "      <td>0.833594</td>\n",
       "      <td>0.947121</td>\n",
       "      <td>0.848176</td>\n",
       "      <td>0.846133</td>\n",
       "      <td>0.814640</td>\n",
       "      <td>0.738758</td>\n",
       "      <td>0.840459</td>\n",
       "      <td>0.920771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.840575</td>\n",
       "      <td>0.839681</td>\n",
       "      <td>0.840574</td>\n",
       "      <td>0.838939</td>\n",
       "      <td>0.950906</td>\n",
       "      <td>0.843599</td>\n",
       "      <td>0.842922</td>\n",
       "      <td>0.823165</td>\n",
       "      <td>0.747591</td>\n",
       "      <td>0.852278</td>\n",
       "      <td>0.931210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold                 model name  accuracy  macro precision  macro recall  \\\n",
       "0      1        Logistic Regression  0.662447         0.642660      0.635844   \n",
       "1      2        Logistic Regression  0.648805         0.629192      0.624645   \n",
       "2      3        Logistic Regression  0.654149         0.633419      0.631316   \n",
       "3      4        Logistic Regression  0.655274         0.635565      0.631775   \n",
       "4      5        Logistic Regression  0.652461         0.633642      0.631076   \n",
       "5      6        Logistic Regression  0.667229         0.648072      0.636790   \n",
       "6      7        Logistic Regression  0.660338         0.640918      0.636118   \n",
       "7      8        Logistic Regression  0.651287         0.631882      0.623944   \n",
       "8      9        Logistic Regression  0.651006         0.629906      0.627827   \n",
       "9     10        Logistic Regression  0.651287         0.632062      0.628146   \n",
       "10     1    Logistic Regression ROS  0.697770         0.695671      0.697769   \n",
       "11     2    Logistic Regression ROS  0.699019         0.697400      0.699017   \n",
       "12     3    Logistic Regression ROS  0.701160         0.698971      0.701159   \n",
       "13     4    Logistic Regression ROS  0.700357         0.698112      0.700356   \n",
       "14     5    Logistic Regression ROS  0.690098         0.688500      0.690096   \n",
       "15     6    Logistic Regression ROS  0.700865         0.699729      0.700867   \n",
       "16     7    Logistic Regression ROS  0.704612         0.703188      0.704613   \n",
       "17     8    Logistic Regression ROS  0.704345         0.703222      0.704346   \n",
       "18     9    Logistic Regression ROS  0.695245         0.693270      0.695245   \n",
       "19    10    Logistic Regression ROS  0.697921         0.696610      0.697922   \n",
       "20     1  Logistic Regression SMOTE  0.725513         0.725278      0.725509   \n",
       "21     2  Logistic Regression SMOTE  0.726048         0.726056      0.726043   \n",
       "22     3  Logistic Regression SMOTE  0.715789         0.715360      0.715785   \n",
       "23     4  Logistic Regression SMOTE  0.728546         0.727649      0.728542   \n",
       "24     5  Logistic Regression SMOTE  0.716325         0.715871      0.716320   \n",
       "25     6  Logistic Regression SMOTE  0.721385         0.721815      0.721390   \n",
       "26     7  Logistic Regression SMOTE  0.732447         0.732450      0.732451   \n",
       "27     8  Logistic Regression SMOTE  0.722009         0.722373      0.722014   \n",
       "28     9  Logistic Regression SMOTE  0.718530         0.718315      0.718534   \n",
       "29    10  Logistic Regression SMOTE  0.723347         0.723811      0.723352   \n",
       "30     1                    XGBoost  0.781294         0.767349      0.770649   \n",
       "31     2                    XGBoost  0.773136         0.759414      0.765416   \n",
       "32     3                    XGBoost  0.774543         0.758874      0.763666   \n",
       "33     4                    XGBoost  0.779466         0.762965      0.769714   \n",
       "34     5                    XGBoost  0.778200         0.760890      0.771392   \n",
       "35     6                    XGBoost  0.782560         0.769287      0.768773   \n",
       "36     7                    XGBoost  0.776371         0.759650      0.767960   \n",
       "37     8                    XGBoost  0.782811         0.767969      0.772822   \n",
       "38     9                    XGBoost  0.774933         0.756293      0.766292   \n",
       "39    10                    XGBoost  0.782670         0.770389      0.775978   \n",
       "40     1                XGBoost ROS  0.835415         0.835754      0.835418   \n",
       "41     2                XGBoost ROS  0.832114         0.832261      0.832118   \n",
       "42     3                XGBoost ROS  0.830152         0.831054      0.830155   \n",
       "43     4                XGBoost ROS  0.837556         0.837769      0.837560   \n",
       "44     5                XGBoost ROS  0.832025         0.832386      0.832029   \n",
       "45     6                XGBoost ROS  0.839147         0.839220      0.839144   \n",
       "46     7                XGBoost ROS  0.831921         0.832100      0.831917   \n",
       "47     8                XGBoost ROS  0.836917         0.837782      0.836913   \n",
       "48     9                XGBoost ROS  0.824159         0.824710      0.824155   \n",
       "49    10                XGBoost ROS  0.828977         0.828693      0.828974   \n",
       "50     1              XGBoost SMOTE  0.843800         0.842987      0.843801   \n",
       "51     2              XGBoost SMOTE  0.844603         0.843706      0.844604   \n",
       "52     3              XGBoost SMOTE  0.838715         0.838310      0.838716   \n",
       "53     4              XGBoost SMOTE  0.842730         0.841988      0.842731   \n",
       "54     5              XGBoost SMOTE  0.838448         0.837719      0.838449   \n",
       "55     6              XGBoost SMOTE  0.845660         0.845140      0.845660   \n",
       "56     7              XGBoost SMOTE  0.839682         0.838790      0.839681   \n",
       "57     8              XGBoost SMOTE  0.844143         0.843637      0.844142   \n",
       "58     9              XGBoost SMOTE  0.835222         0.834425      0.835221   \n",
       "59    10              XGBoost SMOTE  0.840575         0.839681      0.840574   \n",
       "\n",
       "    macro f1   roc auc  class 0.0 precision  class 0.0 recall  \\\n",
       "0   0.634841  0.815304             0.694526          0.540293   \n",
       "1   0.623378  0.811437             0.672206          0.537088   \n",
       "2   0.629259  0.811161             0.670601          0.546496   \n",
       "3   0.631068  0.809459             0.663889          0.547412   \n",
       "4   0.628585  0.808803             0.671461          0.534585   \n",
       "5   0.639103  0.818342             0.697595          0.557692   \n",
       "6   0.634445  0.816665             0.683353          0.537546   \n",
       "7   0.623940  0.814761             0.666862          0.520604   \n",
       "8   0.625117  0.810023             0.680754          0.545788   \n",
       "9   0.626266  0.809271             0.665701          0.526099   \n",
       "10  0.694681  0.830114             0.725838          0.689507   \n",
       "11  0.696026  0.825430             0.728910          0.682281   \n",
       "12  0.697664  0.827267             0.730552          0.693790   \n",
       "13  0.697594  0.825348             0.725253          0.690310   \n",
       "14  0.686913  0.821936             0.726694          0.671842   \n",
       "15  0.698161  0.829001             0.738386          0.680492   \n",
       "16  0.701453  0.829468             0.733484          0.695210   \n",
       "17  0.701420  0.832659             0.738286          0.691464   \n",
       "18  0.692050  0.825965             0.730551          0.688520   \n",
       "19  0.695038  0.823794             0.731352          0.684774   \n",
       "20  0.721564  0.862497             0.750956          0.683619   \n",
       "21  0.722014  0.858991             0.751421          0.672377   \n",
       "22  0.711217  0.854245             0.737541          0.669433   \n",
       "23  0.724400  0.860290             0.745121          0.684690   \n",
       "24  0.711803  0.857729             0.743935          0.664882   \n",
       "25  0.717572  0.855852             0.745603          0.658014   \n",
       "26  0.728479  0.865739             0.755138          0.688253   \n",
       "27  0.717817  0.857953             0.744374          0.663902   \n",
       "28  0.714377  0.855915             0.743826          0.668986   \n",
       "29  0.719031  0.859204             0.749925          0.670859   \n",
       "30  0.768922  0.912022             0.778967          0.766484   \n",
       "31  0.762303  0.909071             0.763000          0.759158   \n",
       "32  0.761175  0.908123             0.776133          0.768667   \n",
       "33  0.766203  0.908416             0.776044          0.774622   \n",
       "34  0.765796  0.907566             0.780948          0.777371   \n",
       "35  0.768864  0.914343             0.791408          0.759158   \n",
       "36  0.763584  0.909577             0.773481          0.769231   \n",
       "37  0.770233  0.913133             0.784543          0.766941   \n",
       "38  0.760948  0.908210             0.780240          0.773810   \n",
       "39  0.773094  0.910389             0.775941          0.773810   \n",
       "40  0.831788  0.938697             0.836573          0.875535   \n",
       "41  0.828423  0.936511             0.832695          0.872591   \n",
       "42  0.826219  0.936594             0.834021          0.866167   \n",
       "43  0.833926  0.937404             0.837955          0.881692   \n",
       "44  0.827814  0.936278             0.834099          0.874732   \n",
       "45  0.836002  0.940865             0.848729          0.875301   \n",
       "46  0.828314  0.936909             0.837887          0.869949   \n",
       "47  0.833218  0.940828             0.841850          0.881723   \n",
       "48  0.819992  0.933895             0.830907          0.870484   \n",
       "49  0.825437  0.936714             0.832213          0.861386   \n",
       "50  0.842196  0.951366             0.848621          0.856799   \n",
       "51  0.843036  0.951004             0.844709          0.854657   \n",
       "52  0.836868  0.948943             0.844291          0.849036   \n",
       "53  0.841197  0.951243             0.845033          0.853854   \n",
       "54  0.836665  0.950855             0.840699          0.850375   \n",
       "55  0.844127  0.952043             0.849650          0.845330   \n",
       "56  0.837972  0.949182             0.846256          0.849880   \n",
       "57  0.842649  0.951832             0.848017          0.852556   \n",
       "58  0.833594  0.947121             0.848176          0.846133   \n",
       "59  0.838939  0.950906             0.843599          0.842922   \n",
       "\n",
       "    class 1.0 precision  class 1.0 recall  class 2.0 precision  \\\n",
       "0              0.687931          0.747391             0.545522   \n",
       "1              0.674789          0.726251             0.540581   \n",
       "2              0.683325          0.728124             0.546331   \n",
       "3              0.682591          0.730265             0.560213   \n",
       "4              0.678803          0.728392             0.550661   \n",
       "5              0.688372          0.752677             0.558249   \n",
       "6              0.686556          0.742238             0.552846   \n",
       "7              0.676694          0.740632             0.552091   \n",
       "8              0.680322          0.724572             0.528644   \n",
       "9              0.678147          0.732602             0.552339   \n",
       "10             0.646989          0.580680             0.714186   \n",
       "11             0.650784          0.588440             0.712506   \n",
       "12             0.651057          0.576666             0.715303   \n",
       "13             0.645919          0.588708             0.723164   \n",
       "14             0.634593          0.575328             0.704212   \n",
       "15             0.647076          0.595289             0.713725   \n",
       "16             0.664350          0.588597             0.711728   \n",
       "17             0.660704          0.593148             0.710677   \n",
       "18             0.639916          0.574143             0.709343   \n",
       "19             0.651335          0.587527             0.707143   \n",
       "20             0.699818          0.615735             0.725061   \n",
       "21             0.701174          0.623495             0.725572   \n",
       "22             0.692070          0.600214             0.716470   \n",
       "23             0.704212          0.617340             0.733615   \n",
       "24             0.684769          0.603960             0.718907   \n",
       "25             0.700713          0.631692             0.719128   \n",
       "26             0.712191          0.623929             0.730022   \n",
       "27             0.704187          0.621253             0.718559   \n",
       "28             0.692121          0.611349             0.718997   \n",
       "29             0.703058          0.615364             0.718451   \n",
       "30             0.802625          0.801980             0.720456   \n",
       "31             0.797729          0.789403             0.717514   \n",
       "32             0.797414          0.792079             0.703074   \n",
       "33             0.806023          0.795023             0.706827   \n",
       "34             0.805085          0.788065             0.696638   \n",
       "35             0.798419          0.811028             0.718033   \n",
       "36             0.803644          0.790953             0.701824   \n",
       "37             0.804505          0.802998             0.714859   \n",
       "38             0.802948          0.787473             0.685692   \n",
       "39             0.803678          0.795503             0.731549   \n",
       "40             0.838773          0.687717             0.831917   \n",
       "41             0.833551          0.683436             0.830537   \n",
       "42             0.837963          0.678084             0.821180   \n",
       "43             0.839804          0.688788             0.835548   \n",
       "44             0.835103          0.673535             0.827957   \n",
       "45             0.836741          0.701017             0.832189   \n",
       "46             0.831979          0.685225             0.826435   \n",
       "47             0.844583          0.686563             0.826914   \n",
       "48             0.827735          0.666221             0.815489   \n",
       "49             0.824742          0.685225             0.829124   \n",
       "50             0.827310          0.747391             0.853028   \n",
       "51             0.828065          0.750067             0.858344   \n",
       "52             0.829922          0.737758             0.840717   \n",
       "53             0.828402          0.749264             0.852528   \n",
       "54             0.825554          0.738293             0.846906   \n",
       "55             0.834513          0.757227             0.851256   \n",
       "56             0.821227          0.741435             0.848886   \n",
       "57             0.833531          0.753212             0.849362   \n",
       "58             0.814640          0.738758             0.840459   \n",
       "59             0.823165          0.747591             0.852278   \n",
       "\n",
       "    class 2.0 recall  \n",
       "0           0.619849  \n",
       "1           0.610597  \n",
       "2           0.619328  \n",
       "3           0.617647  \n",
       "4           0.630252  \n",
       "5           0.600000  \n",
       "6           0.628571  \n",
       "7           0.610597  \n",
       "8           0.613120  \n",
       "9           0.625736  \n",
       "10          0.823120  \n",
       "11          0.826331  \n",
       "12          0.833021  \n",
       "13          0.822050  \n",
       "14          0.823120  \n",
       "15          0.826820  \n",
       "16          0.830032  \n",
       "17          0.828426  \n",
       "18          0.823073  \n",
       "19          0.821467  \n",
       "20          0.877174  \n",
       "21          0.882258  \n",
       "22          0.877709  \n",
       "23          0.883596  \n",
       "24          0.880118  \n",
       "25          0.874465  \n",
       "26          0.885171  \n",
       "27          0.880889  \n",
       "28          0.875268  \n",
       "29          0.883833  \n",
       "30          0.743482  \n",
       "31          0.747687  \n",
       "32          0.730252  \n",
       "33          0.739496  \n",
       "34          0.748739  \n",
       "35          0.736134  \n",
       "36          0.743697  \n",
       "37          0.748528  \n",
       "38          0.737595  \n",
       "39          0.758621  \n",
       "40          0.943002  \n",
       "41          0.940326  \n",
       "42          0.946214  \n",
       "43          0.942200  \n",
       "44          0.947819  \n",
       "45          0.941113  \n",
       "46          0.940578  \n",
       "47          0.942452  \n",
       "48          0.935760  \n",
       "49          0.940310  \n",
       "50          0.927214  \n",
       "51          0.929088  \n",
       "52          0.929355  \n",
       "53          0.925074  \n",
       "54          0.926679  \n",
       "55          0.934422  \n",
       "56          0.927730  \n",
       "57          0.926660  \n",
       "58          0.920771  \n",
       "59          0.931210  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fold Results for Logistic Regression and XGBoost\n",
    "lg_xg_fold_df = pd.read_csv('./data/4_final_fold_results.csv')\n",
    "lg_xg_fold_df.columns = lg_xg_fold_df.columns.str.lower()\n",
    "lg_xg_fold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fold",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "roc_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "attention_weights",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "28b8d447-2f72-4386-a3ac-576b5f208341",
       "rows": [
        [
         "0",
         "1",
         "MLP",
         "0.7722925457102672",
         "0.7562375564259752",
         "0.7601958971339181",
         "0.7578178584708665",
         "0.905976937263489",
         null
        ],
        [
         "1",
         "2",
         "MLP",
         "0.7630098452883263",
         "0.74633725831325",
         "0.7555399875044055",
         "0.7507039652685714",
         "0.8994683934058368",
         null
        ],
        [
         "2",
         "3",
         "MLP",
         "0.7630098452883263",
         "0.7434964853948433",
         "0.7655724456026519",
         "0.7532235462947168",
         "0.9025752225707724",
         null
        ],
        [
         "3",
         "4",
         "MLP",
         "0.7748241912798874",
         "0.7593978701569913",
         "0.7715365060952063",
         "0.7649809438008702",
         "0.906906383030714",
         null
        ],
        [
         "4",
         "5",
         "MLP",
         "0.7624472573839662",
         "0.7453299568221511",
         "0.7579605005466036",
         "0.7511716417494553",
         "0.8985704590745766",
         null
        ],
        [
         "5",
         "6",
         "MLP",
         "0.7784810126582279",
         "0.7601821661117806",
         "0.7772956753117982",
         "0.7679328564899587",
         "0.9108676109787192",
         null
        ],
        [
         "6",
         "7",
         "MLP",
         "0.759774964838256",
         "0.7390067714902652",
         "0.7550099345728518",
         "0.746306447061365",
         "0.899783516938049",
         null
        ],
        [
         "7",
         "8",
         "MLP",
         "0.7676185117456745",
         "0.7503648652681361",
         "0.7602415547411883",
         "0.7549801211402155",
         "0.9038009028646428",
         null
        ],
        [
         "8",
         "9",
         "MLP",
         "0.7624138416092278",
         "0.744389579490965",
         "0.7468777497790775",
         "0.7455313578991327",
         "0.9017003145490669",
         null
        ],
        [
         "9",
         "10",
         "MLP",
         "0.7646645097763398",
         "0.7485345701466329",
         "0.7613256651941924",
         "0.7542417034855524",
         "0.9010425365632893",
         null
        ],
        [
         "10",
         "1",
         "MLP with ROS",
         "0.8457627118644068",
         "0.8478200928149141",
         "0.8457668955448123",
         "0.8402649724803256",
         "0.9405896248291358",
         null
        ],
        [
         "11",
         "2",
         "MLP with ROS",
         "0.8380909901873327",
         "0.8389797754059821",
         "0.8380947599071499",
         "0.8324222251849301",
         "0.9349975383572794",
         null
        ],
        [
         "12",
         "3",
         "MLP with ROS",
         "0.8421944692239072",
         "0.8461548839213485",
         "0.8422012393380087",
         "0.8351756191913133",
         "0.936841191570108",
         null
        ],
        [
         "13",
         "4",
         "MLP with ROS",
         "0.8570026761819803",
         "0.8602793723623386",
         "0.8570085790626635",
         "0.851574073559704",
         "0.9438086452663909",
         null
        ],
        [
         "14",
         "5",
         "MLP with ROS",
         "0.8459411239964317",
         "0.8486209166474924",
         "0.8459458170193429",
         "0.8399918330435089",
         "0.938918567158736",
         null
        ],
        [
         "15",
         "6",
         "MLP with ROS",
         "0.8569899188152378",
         "0.8597420928941953",
         "0.8569842023845883",
         "0.8517857989357741",
         "0.9429462617346028",
         null
        ],
        [
         "16",
         "7",
         "MLP with ROS",
         "0.8444107413685431",
         "0.8501249745674548",
         "0.8444041174763927",
         "0.8374587547079537",
         "0.935405761823236",
         null
        ],
        [
         "17",
         "8",
         "MLP with ROS",
         "0.8489606566152199",
         "0.8531307754015197",
         "0.848955369812113",
         "0.8430143851503242",
         "0.9403744861954234",
         null
        ],
        [
         "18",
         "9",
         "MLP with ROS",
         "0.8487822285663307",
         "0.8511968871799889",
         "0.8487767348411444",
         "0.8430389162941454",
         "0.9374404182614322",
         null
        ],
        [
         "19",
         "10",
         "MLP with ROS",
         "0.8502096529574449",
         "0.853328333799817",
         "0.850204167213411",
         "0.844495068514188",
         "0.9393686875582076",
         null
        ],
        [
         "20",
         "1",
         "MLP with SMOTE",
         "0.8456735057983943",
         "0.8446420657946082",
         "0.8456742355177703",
         "0.8435428282097752",
         "0.947983611268986",
         null
        ],
        [
         "21",
         "2",
         "MLP with SMOTE",
         "0.8390722569134701",
         "0.8391434141616915",
         "0.8390764404873846",
         "0.8364185998842962",
         "0.9459528803347004",
         null
        ],
        [
         "22",
         "3",
         "MLP with SMOTE",
         "0.8324710080285459",
         "0.8312494924130881",
         "0.832472151376258",
         "0.8301991292589502",
         "0.9430626614959696",
         null
        ],
        [
         "23",
         "4",
         "MLP with SMOTE",
         "0.8392506690454951",
         "0.8390862997316271",
         "0.8392515180391239",
         "0.8367767409634977",
         "0.9461329232037768",
         null
        ],
        [
         "24",
         "5",
         "MLP with SMOTE",
         "0.8405887600356824",
         "0.842515515644035",
         "0.840591901079106",
         "0.8368751129761347",
         "0.9469690170297614",
         null
        ],
        [
         "25",
         "6",
         "MLP with SMOTE",
         "0.8477116602729949",
         "0.8469081705784637",
         "0.8477112280936989",
         "0.8460367376180775",
         "0.948341412651133",
         null
        ],
        [
         "26",
         "7",
         "MLP with SMOTE",
         "0.8389686858774199",
         "0.8379439091926151",
         "0.8389675213831934",
         "0.8366034489376458",
         "0.9438308846165828",
         null
        ],
        [
         "27",
         "8",
         "MLP with SMOTE",
         "0.8376304755107503",
         "0.8364902637278494",
         "0.8376302182565035",
         "0.8356893559078039",
         "0.9458187473323866",
         null
        ],
        [
         "28",
         "9",
         "MLP with SMOTE",
         "0.8336158444107413",
         "0.8333373410971175",
         "0.8336135815676596",
         "0.8307351516563505",
         "0.9443525915289528",
         null
        ],
        [
         "29",
         "10",
         "MLP with SMOTE",
         "0.8393255419751985",
         "0.8382353293492275",
         "0.8393242421933031",
         "0.8372414949854067",
         "0.9464758614612472",
         null
        ],
        [
         "30",
         "1",
         "Transformer",
         "0.8036568213783404",
         "0.787590057175311",
         "0.8073980105338098",
         "0.7962897871278308",
         "0.9219625560246852",
         null
        ],
        [
         "31",
         "2",
         "Transformer",
         "0.7980309423347398",
         "0.7851309477251881",
         "0.80223485778012",
         "0.7922330456592297",
         "0.9186711329439768",
         null
        ],
        [
         "32",
         "3",
         "Transformer",
         "0.7933895921237694",
         "0.7777899038715148",
         "0.7924626797513089",
         "0.7843712778811597",
         "0.9189160517200116",
         null
        ],
        [
         "33",
         "4",
         "Transformer",
         "0.7988748241912799",
         "0.7815558755230798",
         "0.8015371765102769",
         "0.7903124182484803",
         "0.920873183972668",
         null
        ],
        [
         "34",
         "5",
         "Transformer",
         "0.79971870604782",
         "0.7848460467602784",
         "0.7979353324762641",
         "0.7905640326528033",
         "0.9188107207654652",
         null
        ],
        [
         "35",
         "6",
         "Transformer",
         "0.8075949367088607",
         "0.7921634508191557",
         "0.8054250465353879",
         "0.7983480765443346",
         "0.9255182905649404",
         null
        ],
        [
         "36",
         "7",
         "Transformer",
         "0.7880450070323488",
         "0.7734781306422617",
         "0.782409000496921",
         "0.7776952268152235",
         "0.9149232252715505",
         null
        ],
        [
         "37",
         "8",
         "Transformer",
         "0.7929385286256857",
         "0.7823359277245401",
         "0.7824644452274062",
         "0.7822521829550081",
         "0.918263192050506",
         null
        ],
        [
         "38",
         "9",
         "Transformer",
         "0.7995498663665775",
         "0.7826952322141053",
         "0.8006551310359596",
         "0.7907739523409326",
         "0.920592354256755",
         null
        ],
        [
         "39",
         "10",
         "Transformer",
         "0.7992685328456886",
         "0.7867798615583409",
         "0.7909249854379911",
         "0.7883992544101329",
         "0.9231076375778716",
         null
        ],
        [
         "40",
         "1",
         "Transformer with ROS",
         "0.8992863514719001",
         "0.9015255615367463",
         "0.8992910851360615",
         "0.896636773180992",
         "0.9599998769312632",
         null
        ],
        [
         "41",
         "2",
         "Transformer with ROS",
         "0.8950044603033006",
         "0.8976254621037917",
         "0.8950091461487141",
         "0.8920854013678444",
         "0.9571416165032276",
         null
        ],
        [
         "42",
         "3",
         "Transformer with ROS",
         "0.8938447814451382",
         "0.8971088791051823",
         "0.8938497617340877",
         "0.8906961236719488",
         "0.9558573950942942",
         null
        ],
        [
         "43",
         "4",
         "Transformer with ROS",
         "0.8867975022301516",
         "0.8891428958815072",
         "0.8868031588736742",
         "0.8836351452416918",
         "0.9535782791922176",
         null
        ],
        [
         "44",
         "5",
         "Transformer with ROS",
         "0.8982158786797502",
         "0.901336434480438",
         "0.8982208510798415",
         "0.8951746789571436",
         "0.9596442821211107",
         null
        ],
        [
         "45",
         "6",
         "Transformer with ROS",
         "0.9067713444553483",
         "0.908798827314802",
         "0.9067674433396232",
         "0.9045158859080468",
         "0.9625085476367792",
         null
        ],
        [
         "46",
         "7",
         "Transformer with ROS",
         "0.8960656615219913",
         "0.8980844994812487",
         "0.8960612588546314",
         "0.8933431973905283",
         "0.9553958198015176",
         null
        ],
        [
         "47",
         "8",
         "Transformer with ROS",
         "0.9013292889642252",
         "0.9049837663659271",
         "0.9013243292903862",
         "0.8984731530350171",
         "0.9598898257336904",
         null
        ],
        [
         "48",
         "9",
         "Transformer with ROS",
         "0.8965117316442145",
         "0.8989978689723742",
         "0.896506748018398",
         "0.8937024743119507",
         "0.9548593216235872",
         null
        ],
        [
         "49",
         "10",
         "Transformer with ROS",
         "0.8975822999375502",
         "0.8997422373308618",
         "0.8975778177100077",
         "0.8950141957878947",
         "0.956610873231896",
         null
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 120
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>model name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>attention_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.772293</td>\n",
       "      <td>0.756238</td>\n",
       "      <td>0.760196</td>\n",
       "      <td>0.757818</td>\n",
       "      <td>0.905977</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.763010</td>\n",
       "      <td>0.746337</td>\n",
       "      <td>0.755540</td>\n",
       "      <td>0.750704</td>\n",
       "      <td>0.899468</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.763010</td>\n",
       "      <td>0.743496</td>\n",
       "      <td>0.765572</td>\n",
       "      <td>0.753224</td>\n",
       "      <td>0.902575</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.774824</td>\n",
       "      <td>0.759398</td>\n",
       "      <td>0.771537</td>\n",
       "      <td>0.764981</td>\n",
       "      <td>0.906906</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.762447</td>\n",
       "      <td>0.745330</td>\n",
       "      <td>0.757961</td>\n",
       "      <td>0.751172</td>\n",
       "      <td>0.898570</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6</td>\n",
       "      <td>FTTransformer with SMOTE</td>\n",
       "      <td>0.873851</td>\n",
       "      <td>0.874368</td>\n",
       "      <td>0.873848</td>\n",
       "      <td>0.872224</td>\n",
       "      <td>0.960346</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>7</td>\n",
       "      <td>FTTransformer with SMOTE</td>\n",
       "      <td>0.874565</td>\n",
       "      <td>0.875111</td>\n",
       "      <td>0.874562</td>\n",
       "      <td>0.872525</td>\n",
       "      <td>0.958017</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>8</td>\n",
       "      <td>FTTransformer with SMOTE</td>\n",
       "      <td>0.877777</td>\n",
       "      <td>0.877824</td>\n",
       "      <td>0.877774</td>\n",
       "      <td>0.876277</td>\n",
       "      <td>0.961730</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>9</td>\n",
       "      <td>FTTransformer with SMOTE</td>\n",
       "      <td>0.880364</td>\n",
       "      <td>0.880113</td>\n",
       "      <td>0.880361</td>\n",
       "      <td>0.879117</td>\n",
       "      <td>0.959837</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>10</td>\n",
       "      <td>FTTransformer with SMOTE</td>\n",
       "      <td>0.877688</td>\n",
       "      <td>0.877599</td>\n",
       "      <td>0.877685</td>\n",
       "      <td>0.876110</td>\n",
       "      <td>0.962655</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fold                model name  accuracy  precision    recall  f1_score  \\\n",
       "0       1                       MLP  0.772293   0.756238  0.760196  0.757818   \n",
       "1       2                       MLP  0.763010   0.746337  0.755540  0.750704   \n",
       "2       3                       MLP  0.763010   0.743496  0.765572  0.753224   \n",
       "3       4                       MLP  0.774824   0.759398  0.771537  0.764981   \n",
       "4       5                       MLP  0.762447   0.745330  0.757961  0.751172   \n",
       "..    ...                       ...       ...        ...       ...       ...   \n",
       "115     6  FTTransformer with SMOTE  0.873851   0.874368  0.873848  0.872224   \n",
       "116     7  FTTransformer with SMOTE  0.874565   0.875111  0.874562  0.872525   \n",
       "117     8  FTTransformer with SMOTE  0.877777   0.877824  0.877774  0.876277   \n",
       "118     9  FTTransformer with SMOTE  0.880364   0.880113  0.880361  0.879117   \n",
       "119    10  FTTransformer with SMOTE  0.877688   0.877599  0.877685  0.876110   \n",
       "\n",
       "      roc_auc  attention_weights  \n",
       "0    0.905977                NaN  \n",
       "1    0.899468                NaN  \n",
       "2    0.902575                NaN  \n",
       "3    0.906906                NaN  \n",
       "4    0.898570                NaN  \n",
       "..        ...                ...  \n",
       "115  0.960346                NaN  \n",
       "116  0.958017                NaN  \n",
       "117  0.961730                NaN  \n",
       "118  0.959837                NaN  \n",
       "119  0.962655                NaN  \n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fold Results for MLP and Transformer\n",
    "mlp_trans_fold_df = pd.read_csv('./data/5_final_fold_results.csv')\n",
    "num_folds = 10\n",
    "model_names = ['MLP'] * num_folds + ['MLP with ROS'] * num_folds + ['MLP with SMOTE'] * num_folds + ['Transformer'] * num_folds + ['Transformer with ROS'] * num_folds + ['Transformer with SMOTE'] * num_folds + ['TabTransformer'] * num_folds + ['TabTransformer with ROS'] * num_folds + ['TabTransformer with SMOTE'] * num_folds + ['FTTransformer'] * num_folds + ['FTTransformer with ROS'] * num_folds + ['FTTransformer with SMOTE'] * num_folds\n",
    "mlp_trans_fold_df['Model Name'] = model_names\n",
    "# Change column order\n",
    "mlp_trans_fold_df.columns = mlp_trans_fold_df.columns.str.lower()\n",
    "mlp_trans_fold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_original_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'Logistic Regression']['macro f1'].values.tolist()\n",
    "lg_ros_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'Logistic Regression ROS']['macro f1'].values.tolist()\n",
    "lg_smote_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'Logistic Regression SMOTE']['macro f1'].values.tolist()\n",
    "xg_original_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'XGBoost']['macro f1'].values.tolist()\n",
    "xg_ros_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'XGBoost ROS']['macro f1'].values.tolist()\n",
    "xg_smote_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'XGBoost SMOTE']['macro f1'].values.tolist()\n",
    "mlp_original_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'MLP']['f1_score'].values.tolist()\n",
    "mlp_ros_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'MLP with ROS']['f1_score'].values.tolist()\n",
    "mlp_smote_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'MLP with SMOTE']['f1_score'].values.tolist()\n",
    "transformer_original_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'Transformer']['f1_score'].values.tolist()\n",
    "transformer_ros_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'Transformer with ROS']['f1_score'].values.tolist()\n",
    "transformer_smote_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'Transformer with SMOTE']['f1_score'].values.tolist()\n",
    "tabtrans_original_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'TabTransformer']['f1_score'].values.tolist()\n",
    "tabtrans_ros_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'TabTransformer with ROS']['f1_score'].values.tolist()\n",
    "tabtrans_smote_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'TabTransformer with SMOTE']['f1_score'].values.tolist()\n",
    "tftransformer_original_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'FTTransformer']['f1_score'].values.tolist()\n",
    "tftransformer_ros_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'FTTransformer with ROS']['f1_score'].values.tolist()\n",
    "tftransformer_smote_f1 = mlp_trans_fold_df[mlp_trans_fold_df['model name'] == 'FTTransformer with SMOTE']['f1_score'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP vs LR: statistic=0.0, p-value=0.001953125\n",
      "MLP vs XGBoost: statistic=0.0, p-value=0.001953125\n",
      "Transformer vs LR: statistic=0.0, p-value=0.001953125\n",
      "Transformer vs XGBoost: statistic=0.0, p-value=0.001953125\n",
      "Transformer vs MLP: statistic=0.0, p-value=0.001953125\n"
     ]
    }
   ],
   "source": [
    "# MLP vs LR\n",
    "stat_mlp_lg, p_value_mlp_lg = wilcoxon(mlp_original_f1, lg_original_f1)\n",
    "# MLP vs XGBoost\n",
    "stat_mlp_xg, p_value_mlp_xg = wilcoxon(mlp_original_f1, xg_original_f1)\n",
    "# Transformer vs LR\n",
    "stat_transformer_lg, p_value_transformer_lg = wilcoxon(transformer_original_f1, lg_original_f1)\n",
    "# Transformer vs XGBoost\n",
    "stat_transformer_xg, p_value_transformer_xg = wilcoxon(transformer_original_f1, xg_original_f1)\n",
    "# Transformer vs MLP\n",
    "stat_transformer_mlp, p_value_transformer_mlp = wilcoxon(transformer_original_f1, mlp_original_f1)\n",
    "\n",
    "print(f\"MLP vs LR: statistic={stat_mlp_lg}, p-value={p_value_mlp_lg}\")\n",
    "print(f\"MLP vs XGBoost: statistic={stat_mlp_xg}, p-value={p_value_mlp_xg}\")\n",
    "print(f\"Transformer vs LR: statistic={stat_transformer_lg}, p-value={p_value_transformer_lg}\")\n",
    "print(f\"Transformer vs XGBoost: statistic={stat_transformer_xg}, p-value={p_value_transformer_xg}\")\n",
    "print(f\"Transformer vs MLP: statistic={stat_transformer_mlp}, p-value={p_value_transformer_mlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP vs TabTransformer: statistic=0.0, p-value=0.001953125\n",
      "MLP vs FTTransformer: statistic=0.0, p-value=0.001953125\n",
      "Transformer vs TabTransformer: statistic=0.0, p-value=0.001953125\n",
      "Transformer vs FTTransformer: statistic=9.0, p-value=0.064453125\n",
      "TabTransformer vs FTTransformer: statistic=0.0, p-value=0.001953125\n"
     ]
    }
   ],
   "source": [
    "# MLP vs TabTransformer\n",
    "stat_mlp_tt, p_mlp_tt = wilcoxon(mlp_original_f1, tabtrans_original_f1)\n",
    "# MLP vs FTTransformer\n",
    "stat_mlp_ttf, p_mlp_ttf = wilcoxon(mlp_original_f1, tftransformer_original_f1)\n",
    "# Transformer vs TabTransformer\n",
    "stat_tt, p_tt = wilcoxon(transformer_original_f1, tabtrans_original_f1)\n",
    "# Transformer vs FTTransformer\n",
    "stat_ft, p_ft = wilcoxon(transformer_original_f1, tftransformer_original_f1)\n",
    "# TabTransformer vs FTTransformer\n",
    "stat_ttf, p_ttf = wilcoxon(tabtrans_original_f1, tftransformer_original_f1)\n",
    "\n",
    "\n",
    "print(f\"MLP vs TabTransformer: statistic={stat_mlp_tt}, p-value={p_mlp_tt}\")\n",
    "print(f\"MLP vs FTTransformer: statistic={stat_mlp_ttf}, p-value={p_mlp_ttf}\")\n",
    "print(f\"Transformer vs TabTransformer: statistic={stat_tt}, p-value={p_tt}\")\n",
    "print(f\"Transformer vs FTTransformer: statistic={stat_ft}, p-value={p_ft}\")\n",
    "print(f\"TabTransformer vs FTTransformer: statistic={stat_ttf}, p-value={p_ttf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison  \t\t\t Raw p \t\t FDR p\t\tSignificance\n",
      "MLP vs LR                     \t 0.0020\t\t 0.0022\t\t**\n",
      "MLP vs XGBoost                \t 0.0020\t\t 0.0022\t\t**\n",
      "Transformer vs LR             \t 0.0020\t\t 0.0022\t\t**\n",
      "Transformer vs XGBoost        \t 0.0020\t\t 0.0022\t\t**\n",
      "Transformer vs MLP            \t 0.0020\t\t 0.0022\t\t**\n",
      "MLP vs TabTransformer         \t 0.0020\t\t 0.0022\t\t**\n",
      "MLP vs FTTransformer          \t 0.0020\t\t 0.0022\t\t**\n",
      "Transformer vs TabTransformer \t 0.0020\t\t 0.0022\t\t**\n",
      "Transformer vs FTTransformer  \t 0.0645\t\t 0.0645\t\tns\n",
      "TabTransformer vs FTTransformer\t 0.0020\t\t 0.0022\t\t**\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Combine p-values for multiple comparisons\n",
    "p_values = [\n",
    "    p_value_mlp_lg, p_value_mlp_xg, p_value_transformer_lg, p_value_transformer_xg,\n",
    "    p_value_transformer_mlp, p_mlp_tt, p_mlp_ttf, p_tt, p_ft, p_ttf\n",
    "]\n",
    "comparison_names = [\n",
    "    \"MLP vs LR\", \"MLP vs XGBoost\", \"Transformer vs LR\", \"Transformer vs XGBoost\",\n",
    "    \"Transformer vs MLP\", \"MLP vs TabTransformer\", \"MLP vs FTTransformer\",\n",
    "    \"Transformer vs TabTransformer\", \"Transformer vs FTTransformer\",\n",
    "    \"TabTransformer vs FTTransformer\"\n",
    "]\n",
    "\n",
    "# # Bonferroni Correction\n",
    "# _, pvals_bonf, _, _ = multipletests(p_values, method='bonferroni')\n",
    "# FDR Correction\n",
    "_, pvals_bonf, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "\n",
    "# Significance function\n",
    "def get_sig(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'ns'\n",
    "\n",
    "# Results\n",
    "print(\"Comparison  \\t\\t\\t Raw p \\t\\t FDR p\\t\\tSignificance\")\n",
    "for name, raw, bonf in zip(comparison_names, p_values, pvals_bonf):\n",
    "    print(f\"{name:<30}\\t {raw:.4f}\\t\\t {bonf:.4f}\\t\\t{get_sig(bonf)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROS / SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Original vs ROS: statistic=0.0, p-value=0.001953125\n",
      "Logistic Regression Original vs SMOTE: statistic=0.0, p-value=0.001953125\n",
      "XGBoost Original vs ROS: statistic=0.0, p-value=0.001953125\n",
      "XGBoost Original vs SMOTE: statistic=0.0, p-value=0.001953125\n",
      "MLP Original vs ROS: statistic=0.0, p-value=0.001953125\n",
      "MLP Original vs SMOTE: statistic=0.0, p-value=0.001953125\n",
      "Transformer Original vs ROS: statistic=0.0, p-value=0.001953125\n",
      "Transformer Original vs SMOTE: statistic=0.0, p-value=0.001953125\n",
      "TabTransformer Original vs ROS: statistic=0.0, p-value=0.001953125\n",
      "TabTransformer Original vs SMOTE: statistic=0.0, p-value=0.001953125\n",
      "FTTransformer Original vs ROS: statistic=0.0, p-value=0.001953125\n",
      "FTTransformer Original vs SMOTE: statistic=0.0, p-value=0.001953125\n",
      "Comparison  \t\t\t\t\t Raw p\t\t FDR p\t\tSignificance\n",
      "Logistic Regression Original vs ROS     \t 0.0020\t\t 0.0020\t\t**\n",
      "Logistic Regression Original vs SMOTE   \t 0.0020\t\t 0.0020\t\t**\n",
      "XGBoost Original vs ROS                 \t 0.0020\t\t 0.0020\t\t**\n",
      "XGBoost Original vs SMOTE               \t 0.0020\t\t 0.0020\t\t**\n",
      "MLP Original vs ROS                     \t 0.0020\t\t 0.0020\t\t**\n",
      "MLP Original vs SMOTE                   \t 0.0020\t\t 0.0020\t\t**\n",
      "Transformer Original vs ROS             \t 0.0020\t\t 0.0020\t\t**\n",
      "Transformer Original vs SMOTE           \t 0.0020\t\t 0.0020\t\t**\n",
      "TabTransformer Original vs ROS          \t 0.0020\t\t 0.0020\t\t**\n",
      "TabTransformer Original vs SMOTE        \t 0.0020\t\t 0.0020\t\t**\n",
      "FTTransformer Original vs ROS           \t 0.0020\t\t 0.0020\t\t**\n",
      "FTTransformer Original vs SMOTE         \t 0.0020\t\t 0.0020\t\t**\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression: Original vs ROS\n",
    "stat_lo_ros, p_lo_ros = wilcoxon(lg_original_f1, lg_ros_f1)\n",
    "# Logistic Regression: Original vs SMOTE\n",
    "stat_lo_smote, p_lo_smote = wilcoxon(lg_original_f1, lg_smote_f1)\n",
    "print(\"Logistic Regression Original vs ROS: statistic={}, p-value={}\".format(stat_lo_ros, p_lo_ros))\n",
    "print(\"Logistic Regression Original vs SMOTE: statistic={}, p-value={}\".format(stat_lo_smote, p_lo_smote))\n",
    "\n",
    "# XGBoost: Original vs ROS\n",
    "stat_xg_ros, p_xg_ros = wilcoxon(xg_original_f1, xg_ros_f1)\n",
    "# XGBoost: Original vs SMOTE\n",
    "stat_xg_smote, p_xg_smote = wilcoxon(xg_original_f1, xg_smote_f1)\n",
    "print(\"XGBoost Original vs ROS: statistic={}, p-value={}\".format(stat_xg_ros, p_xg_ros))\n",
    "print(\"XGBoost Original vs SMOTE: statistic={}, p-value={}\".format(stat_xg_smote, p_xg_smote))\n",
    "\n",
    "# MLP: Original vs ROS\n",
    "stat_mlp_ros, p_mlp_ros = wilcoxon(mlp_original_f1, mlp_ros_f1)\n",
    "# MLP: Original vs SMOTE\n",
    "stat_mlp_smote, p_mlp_smote = wilcoxon(mlp_original_f1, mlp_smote_f1)\n",
    "print(\"MLP Original vs ROS: statistic={}, p-value={}\".format(stat_mlp_ros, p_mlp_ros))\n",
    "print(\"MLP Original vs SMOTE: statistic={}, p-value={}\".format(stat_mlp_smote, p_mlp_smote))\n",
    "\n",
    "# Transformer: Original vs ROS\n",
    "stat_transformer_ros, p_transformer_ros = wilcoxon(transformer_original_f1, transformer_ros_f1)\n",
    "# Transformer: Original vs SMOTE\n",
    "stat_transformer_smote, p_transformer_smote = wilcoxon(transformer_original_f1, transformer_smote_f1)\n",
    "print(\"Transformer Original vs ROS: statistic={}, p-value={}\".format(stat_transformer_ros, p_transformer_ros))\n",
    "print(\"Transformer Original vs SMOTE: statistic={}, p-value={}\".format(stat_transformer_smote, p_transformer_smote))\n",
    "\n",
    "# TabTransformer: Original vs ROS\n",
    "stat_tabtrans_ros, p_tabtrans_ros = wilcoxon(tabtrans_original_f1, tabtrans_ros_f1)\n",
    "# TabTransformer: Original vs SMOTE\n",
    "stat_tabtrans_smote, p_tabtrans_smote = wilcoxon(tabtrans_original_f1, tabtrans_smote_f1)\n",
    "print(\"TabTransformer Original vs ROS: statistic={}, p-value={}\".format(stat_tabtrans_ros, p_tabtrans_ros))\n",
    "print(\"TabTransformer Original vs SMOTE: statistic={}, p-value={}\".format(stat_tabtrans_smote, p_tabtrans_smote))\n",
    "\n",
    "# FTTransformer: Original vs ROS\n",
    "stat_tftransformer_ros, p_tftransformer_ros = wilcoxon(tftransformer_original_f1, tftransformer_ros_f1)\n",
    "# FTTransformer: Original vs SMOTE\n",
    "stat_tftransformer_smote, p_tftransformer_smote = wilcoxon(tftransformer_original_f1, tftransformer_smote_f1)\n",
    "print(\"FTTransformer Original vs ROS: statistic={}, p-value={}\".format(stat_tftransformer_ros, p_tftransformer_ros))\n",
    "print(\"FTTransformer Original vs SMOTE: statistic={}, p-value={}\".format(stat_tftransformer_smote, p_tftransformer_smote))\n",
    "\n",
    "# Combine p-values for multiple comparisons\n",
    "p_values = [\n",
    "    p_lo_ros, p_lo_smote,\n",
    "    p_xg_ros, p_xg_smote,\n",
    "    p_mlp_ros, p_mlp_smote,\n",
    "    p_transformer_ros, p_transformer_smote,\n",
    "    p_tabtrans_ros, p_tabtrans_smote,\n",
    "    p_tftransformer_ros, p_tftransformer_smote\n",
    "]\n",
    "\n",
    "comparison_names = [\n",
    "    \"Logistic Regression Original vs ROS\", \"Logistic Regression Original vs SMOTE\",\n",
    "    \"XGBoost Original vs ROS\", \"XGBoost Original vs SMOTE\",\n",
    "    \"MLP Original vs ROS\", \"MLP Original vs SMOTE\",\n",
    "    \"Transformer Original vs ROS\", \"Transformer Original vs SMOTE\",\n",
    "    \"TabTransformer Original vs ROS\", \"TabTransformer Original vs SMOTE\",\n",
    "    \"FTTransformer Original vs ROS\", \"FTTransformer Original vs SMOTE\"\n",
    "]\n",
    "\n",
    "# # Bonferroni Correction\n",
    "# _, pvals_bonf, _, _ = multipletests(p_values, method='bonferroni')\n",
    "# FDR Correction\n",
    "_, pvals_bonf, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "\n",
    "# Significance function\n",
    "def get_sig(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'ns'\n",
    "    \n",
    "# Results\n",
    "print(\"Comparison  \\t\\t\\t\\t\\t Raw p\\t\\t FDR p\\t\\tSignificance\")\n",
    "for name, raw, bonf in zip(comparison_names, p_values, pvals_bonf):\n",
    "    print(f\"{name:<40}\\t {raw:.4f}\\t\\t {bonf:.4f}\\t\\t{get_sig(bonf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RQ1\n",
    "# from scipy.stats import wilcoxon\n",
    "# import pandas as pd\n",
    "\n",
    "# # \n",
    "# results = {}\n",
    "\n",
    "# # RQ1 - \n",
    "# def run_rq1_tests():\n",
    "#     comparisons = [\n",
    "#         (\"MLP\", \"Logistic Regression\"),\n",
    "#         (\"MLP\", \"XGBoost\"),\n",
    "#         (\"TabTransformer\", \"Logistic Regression\"),\n",
    "#         (\"TabTransformer\", \"XGBoost\"),\n",
    "#         (\"TabTransformer\", \"MLP\")  # \n",
    "#     ]\n",
    "    \n",
    "#     model_f1 = {\n",
    "#         \"Logistic Regression\": lg_original_f1,\n",
    "#         \"XGBoost\": xg_original_f1,\n",
    "#         \"MLP\": mlp_original_f1,\n",
    "#         \"TabTransformer\": tabtrans_original_f1\n",
    "#     }\n",
    "\n",
    "#     rq1_results = []\n",
    "#     for model_a, model_b in comparisons:\n",
    "#         stat, p = wilcoxon(model_f1[model_a], model_f1[model_b])\n",
    "#         rq1_results.append({\n",
    "#             \"Comparison\": f\"{model_a} vs {model_b}\",\n",
    "#             \"Statistic\": stat,\n",
    "#             \"p-value\": p\n",
    "#         })\n",
    "#     return pd.DataFrame(rq1_results)\n",
    "\n",
    "# # RQ2 -  vs ROS / SMOTE\n",
    "# def run_rq2_tests():\n",
    "#     comparisons = [\n",
    "#         (\"Logistic Regression\", lg_original_f1, lg_ros_f1, \"ROS\"),\n",
    "#         (\"Logistic Regression\", lg_original_f1, lg_smote_f1, \"SMOTE\"),\n",
    "#         (\"XGBoost\", xg_original_f1, xg_ros_f1, \"ROS\"),\n",
    "#         (\"XGBoost\", xg_original_f1, xg_smote_f1, \"SMOTE\"),\n",
    "#         (\"MLP\", mlp_original_f1, mlp_ros_f1, \"ROS\"),\n",
    "#         (\"MLP\", mlp_original_f1, mlp_smote_f1, \"SMOTE\"),\n",
    "#         (\"TabTransformer\", tabtrans_original_f1, tabtrans_ros_f1, \"ROS\"),\n",
    "#         (\"TabTransformer\", tabtrans_original_f1, tabtrans_smote_f1, \"SMOTE\"),\n",
    "#     ]\n",
    "\n",
    "#     rq2_results = []\n",
    "#     for model, original, sampled, method in comparisons:\n",
    "#         stat, p = wilcoxon(sampled, original)\n",
    "#         rq2_results.append({\n",
    "#             \"Model\": model,\n",
    "#             \"Method\": method,\n",
    "#             \"Statistic\": stat,\n",
    "#             \"p-value\": p\n",
    "#         })\n",
    "#     return pd.DataFrame(rq2_results)\n",
    "\n",
    "# # \n",
    "# rq1_df = run_rq1_tests()\n",
    "# rq2_df = run_rq2_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from itertools import combinations\n",
    "# from typing import List, Tuple\n",
    "\n",
    "# # Define permutation test function\n",
    "# def permutation_test(x: List[float], y: List[float], n_permutations=10000, metric='mean', alternative='two-sided', seed=42) -> Tuple[float, float]:\n",
    "#     np.random.seed(seed)\n",
    "#     x = np.array(x)\n",
    "#     y = np.array(y)\n",
    "#     observed_diff = np.mean(x - y) if metric == 'mean' else np.median(x - y)\n",
    "#     diffs = []\n",
    "\n",
    "#     for _ in range(n_permutations):\n",
    "#         mask = np.random.rand(len(x)) > 0.5\n",
    "#         x_perm = np.where(mask, x, y)\n",
    "#         y_perm = np.where(mask, y, x)\n",
    "#         diff = np.mean(x_perm - y_perm) if metric == 'mean' else np.median(x_perm - y_perm)\n",
    "#         diffs.append(diff)\n",
    "\n",
    "#     diffs = np.array(diffs)\n",
    "\n",
    "#     if alternative == 'two-sided':\n",
    "#         p = np.mean(np.abs(diffs) >= np.abs(observed_diff))\n",
    "#     elif alternative == 'greater':\n",
    "#         p = np.mean(diffs >= observed_diff)\n",
    "#     else:\n",
    "#         p = np.mean(diffs <= observed_diff)\n",
    "\n",
    "#     return observed_diff, p\n",
    "\n",
    "# # Placeholder for actual score lists (to be filled from user data)\n",
    "# scores = {\n",
    "#     \"Logistic Regression\": [],\n",
    "#     \"Logistic Regression ROS\": [],\n",
    "#     \"Logistic Regression SMOTE\": [],\n",
    "#     \"XGBoost\": [],\n",
    "#     \"XGBoost ROS\": [],\n",
    "#     \"XGBoost SMOTE\": [],\n",
    "#     \"MLP\": [],\n",
    "#     \"MLP with ROS\": [],\n",
    "#     \"MLP with SMOTE\": [],\n",
    "#     \"TabTransformer\": [],\n",
    "#     \"TabTransformer with ROS\": [],\n",
    "#     \"TabTransformer with SMOTE\": [],\n",
    "# }\n",
    "\n",
    "# # Replace this with actual values from user\n",
    "# scores[\"Logistic Regression\"] = lg_original_f1\n",
    "# scores[\"Logistic Regression ROS\"] = lg_ros_f1\n",
    "# scores[\"Logistic Regression SMOTE\"] = lg_smote_f1\n",
    "# scores[\"XGBoost\"] = xg_original_f1\n",
    "# scores[\"XGBoost ROS\"] = xg_ros_f1\n",
    "# scores[\"XGBoost SMOTE\"] = xg_smote_f1\n",
    "# scores[\"MLP\"] = mlp_original_f1\n",
    "# scores[\"MLP with ROS\"] = mlp_ros_f1\n",
    "# scores[\"MLP with SMOTE\"] = mlp_smote_f1\n",
    "# scores[\"TabTransformer\"] = tabtrans_original_f1\n",
    "# scores[\"TabTransformer with ROS\"] = tabtrans_ros_f1\n",
    "# scores[\"TabTransformer with SMOTE\"] = tabtrans_smote_f1\n",
    "\n",
    "# # Define RQ1 comparisons (between models on original data)\n",
    "# rq1_comparisons = [\n",
    "#     (\"MLP\", \"Logistic Regression\"),\n",
    "#     (\"MLP\", \"XGBoost\"),\n",
    "#     (\"TabTransformer\", \"Logistic Regression\"),\n",
    "#     (\"TabTransformer\", \"XGBoost\"),\n",
    "#     (\"TabTransformer\", \"MLP\"),\n",
    "# ]\n",
    "\n",
    "# # Define RQ2 comparisons (same model with different sampling)\n",
    "# rq2_comparisons = [\n",
    "#     (\"Logistic Regression\", \"Logistic Regression ROS\"),\n",
    "#     (\"Logistic Regression\", \"Logistic Regression SMOTE\"),\n",
    "#     (\"XGBoost\", \"XGBoost ROS\"),\n",
    "#     (\"XGBoost\", \"XGBoost SMOTE\"),\n",
    "#     (\"MLP\", \"MLP with ROS\"),\n",
    "#     (\"MLP\", \"MLP with SMOTE\"),\n",
    "#     (\"TabTransformer\", \"TabTransformer with ROS\"),\n",
    "#     (\"TabTransformer\", \"TabTransformer with SMOTE\"),\n",
    "# ]\n",
    "\n",
    "# # Run permutation tests\n",
    "# def run_tests(pairs, label):\n",
    "#     results = []\n",
    "#     for a, b in pairs:\n",
    "#         diff, p = permutation_test(scores[a], scores[b])\n",
    "#         results.append({\n",
    "#             \"Comparison\": f\"{a} vs {b}\",\n",
    "#             \"Mean Difference\": round(diff, 4),\n",
    "#             \"p-value\": round(p, 4)\n",
    "#         })\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# rq1_results = run_tests(rq1_comparisons, \"RQ1\")\n",
    "# rq2_results = run_tests(rq2_comparisons, \"RQ2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RQ1 Results:\n",
      "                   Comparison  Mean A  Mean B  \\\n",
      "0             MLP vs Logistic  0.7547  0.6296   \n",
      "1              MLP vs XGBoost  0.7547  0.7661   \n",
      "2  TabTransformer vs Logistic  0.7794  0.6296   \n",
      "3   TabTransformer vs XGBoost  0.7794  0.7661   \n",
      "4       MLP vs TabTransformer  0.7547  0.7794   \n",
      "\n",
      "   Statistic\": round(stat, 4),Mean Difference  p-value  Significant (p < 0.05)  \n",
      "0                                      0.1251   0.0020                    True  \n",
      "1                                     -0.0114   0.0020                    True  \n",
      "2                                      0.1498   0.0020                    True  \n",
      "3                                      0.0133   0.0039                    True  \n",
      "4                                     -0.0247   0.0020                    True  \n",
      "\n",
      "RQ2 Results:\n",
      "                          Comparison  Mean A  Mean B  \\\n",
      "0          Logistic: Original vs ROS  0.6296  0.6961   \n",
      "1        Logistic: Original vs SMOTE  0.6296  0.7188   \n",
      "2           XGBoost: Original vs ROS  0.7661  0.8291   \n",
      "3         XGBoost: Original vs SMOTE  0.7661  0.8397   \n",
      "4               MLP: Original vs ROS  0.7547  0.8419   \n",
      "5             MLP: Original vs SMOTE  0.7547  0.8370   \n",
      "6    TabTransformer: Original vs ROS  0.7794  0.8695   \n",
      "7  TabTransformer: Original vs SMOTE  0.7794  0.8598   \n",
      "\n",
      "   Statistic\": round(stat, 4),Mean Difference  p-value  Significant (p < 0.05)  \n",
      "0                                     -0.0665    0.002                    True  \n",
      "1                                     -0.0892    0.002                    True  \n",
      "2                                     -0.0630    0.002                    True  \n",
      "3                                     -0.0736    0.002                    True  \n",
      "4                                     -0.0872    0.002                    True  \n",
      "5                                     -0.0823    0.002                    True  \n",
      "6                                     -0.0900    0.002                    True  \n",
      "7                                     -0.0803    0.002                    True  \n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from scipy.stats import wilcoxon\n",
    "\n",
    "# # RQ1  vs \n",
    "# rq1_comparisons = {\n",
    "#     \"MLP vs Logistic\": (\"mlp_original_f1\", \"lg_original_f1\"),\n",
    "#     \"MLP vs XGBoost\": (\"mlp_original_f1\", \"xg_original_f1\"),\n",
    "#     \"TabTransformer vs Logistic\": (\"tabtrans_original_f1\", \"lg_original_f1\"),\n",
    "#     \"TabTransformer vs XGBoost\": (\"tabtrans_original_f1\", \"xg_original_f1\"),\n",
    "#     \"MLP vs TabTransformer\": (\"mlp_original_f1\", \"tabtrans_original_f1\"),\n",
    "# }\n",
    "\n",
    "# # RQ2\n",
    "# rq2_comparisons = {\n",
    "#     \"Logistic: Original vs ROS\": (\"lg_original_f1\", \"lg_ros_f1\"),\n",
    "#     \"Logistic: Original vs SMOTE\": (\"lg_original_f1\", \"lg_smote_f1\"),\n",
    "#     \"XGBoost: Original vs ROS\": (\"xg_original_f1\", \"xg_ros_f1\"),\n",
    "#     \"XGBoost: Original vs SMOTE\": (\"xg_original_f1\", \"xg_smote_f1\"),\n",
    "#     \"MLP: Original vs ROS\": (\"mlp_original_f1\", \"mlp_ros_f1\"),\n",
    "#     \"MLP: Original vs SMOTE\": (\"mlp_original_f1\", \"mlp_smote_f1\"),\n",
    "#     \"TabTransformer: Original vs ROS\": (\"tabtrans_original_f1\", \"tabtrans_ros_f1\"),\n",
    "#     \"TabTransformer: Original vs SMOTE\": (\"tabtrans_original_f1\", \"tabtrans_smote_f1\"),\n",
    "# }\n",
    "\n",
    "# #  F1 \n",
    "# global_vars = globals()\n",
    "\n",
    "# #  Wilcoxon \n",
    "# def run_wilcoxon_tests(comparison_dict):\n",
    "#     results = []\n",
    "#     for label, (a_name, b_name) in comparison_dict.items():\n",
    "#         a = global_vars[a_name]\n",
    "#         b = global_vars[b_name]\n",
    "#         stat, p = wilcoxon(a, b)\n",
    "#         results.append({\n",
    "#             \"Comparison\": label,\n",
    "#             \"Mean A\": round(pd.Series(a).mean(), 4),\n",
    "#             \"Mean B\": round(pd.Series(b).mean(), 4),\n",
    "#             'Statistic\": round(stat, 4),'\n",
    "#             'Mean Difference': round(pd.Series(a).mean() - pd.Series(b).mean(), 4),\n",
    "#             \"p-value\": round(p, 4),\n",
    "#             \"Significant (p < 0.05)\": p < 0.05\n",
    "#         })\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# rq1_results = run_wilcoxon_tests(rq1_comparisons)\n",
    "# rq2_results = run_wilcoxon_tests(rq2_comparisons)\n",
    "\n",
    "# print(\"RQ1 Results:\")\n",
    "# print(rq1_results)\n",
    "# print(\"\\nRQ2 Results:\")\n",
    "# print(rq2_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
