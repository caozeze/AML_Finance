{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>model name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro precision</th>\n",
       "      <th>macro recall</th>\n",
       "      <th>macro f1</th>\n",
       "      <th>roc auc</th>\n",
       "      <th>class 0.0 precision</th>\n",
       "      <th>class 0.0 recall</th>\n",
       "      <th>class 1.0 precision</th>\n",
       "      <th>class 1.0 recall</th>\n",
       "      <th>class 2.0 precision</th>\n",
       "      <th>class 2.0 recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.656188</td>\n",
       "      <td>0.636743</td>\n",
       "      <td>0.631663</td>\n",
       "      <td>0.630089</td>\n",
       "      <td>0.813556</td>\n",
       "      <td>0.684241</td>\n",
       "      <td>0.537775</td>\n",
       "      <td>0.681942</td>\n",
       "      <td>0.736786</td>\n",
       "      <td>0.544047</td>\n",
       "      <td>0.620429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.654219</td>\n",
       "      <td>0.633989</td>\n",
       "      <td>0.631809</td>\n",
       "      <td>0.630154</td>\n",
       "      <td>0.810149</td>\n",
       "      <td>0.665740</td>\n",
       "      <td>0.548535</td>\n",
       "      <td>0.682755</td>\n",
       "      <td>0.726883</td>\n",
       "      <td>0.553471</td>\n",
       "      <td>0.620008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.658063</td>\n",
       "      <td>0.638786</td>\n",
       "      <td>0.631811</td>\n",
       "      <td>0.631687</td>\n",
       "      <td>0.813442</td>\n",
       "      <td>0.682997</td>\n",
       "      <td>0.542707</td>\n",
       "      <td>0.682340</td>\n",
       "      <td>0.739864</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.612863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.656657</td>\n",
       "      <td>0.637401</td>\n",
       "      <td>0.630989</td>\n",
       "      <td>0.630180</td>\n",
       "      <td>0.815688</td>\n",
       "      <td>0.674949</td>\n",
       "      <td>0.528738</td>\n",
       "      <td>0.682282</td>\n",
       "      <td>0.742540</td>\n",
       "      <td>0.554972</td>\n",
       "      <td>0.621690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.649905</td>\n",
       "      <td>0.629709</td>\n",
       "      <td>0.627788</td>\n",
       "      <td>0.624831</td>\n",
       "      <td>0.809790</td>\n",
       "      <td>0.670893</td>\n",
       "      <td>0.533089</td>\n",
       "      <td>0.678795</td>\n",
       "      <td>0.726482</td>\n",
       "      <td>0.539440</td>\n",
       "      <td>0.623792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.698515</td>\n",
       "      <td>0.696682</td>\n",
       "      <td>0.698515</td>\n",
       "      <td>0.695504</td>\n",
       "      <td>0.827916</td>\n",
       "      <td>0.727698</td>\n",
       "      <td>0.686605</td>\n",
       "      <td>0.649287</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.713062</td>\n",
       "      <td>0.824033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.700566</td>\n",
       "      <td>0.698204</td>\n",
       "      <td>0.700566</td>\n",
       "      <td>0.697366</td>\n",
       "      <td>0.826176</td>\n",
       "      <td>0.727324</td>\n",
       "      <td>0.693162</td>\n",
       "      <td>0.647753</td>\n",
       "      <td>0.580490</td>\n",
       "      <td>0.719535</td>\n",
       "      <td>0.828048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.695526</td>\n",
       "      <td>0.694199</td>\n",
       "      <td>0.695526</td>\n",
       "      <td>0.692593</td>\n",
       "      <td>0.825359</td>\n",
       "      <td>0.732762</td>\n",
       "      <td>0.675498</td>\n",
       "      <td>0.640902</td>\n",
       "      <td>0.585842</td>\n",
       "      <td>0.708932</td>\n",
       "      <td>0.825238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.704804</td>\n",
       "      <td>0.703546</td>\n",
       "      <td>0.704804</td>\n",
       "      <td>0.701771</td>\n",
       "      <td>0.831105</td>\n",
       "      <td>0.736304</td>\n",
       "      <td>0.694233</td>\n",
       "      <td>0.663213</td>\n",
       "      <td>0.591061</td>\n",
       "      <td>0.711121</td>\n",
       "      <td>0.829118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Logistic Regression ROS</td>\n",
       "      <td>0.696686</td>\n",
       "      <td>0.695080</td>\n",
       "      <td>0.696686</td>\n",
       "      <td>0.693669</td>\n",
       "      <td>0.824775</td>\n",
       "      <td>0.731252</td>\n",
       "      <td>0.686337</td>\n",
       "      <td>0.645713</td>\n",
       "      <td>0.581426</td>\n",
       "      <td>0.708276</td>\n",
       "      <td>0.822294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.725188</td>\n",
       "      <td>0.725018</td>\n",
       "      <td>0.725188</td>\n",
       "      <td>0.721222</td>\n",
       "      <td>0.860832</td>\n",
       "      <td>0.750037</td>\n",
       "      <td>0.677773</td>\n",
       "      <td>0.699894</td>\n",
       "      <td>0.619162</td>\n",
       "      <td>0.725124</td>\n",
       "      <td>0.878630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.722155</td>\n",
       "      <td>0.721464</td>\n",
       "      <td>0.722155</td>\n",
       "      <td>0.717821</td>\n",
       "      <td>0.857293</td>\n",
       "      <td>0.741115</td>\n",
       "      <td>0.678041</td>\n",
       "      <td>0.698249</td>\n",
       "      <td>0.608457</td>\n",
       "      <td>0.725028</td>\n",
       "      <td>0.879968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.718810</td>\n",
       "      <td>0.718847</td>\n",
       "      <td>0.718810</td>\n",
       "      <td>0.714656</td>\n",
       "      <td>0.856906</td>\n",
       "      <td>0.745205</td>\n",
       "      <td>0.660244</td>\n",
       "      <td>0.692227</td>\n",
       "      <td>0.618493</td>\n",
       "      <td>0.719110</td>\n",
       "      <td>0.877693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.727731</td>\n",
       "      <td>0.727955</td>\n",
       "      <td>0.727731</td>\n",
       "      <td>0.723673</td>\n",
       "      <td>0.861846</td>\n",
       "      <td>0.750556</td>\n",
       "      <td>0.677238</td>\n",
       "      <td>0.708911</td>\n",
       "      <td>0.622775</td>\n",
       "      <td>0.724399</td>\n",
       "      <td>0.883179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>Logistic Regression SMOTE</td>\n",
       "      <td>0.721174</td>\n",
       "      <td>0.721335</td>\n",
       "      <td>0.721174</td>\n",
       "      <td>0.716918</td>\n",
       "      <td>0.857470</td>\n",
       "      <td>0.747240</td>\n",
       "      <td>0.670146</td>\n",
       "      <td>0.698202</td>\n",
       "      <td>0.613274</td>\n",
       "      <td>0.718562</td>\n",
       "      <td>0.880102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.815893</td>\n",
       "      <td>0.807200</td>\n",
       "      <td>0.808530</td>\n",
       "      <td>0.807620</td>\n",
       "      <td>0.929637</td>\n",
       "      <td>0.799477</td>\n",
       "      <td>0.840659</td>\n",
       "      <td>0.835547</td>\n",
       "      <td>0.816540</td>\n",
       "      <td>0.786575</td>\n",
       "      <td>0.768390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.815331</td>\n",
       "      <td>0.804330</td>\n",
       "      <td>0.808806</td>\n",
       "      <td>0.806282</td>\n",
       "      <td>0.928788</td>\n",
       "      <td>0.801256</td>\n",
       "      <td>0.847299</td>\n",
       "      <td>0.837774</td>\n",
       "      <td>0.811990</td>\n",
       "      <td>0.773961</td>\n",
       "      <td>0.767129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.815177</td>\n",
       "      <td>0.805248</td>\n",
       "      <td>0.805394</td>\n",
       "      <td>0.805024</td>\n",
       "      <td>0.928015</td>\n",
       "      <td>0.803692</td>\n",
       "      <td>0.847493</td>\n",
       "      <td>0.834131</td>\n",
       "      <td>0.816272</td>\n",
       "      <td>0.777923</td>\n",
       "      <td>0.752417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.815177</td>\n",
       "      <td>0.804817</td>\n",
       "      <td>0.809567</td>\n",
       "      <td>0.806920</td>\n",
       "      <td>0.928856</td>\n",
       "      <td>0.798525</td>\n",
       "      <td>0.843142</td>\n",
       "      <td>0.837912</td>\n",
       "      <td>0.812124</td>\n",
       "      <td>0.778013</td>\n",
       "      <td>0.773434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.817216</td>\n",
       "      <td>0.807750</td>\n",
       "      <td>0.809647</td>\n",
       "      <td>0.808396</td>\n",
       "      <td>0.928655</td>\n",
       "      <td>0.802555</td>\n",
       "      <td>0.848866</td>\n",
       "      <td>0.837318</td>\n",
       "      <td>0.815469</td>\n",
       "      <td>0.783376</td>\n",
       "      <td>0.764607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>Random Forest ROS</td>\n",
       "      <td>0.909318</td>\n",
       "      <td>0.911705</td>\n",
       "      <td>0.909318</td>\n",
       "      <td>0.907191</td>\n",
       "      <td>0.981544</td>\n",
       "      <td>0.877386</td>\n",
       "      <td>0.959454</td>\n",
       "      <td>0.938762</td>\n",
       "      <td>0.783621</td>\n",
       "      <td>0.918966</td>\n",
       "      <td>0.984879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>Random Forest ROS</td>\n",
       "      <td>0.907043</td>\n",
       "      <td>0.909618</td>\n",
       "      <td>0.907043</td>\n",
       "      <td>0.904750</td>\n",
       "      <td>0.980639</td>\n",
       "      <td>0.875275</td>\n",
       "      <td>0.957848</td>\n",
       "      <td>0.938288</td>\n",
       "      <td>0.777198</td>\n",
       "      <td>0.915290</td>\n",
       "      <td>0.986083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>Random Forest ROS</td>\n",
       "      <td>0.910701</td>\n",
       "      <td>0.912916</td>\n",
       "      <td>0.910701</td>\n",
       "      <td>0.908592</td>\n",
       "      <td>0.981724</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.939258</td>\n",
       "      <td>0.786297</td>\n",
       "      <td>0.919796</td>\n",
       "      <td>0.989830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest ROS</td>\n",
       "      <td>0.912663</td>\n",
       "      <td>0.915305</td>\n",
       "      <td>0.912663</td>\n",
       "      <td>0.910555</td>\n",
       "      <td>0.981406</td>\n",
       "      <td>0.879118</td>\n",
       "      <td>0.960525</td>\n",
       "      <td>0.945346</td>\n",
       "      <td>0.786966</td>\n",
       "      <td>0.921449</td>\n",
       "      <td>0.990499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>Random Forest ROS</td>\n",
       "      <td>0.908872</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.908872</td>\n",
       "      <td>0.906704</td>\n",
       "      <td>0.980768</td>\n",
       "      <td>0.874601</td>\n",
       "      <td>0.953834</td>\n",
       "      <td>0.936719</td>\n",
       "      <td>0.782417</td>\n",
       "      <td>0.922013</td>\n",
       "      <td>0.990365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>Random Forest SMOTE</td>\n",
       "      <td>0.881038</td>\n",
       "      <td>0.881412</td>\n",
       "      <td>0.881038</td>\n",
       "      <td>0.879342</td>\n",
       "      <td>0.965371</td>\n",
       "      <td>0.865142</td>\n",
       "      <td>0.915964</td>\n",
       "      <td>0.887968</td>\n",
       "      <td>0.774254</td>\n",
       "      <td>0.891128</td>\n",
       "      <td>0.952897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>Random Forest SMOTE</td>\n",
       "      <td>0.876578</td>\n",
       "      <td>0.877127</td>\n",
       "      <td>0.876578</td>\n",
       "      <td>0.874714</td>\n",
       "      <td>0.963844</td>\n",
       "      <td>0.862909</td>\n",
       "      <td>0.909675</td>\n",
       "      <td>0.886497</td>\n",
       "      <td>0.766091</td>\n",
       "      <td>0.881975</td>\n",
       "      <td>0.953968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>Random Forest SMOTE</td>\n",
       "      <td>0.878273</td>\n",
       "      <td>0.878738</td>\n",
       "      <td>0.878273</td>\n",
       "      <td>0.876539</td>\n",
       "      <td>0.964565</td>\n",
       "      <td>0.867346</td>\n",
       "      <td>0.911682</td>\n",
       "      <td>0.886923</td>\n",
       "      <td>0.771444</td>\n",
       "      <td>0.881944</td>\n",
       "      <td>0.951693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest SMOTE</td>\n",
       "      <td>0.879254</td>\n",
       "      <td>0.879668</td>\n",
       "      <td>0.879254</td>\n",
       "      <td>0.877537</td>\n",
       "      <td>0.964287</td>\n",
       "      <td>0.868461</td>\n",
       "      <td>0.915295</td>\n",
       "      <td>0.886908</td>\n",
       "      <td>0.772381</td>\n",
       "      <td>0.883634</td>\n",
       "      <td>0.950087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>Random Forest SMOTE</td>\n",
       "      <td>0.877247</td>\n",
       "      <td>0.877437</td>\n",
       "      <td>0.877247</td>\n",
       "      <td>0.875540</td>\n",
       "      <td>0.964447</td>\n",
       "      <td>0.864138</td>\n",
       "      <td>0.911548</td>\n",
       "      <td>0.881408</td>\n",
       "      <td>0.770775</td>\n",
       "      <td>0.886764</td>\n",
       "      <td>0.949418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.773769</td>\n",
       "      <td>0.760082</td>\n",
       "      <td>0.762403</td>\n",
       "      <td>0.761219</td>\n",
       "      <td>0.907666</td>\n",
       "      <td>0.767849</td>\n",
       "      <td>0.763278</td>\n",
       "      <td>0.795814</td>\n",
       "      <td>0.793791</td>\n",
       "      <td>0.716584</td>\n",
       "      <td>0.730139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.774754</td>\n",
       "      <td>0.757701</td>\n",
       "      <td>0.764677</td>\n",
       "      <td>0.761048</td>\n",
       "      <td>0.907955</td>\n",
       "      <td>0.773632</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.801359</td>\n",
       "      <td>0.789241</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.730979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.778536</td>\n",
       "      <td>0.763891</td>\n",
       "      <td>0.767070</td>\n",
       "      <td>0.765384</td>\n",
       "      <td>0.909393</td>\n",
       "      <td>0.784579</td>\n",
       "      <td>0.768949</td>\n",
       "      <td>0.797914</td>\n",
       "      <td>0.798341</td>\n",
       "      <td>0.709180</td>\n",
       "      <td>0.733922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.760487</td>\n",
       "      <td>0.762973</td>\n",
       "      <td>0.761628</td>\n",
       "      <td>0.908523</td>\n",
       "      <td>0.778977</td>\n",
       "      <td>0.760247</td>\n",
       "      <td>0.797440</td>\n",
       "      <td>0.800214</td>\n",
       "      <td>0.705045</td>\n",
       "      <td>0.728457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.773894</td>\n",
       "      <td>0.758795</td>\n",
       "      <td>0.763365</td>\n",
       "      <td>0.760968</td>\n",
       "      <td>0.907471</td>\n",
       "      <td>0.775866</td>\n",
       "      <td>0.764140</td>\n",
       "      <td>0.795754</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.704766</td>\n",
       "      <td>0.733501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.830412</td>\n",
       "      <td>0.830970</td>\n",
       "      <td>0.830412</td>\n",
       "      <td>0.826596</td>\n",
       "      <td>0.936437</td>\n",
       "      <td>0.830100</td>\n",
       "      <td>0.870199</td>\n",
       "      <td>0.836352</td>\n",
       "      <td>0.679781</td>\n",
       "      <td>0.826460</td>\n",
       "      <td>0.941255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.834515</td>\n",
       "      <td>0.834533</td>\n",
       "      <td>0.834515</td>\n",
       "      <td>0.830891</td>\n",
       "      <td>0.937543</td>\n",
       "      <td>0.837269</td>\n",
       "      <td>0.873010</td>\n",
       "      <td>0.833902</td>\n",
       "      <td>0.687274</td>\n",
       "      <td>0.832428</td>\n",
       "      <td>0.943262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.831928</td>\n",
       "      <td>0.832091</td>\n",
       "      <td>0.831928</td>\n",
       "      <td>0.828267</td>\n",
       "      <td>0.936230</td>\n",
       "      <td>0.837293</td>\n",
       "      <td>0.872474</td>\n",
       "      <td>0.832085</td>\n",
       "      <td>0.683661</td>\n",
       "      <td>0.826896</td>\n",
       "      <td>0.939649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.834248</td>\n",
       "      <td>0.834881</td>\n",
       "      <td>0.834248</td>\n",
       "      <td>0.830475</td>\n",
       "      <td>0.937941</td>\n",
       "      <td>0.837593</td>\n",
       "      <td>0.877158</td>\n",
       "      <td>0.839941</td>\n",
       "      <td>0.683260</td>\n",
       "      <td>0.827108</td>\n",
       "      <td>0.942326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5</td>\n",
       "      <td>XGBoost ROS</td>\n",
       "      <td>0.827066</td>\n",
       "      <td>0.827065</td>\n",
       "      <td>0.827066</td>\n",
       "      <td>0.823320</td>\n",
       "      <td>0.934919</td>\n",
       "      <td>0.834025</td>\n",
       "      <td>0.860029</td>\n",
       "      <td>0.824590</td>\n",
       "      <td>0.679379</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.941790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.841831</td>\n",
       "      <td>0.841123</td>\n",
       "      <td>0.841831</td>\n",
       "      <td>0.840318</td>\n",
       "      <td>0.950798</td>\n",
       "      <td>0.845693</td>\n",
       "      <td>0.849993</td>\n",
       "      <td>0.827332</td>\n",
       "      <td>0.750167</td>\n",
       "      <td>0.850344</td>\n",
       "      <td>0.925331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.842633</td>\n",
       "      <td>0.842120</td>\n",
       "      <td>0.842633</td>\n",
       "      <td>0.840935</td>\n",
       "      <td>0.950417</td>\n",
       "      <td>0.846767</td>\n",
       "      <td>0.853339</td>\n",
       "      <td>0.832436</td>\n",
       "      <td>0.745216</td>\n",
       "      <td>0.847158</td>\n",
       "      <td>0.929346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.841786</td>\n",
       "      <td>0.841291</td>\n",
       "      <td>0.841786</td>\n",
       "      <td>0.840114</td>\n",
       "      <td>0.950558</td>\n",
       "      <td>0.844685</td>\n",
       "      <td>0.846380</td>\n",
       "      <td>0.832266</td>\n",
       "      <td>0.747625</td>\n",
       "      <td>0.846921</td>\n",
       "      <td>0.931353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.840894</td>\n",
       "      <td>0.840170</td>\n",
       "      <td>0.840894</td>\n",
       "      <td>0.839318</td>\n",
       "      <td>0.949518</td>\n",
       "      <td>0.846952</td>\n",
       "      <td>0.851599</td>\n",
       "      <td>0.825392</td>\n",
       "      <td>0.746420</td>\n",
       "      <td>0.848165</td>\n",
       "      <td>0.924662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5</td>\n",
       "      <td>XGBoost SMOTE</td>\n",
       "      <td>0.834025</td>\n",
       "      <td>0.832965</td>\n",
       "      <td>0.834025</td>\n",
       "      <td>0.832345</td>\n",
       "      <td>0.948533</td>\n",
       "      <td>0.841367</td>\n",
       "      <td>0.843169</td>\n",
       "      <td>0.811468</td>\n",
       "      <td>0.736652</td>\n",
       "      <td>0.846059</td>\n",
       "      <td>0.922253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold                 model name  accuracy  macro precision  macro recall  \\\n",
       "0      1        Logistic Regression  0.656188         0.636743      0.631663   \n",
       "1      2        Logistic Regression  0.654219         0.633989      0.631809   \n",
       "2      3        Logistic Regression  0.658063         0.638786      0.631811   \n",
       "3      4        Logistic Regression  0.656657         0.637401      0.630989   \n",
       "4      5        Logistic Regression  0.649905         0.629709      0.627788   \n",
       "5      1    Logistic Regression ROS  0.698515         0.696682      0.698515   \n",
       "6      2    Logistic Regression ROS  0.700566         0.698204      0.700566   \n",
       "7      3    Logistic Regression ROS  0.695526         0.694199      0.695526   \n",
       "8      4    Logistic Regression ROS  0.704804         0.703546      0.704804   \n",
       "9      5    Logistic Regression ROS  0.696686         0.695080      0.696686   \n",
       "10     1  Logistic Regression SMOTE  0.725188         0.725018      0.725188   \n",
       "11     2  Logistic Regression SMOTE  0.722155         0.721464      0.722155   \n",
       "12     3  Logistic Regression SMOTE  0.718810         0.718847      0.718810   \n",
       "13     4  Logistic Regression SMOTE  0.727731         0.727955      0.727731   \n",
       "14     5  Logistic Regression SMOTE  0.721174         0.721335      0.721174   \n",
       "15     1              Random Forest  0.815893         0.807200      0.808530   \n",
       "16     2              Random Forest  0.815331         0.804330      0.808806   \n",
       "17     3              Random Forest  0.815177         0.805248      0.805394   \n",
       "18     4              Random Forest  0.815177         0.804817      0.809567   \n",
       "19     5              Random Forest  0.817216         0.807750      0.809647   \n",
       "20     1          Random Forest ROS  0.909318         0.911705      0.909318   \n",
       "21     2          Random Forest ROS  0.907043         0.909618      0.907043   \n",
       "22     3          Random Forest ROS  0.910701         0.912916      0.910701   \n",
       "23     4          Random Forest ROS  0.912663         0.915305      0.912663   \n",
       "24     5          Random Forest ROS  0.908872         0.911111      0.908872   \n",
       "25     1        Random Forest SMOTE  0.881038         0.881412      0.881038   \n",
       "26     2        Random Forest SMOTE  0.876578         0.877127      0.876578   \n",
       "27     3        Random Forest SMOTE  0.878273         0.878738      0.878273   \n",
       "28     4        Random Forest SMOTE  0.879254         0.879668      0.879254   \n",
       "29     5        Random Forest SMOTE  0.877247         0.877437      0.877247   \n",
       "30     1                    XGBoost  0.773769         0.760082      0.762403   \n",
       "31     2                    XGBoost  0.774754         0.757701      0.764677   \n",
       "32     3                    XGBoost  0.778536         0.763891      0.767070   \n",
       "33     4                    XGBoost  0.775934         0.760487      0.762973   \n",
       "34     5                    XGBoost  0.773894         0.758795      0.763365   \n",
       "35     1                XGBoost ROS  0.830412         0.830970      0.830412   \n",
       "36     2                XGBoost ROS  0.834515         0.834533      0.834515   \n",
       "37     3                XGBoost ROS  0.831928         0.832091      0.831928   \n",
       "38     4                XGBoost ROS  0.834248         0.834881      0.834248   \n",
       "39     5                XGBoost ROS  0.827066         0.827065      0.827066   \n",
       "40     1              XGBoost SMOTE  0.841831         0.841123      0.841831   \n",
       "41     2              XGBoost SMOTE  0.842633         0.842120      0.842633   \n",
       "42     3              XGBoost SMOTE  0.841786         0.841291      0.841786   \n",
       "43     4              XGBoost SMOTE  0.840894         0.840170      0.840894   \n",
       "44     5              XGBoost SMOTE  0.834025         0.832965      0.834025   \n",
       "\n",
       "    macro f1   roc auc  class 0.0 precision  class 0.0 recall  \\\n",
       "0   0.630089  0.813556             0.684241          0.537775   \n",
       "1   0.630154  0.810149             0.665740          0.548535   \n",
       "2   0.631687  0.813442             0.682997          0.542707   \n",
       "3   0.630180  0.815688             0.674949          0.528738   \n",
       "4   0.624831  0.809790             0.670893          0.533089   \n",
       "5   0.695504  0.827916             0.727698          0.686605   \n",
       "6   0.697366  0.826176             0.727324          0.693162   \n",
       "7   0.692593  0.825359             0.732762          0.675498   \n",
       "8   0.701771  0.831105             0.736304          0.694233   \n",
       "9   0.693669  0.824775             0.731252          0.686337   \n",
       "10  0.721222  0.860832             0.750037          0.677773   \n",
       "11  0.717821  0.857293             0.741115          0.678041   \n",
       "12  0.714656  0.856906             0.745205          0.660244   \n",
       "13  0.723673  0.861846             0.750556          0.677238   \n",
       "14  0.716918  0.857470             0.747240          0.670146   \n",
       "15  0.807620  0.929637             0.799477          0.840659   \n",
       "16  0.806282  0.928788             0.801256          0.847299   \n",
       "17  0.805024  0.928015             0.803692          0.847493   \n",
       "18  0.806920  0.928856             0.798525          0.843142   \n",
       "19  0.808396  0.928655             0.802555          0.848866   \n",
       "20  0.907191  0.981544             0.877386          0.959454   \n",
       "21  0.904750  0.980639             0.875275          0.957848   \n",
       "22  0.908592  0.981724             0.879695          0.955975   \n",
       "23  0.910555  0.981406             0.879118          0.960525   \n",
       "24  0.906704  0.980768             0.874601          0.953834   \n",
       "25  0.879342  0.965371             0.865142          0.915964   \n",
       "26  0.874714  0.963844             0.862909          0.909675   \n",
       "27  0.876539  0.964565             0.867346          0.911682   \n",
       "28  0.877537  0.964287             0.868461          0.915295   \n",
       "29  0.875540  0.964447             0.864138          0.911548   \n",
       "30  0.761219  0.907666             0.767849          0.763278   \n",
       "31  0.761048  0.907955             0.773632          0.773810   \n",
       "32  0.765384  0.909393             0.784579          0.768949   \n",
       "33  0.761628  0.908523             0.778977          0.760247   \n",
       "34  0.760968  0.907471             0.775866          0.764140   \n",
       "35  0.826596  0.936437             0.830100          0.870199   \n",
       "36  0.830891  0.937543             0.837269          0.873010   \n",
       "37  0.828267  0.936230             0.837293          0.872474   \n",
       "38  0.830475  0.937941             0.837593          0.877158   \n",
       "39  0.823320  0.934919             0.834025          0.860029   \n",
       "40  0.840318  0.950798             0.845693          0.849993   \n",
       "41  0.840935  0.950417             0.846767          0.853339   \n",
       "42  0.840114  0.950558             0.844685          0.846380   \n",
       "43  0.839318  0.949518             0.846952          0.851599   \n",
       "44  0.832345  0.948533             0.841367          0.843169   \n",
       "\n",
       "    class 1.0 precision  class 1.0 recall  class 2.0 precision  \\\n",
       "0              0.681942          0.736786             0.544047   \n",
       "1              0.682755          0.726883             0.553471   \n",
       "2              0.682340          0.739864             0.551020   \n",
       "3              0.682282          0.742540             0.554972   \n",
       "4              0.678795          0.726482             0.539440   \n",
       "5              0.649287          0.584906             0.713062   \n",
       "6              0.647753          0.580490             0.719535   \n",
       "7              0.640902          0.585842             0.708932   \n",
       "8              0.663213          0.591061             0.711121   \n",
       "9              0.645713          0.581426             0.708276   \n",
       "10             0.699894          0.619162             0.725124   \n",
       "11             0.698249          0.608457             0.725028   \n",
       "12             0.692227          0.618493             0.719110   \n",
       "13             0.708911          0.622775             0.724399   \n",
       "14             0.698202          0.613274             0.718562   \n",
       "15             0.835547          0.816540             0.786575   \n",
       "16             0.837774          0.811990             0.773961   \n",
       "17             0.834131          0.816272             0.777923   \n",
       "18             0.837912          0.812124             0.778013   \n",
       "19             0.837318          0.815469             0.783376   \n",
       "20             0.938762          0.783621             0.918966   \n",
       "21             0.938288          0.777198             0.915290   \n",
       "22             0.939258          0.786297             0.919796   \n",
       "23             0.945346          0.786966             0.921449   \n",
       "24             0.936719          0.782417             0.922013   \n",
       "25             0.887968          0.774254             0.891128   \n",
       "26             0.886497          0.766091             0.881975   \n",
       "27             0.886923          0.771444             0.881944   \n",
       "28             0.886908          0.772381             0.883634   \n",
       "29             0.881408          0.770775             0.886764   \n",
       "30             0.795814          0.793791             0.716584   \n",
       "31             0.801359          0.789241             0.698113   \n",
       "32             0.797914          0.798341             0.709180   \n",
       "33             0.797440          0.800214             0.705045   \n",
       "34             0.795754          0.792453             0.704766   \n",
       "35             0.836352          0.679781             0.826460   \n",
       "36             0.833902          0.687274             0.832428   \n",
       "37             0.832085          0.683661             0.826896   \n",
       "38             0.839941          0.683260             0.827108   \n",
       "39             0.824590          0.679379             0.822581   \n",
       "40             0.827332          0.750167             0.850344   \n",
       "41             0.832436          0.745216             0.847158   \n",
       "42             0.832266          0.747625             0.846921   \n",
       "43             0.825392          0.746420             0.848165   \n",
       "44             0.811468          0.736652             0.846059   \n",
       "\n",
       "    class 2.0 recall  \n",
       "0           0.620429  \n",
       "1           0.620008  \n",
       "2           0.612863  \n",
       "3           0.621690  \n",
       "4           0.623792  \n",
       "5           0.824033  \n",
       "6           0.828048  \n",
       "7           0.825238  \n",
       "8           0.829118  \n",
       "9           0.822294  \n",
       "10          0.878630  \n",
       "11          0.879968  \n",
       "12          0.877693  \n",
       "13          0.883179  \n",
       "14          0.880102  \n",
       "15          0.768390  \n",
       "16          0.767129  \n",
       "17          0.752417  \n",
       "18          0.773434  \n",
       "19          0.764607  \n",
       "20          0.984879  \n",
       "21          0.986083  \n",
       "22          0.989830  \n",
       "23          0.990499  \n",
       "24          0.990365  \n",
       "25          0.952897  \n",
       "26          0.953968  \n",
       "27          0.951693  \n",
       "28          0.950087  \n",
       "29          0.949418  \n",
       "30          0.730139  \n",
       "31          0.730979  \n",
       "32          0.733922  \n",
       "33          0.728457  \n",
       "34          0.733501  \n",
       "35          0.941255  \n",
       "36          0.943262  \n",
       "37          0.939649  \n",
       "38          0.942326  \n",
       "39          0.941790  \n",
       "40          0.925331  \n",
       "41          0.929346  \n",
       "42          0.931353  \n",
       "43          0.924662  \n",
       "44          0.922253  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fold Results for Logistic Regression and XGBoost\n",
    "lg_xg_fold_df = pd.read_csv('./data/4_final_fold_results.csv')\n",
    "lg_xg_fold_df.columns = lg_xg_fold_df.columns.str.lower()\n",
    "lg_xg_fold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>model name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>attention_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.757032</td>\n",
       "      <td>0.739363</td>\n",
       "      <td>0.748921</td>\n",
       "      <td>0.743902</td>\n",
       "      <td>0.898574</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.749086</td>\n",
       "      <td>0.729895</td>\n",
       "      <td>0.732064</td>\n",
       "      <td>0.730798</td>\n",
       "      <td>0.891870</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.750686</td>\n",
       "      <td>0.729153</td>\n",
       "      <td>0.745876</td>\n",
       "      <td>0.736666</td>\n",
       "      <td>0.893049</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.757015</td>\n",
       "      <td>0.744099</td>\n",
       "      <td>0.747905</td>\n",
       "      <td>0.745501</td>\n",
       "      <td>0.898466</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.740146</td>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.744630</td>\n",
       "      <td>0.898536</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>MLP with ROS</td>\n",
       "      <td>0.848967</td>\n",
       "      <td>0.853474</td>\n",
       "      <td>0.848967</td>\n",
       "      <td>0.842633</td>\n",
       "      <td>0.939195</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>MLP with ROS</td>\n",
       "      <td>0.841251</td>\n",
       "      <td>0.848059</td>\n",
       "      <td>0.841251</td>\n",
       "      <td>0.833061</td>\n",
       "      <td>0.936388</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>MLP with ROS</td>\n",
       "      <td>0.843659</td>\n",
       "      <td>0.848760</td>\n",
       "      <td>0.843659</td>\n",
       "      <td>0.836824</td>\n",
       "      <td>0.938925</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>MLP with ROS</td>\n",
       "      <td>0.844105</td>\n",
       "      <td>0.848669</td>\n",
       "      <td>0.844105</td>\n",
       "      <td>0.837470</td>\n",
       "      <td>0.938506</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>MLP with ROS</td>\n",
       "      <td>0.844016</td>\n",
       "      <td>0.846548</td>\n",
       "      <td>0.844016</td>\n",
       "      <td>0.838080</td>\n",
       "      <td>0.938037</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>MLP with SMOTE</td>\n",
       "      <td>0.837370</td>\n",
       "      <td>0.837068</td>\n",
       "      <td>0.837370</td>\n",
       "      <td>0.834679</td>\n",
       "      <td>0.944926</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>MLP with SMOTE</td>\n",
       "      <td>0.834515</td>\n",
       "      <td>0.833509</td>\n",
       "      <td>0.834515</td>\n",
       "      <td>0.832132</td>\n",
       "      <td>0.942106</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>MLP with SMOTE</td>\n",
       "      <td>0.830679</td>\n",
       "      <td>0.829924</td>\n",
       "      <td>0.830679</td>\n",
       "      <td>0.828157</td>\n",
       "      <td>0.942585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>MLP with SMOTE</td>\n",
       "      <td>0.834159</td>\n",
       "      <td>0.833692</td>\n",
       "      <td>0.834159</td>\n",
       "      <td>0.831660</td>\n",
       "      <td>0.942923</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>MLP with SMOTE</td>\n",
       "      <td>0.836166</td>\n",
       "      <td>0.835020</td>\n",
       "      <td>0.836166</td>\n",
       "      <td>0.834119</td>\n",
       "      <td>0.944199</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.787693</td>\n",
       "      <td>0.771791</td>\n",
       "      <td>0.797845</td>\n",
       "      <td>0.782873</td>\n",
       "      <td>0.913985</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.778481</td>\n",
       "      <td>0.769272</td>\n",
       "      <td>0.762722</td>\n",
       "      <td>0.765269</td>\n",
       "      <td>0.909193</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.772417</td>\n",
       "      <td>0.757992</td>\n",
       "      <td>0.773678</td>\n",
       "      <td>0.764980</td>\n",
       "      <td>0.905564</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.770589</td>\n",
       "      <td>0.754841</td>\n",
       "      <td>0.773639</td>\n",
       "      <td>0.763308</td>\n",
       "      <td>0.906140</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>0.776145</td>\n",
       "      <td>0.761104</td>\n",
       "      <td>0.777984</td>\n",
       "      <td>0.768726</td>\n",
       "      <td>0.907582</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>TabTransformer with ROS</td>\n",
       "      <td>0.869307</td>\n",
       "      <td>0.871994</td>\n",
       "      <td>0.869307</td>\n",
       "      <td>0.864970</td>\n",
       "      <td>0.947784</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>TabTransformer with ROS</td>\n",
       "      <td>0.846603</td>\n",
       "      <td>0.849312</td>\n",
       "      <td>0.846603</td>\n",
       "      <td>0.840448</td>\n",
       "      <td>0.940408</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>TabTransformer with ROS</td>\n",
       "      <td>0.873054</td>\n",
       "      <td>0.876779</td>\n",
       "      <td>0.873054</td>\n",
       "      <td>0.868446</td>\n",
       "      <td>0.947272</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>TabTransformer with ROS</td>\n",
       "      <td>0.870199</td>\n",
       "      <td>0.874159</td>\n",
       "      <td>0.870199</td>\n",
       "      <td>0.865687</td>\n",
       "      <td>0.946356</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>TabTransformer with ROS</td>\n",
       "      <td>0.866765</td>\n",
       "      <td>0.869066</td>\n",
       "      <td>0.866765</td>\n",
       "      <td>0.862379</td>\n",
       "      <td>0.944116</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>TabTransformer with SMOTE</td>\n",
       "      <td>0.860074</td>\n",
       "      <td>0.860855</td>\n",
       "      <td>0.860074</td>\n",
       "      <td>0.857856</td>\n",
       "      <td>0.954384</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>TabTransformer with SMOTE</td>\n",
       "      <td>0.855524</td>\n",
       "      <td>0.855524</td>\n",
       "      <td>0.855524</td>\n",
       "      <td>0.853376</td>\n",
       "      <td>0.952432</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>TabTransformer with SMOTE</td>\n",
       "      <td>0.852268</td>\n",
       "      <td>0.851882</td>\n",
       "      <td>0.852268</td>\n",
       "      <td>0.850275</td>\n",
       "      <td>0.950406</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>TabTransformer with SMOTE</td>\n",
       "      <td>0.850796</td>\n",
       "      <td>0.851492</td>\n",
       "      <td>0.850796</td>\n",
       "      <td>0.847918</td>\n",
       "      <td>0.949385</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>TabTransformer with SMOTE</td>\n",
       "      <td>0.850261</td>\n",
       "      <td>0.849346</td>\n",
       "      <td>0.850261</td>\n",
       "      <td>0.848802</td>\n",
       "      <td>0.950281</td>\n",
       "      <td>[[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold                 model name  accuracy  precision    recall  f1_score  \\\n",
       "0      1                        MLP  0.757032   0.739363  0.748921  0.743902   \n",
       "1      2                        MLP  0.749086   0.729895  0.732064  0.730798   \n",
       "2      3                        MLP  0.750686   0.729153  0.745876  0.736666   \n",
       "3      4                        MLP  0.757015   0.744099  0.747905  0.745501   \n",
       "4      5                        MLP  0.758000   0.740146  0.749656  0.744630   \n",
       "5      1               MLP with ROS  0.848967   0.853474  0.848967  0.842633   \n",
       "6      2               MLP with ROS  0.841251   0.848059  0.841251  0.833061   \n",
       "7      3               MLP with ROS  0.843659   0.848760  0.843659  0.836824   \n",
       "8      4               MLP with ROS  0.844105   0.848669  0.844105  0.837470   \n",
       "9      5               MLP with ROS  0.844016   0.846548  0.844016  0.838080   \n",
       "10     1             MLP with SMOTE  0.837370   0.837068  0.837370  0.834679   \n",
       "11     2             MLP with SMOTE  0.834515   0.833509  0.834515  0.832132   \n",
       "12     3             MLP with SMOTE  0.830679   0.829924  0.830679  0.828157   \n",
       "13     4             MLP with SMOTE  0.834159   0.833692  0.834159  0.831660   \n",
       "14     5             MLP with SMOTE  0.836166   0.835020  0.836166  0.834119   \n",
       "15     1             TabTransformer  0.787693   0.771791  0.797845  0.782873   \n",
       "16     2             TabTransformer  0.778481   0.769272  0.762722  0.765269   \n",
       "17     3             TabTransformer  0.772417   0.757992  0.773678  0.764980   \n",
       "18     4             TabTransformer  0.770589   0.754841  0.773639  0.763308   \n",
       "19     5             TabTransformer  0.776145   0.761104  0.777984  0.768726   \n",
       "20     1    TabTransformer with ROS  0.869307   0.871994  0.869307  0.864970   \n",
       "21     2    TabTransformer with ROS  0.846603   0.849312  0.846603  0.840448   \n",
       "22     3    TabTransformer with ROS  0.873054   0.876779  0.873054  0.868446   \n",
       "23     4    TabTransformer with ROS  0.870199   0.874159  0.870199  0.865687   \n",
       "24     5    TabTransformer with ROS  0.866765   0.869066  0.866765  0.862379   \n",
       "25     1  TabTransformer with SMOTE  0.860074   0.860855  0.860074  0.857856   \n",
       "26     2  TabTransformer with SMOTE  0.855524   0.855524  0.855524  0.853376   \n",
       "27     3  TabTransformer with SMOTE  0.852268   0.851882  0.852268  0.850275   \n",
       "28     4  TabTransformer with SMOTE  0.850796   0.851492  0.850796  0.847918   \n",
       "29     5  TabTransformer with SMOTE  0.850261   0.849346  0.850261  0.848802   \n",
       "\n",
       "     roc_auc                                  attention_weights  \n",
       "0   0.898574                                                NaN  \n",
       "1   0.891870                                                NaN  \n",
       "2   0.893049                                                NaN  \n",
       "3   0.898466                                                NaN  \n",
       "4   0.898536                                                NaN  \n",
       "5   0.939195                                                NaN  \n",
       "6   0.936388                                                NaN  \n",
       "7   0.938925                                                NaN  \n",
       "8   0.938506                                                NaN  \n",
       "9   0.938037                                                NaN  \n",
       "10  0.944926                                                NaN  \n",
       "11  0.942106                                                NaN  \n",
       "12  0.942585                                                NaN  \n",
       "13  0.942923                                                NaN  \n",
       "14  0.944199                                                NaN  \n",
       "15  0.913985  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "16  0.909193  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "17  0.905564  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "18  0.906140  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "19  0.907582  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "20  0.947784  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "21  0.940408  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "22  0.947272  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "23  0.946356  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "24  0.944116  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "25  0.954384  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "26  0.952432  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "27  0.950406  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "28  0.949385  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  \n",
       "29  0.950281  [[[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n [[1.]]\\r\\n\\r\\n ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fold Results for MLP and TabTransformer\n",
    "mlp_tabtrans_fold_df = pd.read_csv('./data/5_final_fold_results.csv')\n",
    "folds = [1, 2, 3, 4, 5] * 6\n",
    "mlp_tabtrans_fold_df['Fold'] = folds\n",
    "model_names = ['MLP'] * 5 + ['MLP with ROS'] * 5 + ['MLP with SMOTE'] * 5 + ['TabTransformer'] * 5 + ['TabTransformer with ROS'] * 5 + ['TabTransformer with SMOTE'] * 5\n",
    "mlp_tabtrans_fold_df['Model Name'] = model_names\n",
    "# Change column order\n",
    "mlp_tabtrans_fold_df = mlp_tabtrans_fold_df[['Fold', 'Model Name', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'attention_weights']]\n",
    "mlp_tabtrans_fold_df.columns = mlp_tabtrans_fold_df.columns.str.lower()\n",
    "mlp_tabtrans_fold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_original_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'Logistic Regression']['macro f1'].values.tolist()\n",
    "lg_ros_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'Logistic Regression ROS']['macro f1'].values.tolist()\n",
    "lg_smote_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'Logistic Regression SMOTE']['macro f1'].values.tolist()\n",
    "xg_original_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'XGBoost']['macro f1'].values.tolist()\n",
    "xg_ros_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'XGBoost ROS']['macro f1'].values.tolist()\n",
    "xg_smote_f1 = lg_xg_fold_df[lg_xg_fold_df['model name'] == 'XGBoost SMOTE']['macro f1'].values.tolist()\n",
    "mlp_original_f1 = mlp_tabtrans_fold_df[mlp_tabtrans_fold_df['model name'] == 'MLP']['f1_score'].values.tolist()\n",
    "mlp_ros_f1 = mlp_tabtrans_fold_df[mlp_tabtrans_fold_df['model name'] == 'MLP with ROS']['f1_score'].values.tolist()\n",
    "mlp_smote_f1 = mlp_tabtrans_fold_df[mlp_tabtrans_fold_df['model name'] == 'MLP with SMOTE']['f1_score'].values.tolist()\n",
    "tabtrans_original_f1 = mlp_tabtrans_fold_df[mlp_tabtrans_fold_df['model name'] == 'TabTransformer']['f1_score'].values.tolist()\n",
    "tabtrans_ros_f1 = mlp_tabtrans_fold_df[mlp_tabtrans_fold_df['model name'] == 'TabTransformer with ROS']['f1_score'].values.tolist()\n",
    "tabtrans_smote_f1 = mlp_tabtrans_fold_df[mlp_tabtrans_fold_df['model name'] == 'TabTransformer with SMOTE']['f1_score'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ1\n",
    "from scipy.stats import wilcoxon\n",
    "import pandas as pd\n",
    "\n",
    "# 模拟数据结构（用户已定义）\n",
    "results = {}\n",
    "\n",
    "# 模拟：RQ1 - 模型对比（原始数据）\n",
    "def run_rq1_tests():\n",
    "    comparisons = [\n",
    "        (\"MLP\", \"Logistic Regression\"),\n",
    "        (\"MLP\", \"XGBoost\"),\n",
    "        (\"TabTransformer\", \"Logistic Regression\"),\n",
    "        (\"TabTransformer\", \"XGBoost\"),\n",
    "        (\"TabTransformer\", \"MLP\")  # 可选\n",
    "    ]\n",
    "    \n",
    "    model_f1 = {\n",
    "        \"Logistic Regression\": lg_original_f1,\n",
    "        \"XGBoost\": xg_original_f1,\n",
    "        \"MLP\": mlp_original_f1,\n",
    "        \"TabTransformer\": tabtrans_original_f1\n",
    "    }\n",
    "\n",
    "    rq1_results = []\n",
    "    for model_a, model_b in comparisons:\n",
    "        stat, p = wilcoxon(model_f1[model_a], model_f1[model_b])\n",
    "        rq1_results.append({\n",
    "            \"Comparison\": f\"{model_a} vs {model_b}\",\n",
    "            \"Statistic\": stat,\n",
    "            \"p-value\": p\n",
    "        })\n",
    "    return pd.DataFrame(rq1_results)\n",
    "\n",
    "# 模拟：RQ2 - 采样方法对比（每个模型原始 vs ROS / SMOTE）\n",
    "def run_rq2_tests():\n",
    "    comparisons = [\n",
    "        (\"Logistic Regression\", lg_original_f1, lg_ros_f1, \"ROS\"),\n",
    "        (\"Logistic Regression\", lg_original_f1, lg_smote_f1, \"SMOTE\"),\n",
    "        (\"XGBoost\", xg_original_f1, xg_ros_f1, \"ROS\"),\n",
    "        (\"XGBoost\", xg_original_f1, xg_smote_f1, \"SMOTE\"),\n",
    "        (\"MLP\", mlp_original_f1, mlp_ros_f1, \"ROS\"),\n",
    "        (\"MLP\", mlp_original_f1, mlp_smote_f1, \"SMOTE\"),\n",
    "        (\"TabTransformer\", tabtrans_original_f1, tabtrans_ros_f1, \"ROS\"),\n",
    "        (\"TabTransformer\", tabtrans_original_f1, tabtrans_smote_f1, \"SMOTE\"),\n",
    "    ]\n",
    "\n",
    "    rq2_results = []\n",
    "    for model, original, sampled, method in comparisons:\n",
    "        stat, p = wilcoxon(sampled, original)\n",
    "        rq2_results.append({\n",
    "            \"Model\": model,\n",
    "            \"Method\": method,\n",
    "            \"Statistic\": stat,\n",
    "            \"p-value\": p\n",
    "        })\n",
    "    return pd.DataFrame(rq2_results)\n",
    "\n",
    "# 执行检验\n",
    "rq1_df = run_rq1_tests()\n",
    "rq2_df = run_rq2_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comparison</th>\n",
       "      <th>Statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP vs Logistic Regression</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP vs XGBoost</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TabTransformer vs Logistic Regression</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TabTransformer vs XGBoost</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TabTransformer vs MLP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Comparison  Statistic  p-value\n",
       "0             MLP vs Logistic Regression        0.0   0.0625\n",
       "1                         MLP vs XGBoost        0.0   0.0625\n",
       "2  TabTransformer vs Logistic Regression        0.0   0.0625\n",
       "3              TabTransformer vs XGBoost        1.0   0.1250\n",
       "4                  TabTransformer vs MLP        0.0   0.0625"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rq1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Method</th>\n",
       "      <th>Statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>ROS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TabTransformer</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Method  Statistic  p-value\n",
       "0  Logistic Regression    ROS        0.0   0.0625\n",
       "1  Logistic Regression  SMOTE        0.0   0.0625\n",
       "2              XGBoost    ROS        0.0   0.0625\n",
       "3              XGBoost  SMOTE        0.0   0.0625\n",
       "4                  MLP    ROS        0.0   0.0625\n",
       "5                  MLP  SMOTE        0.0   0.0625\n",
       "6       TabTransformer    ROS        0.0   0.0625\n",
       "7       TabTransformer  SMOTE        0.0   0.0625"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rq2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Define permutation test function\n",
    "def permutation_test(x: List[float], y: List[float], n_permutations=10000, metric='mean', alternative='two-sided', seed=42) -> Tuple[float, float]:\n",
    "    np.random.seed(seed)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    observed_diff = np.mean(x - y) if metric == 'mean' else np.median(x - y)\n",
    "    diffs = []\n",
    "\n",
    "    for _ in range(n_permutations):\n",
    "        mask = np.random.rand(len(x)) > 0.5\n",
    "        x_perm = np.where(mask, x, y)\n",
    "        y_perm = np.where(mask, y, x)\n",
    "        diff = np.mean(x_perm - y_perm) if metric == 'mean' else np.median(x_perm - y_perm)\n",
    "        diffs.append(diff)\n",
    "\n",
    "    diffs = np.array(diffs)\n",
    "\n",
    "    if alternative == 'two-sided':\n",
    "        p = np.mean(np.abs(diffs) >= np.abs(observed_diff))\n",
    "    elif alternative == 'greater':\n",
    "        p = np.mean(diffs >= observed_diff)\n",
    "    else:\n",
    "        p = np.mean(diffs <= observed_diff)\n",
    "\n",
    "    return observed_diff, p\n",
    "\n",
    "# Placeholder for actual score lists (to be filled from user data)\n",
    "scores = {\n",
    "    \"Logistic Regression\": [],\n",
    "    \"Logistic Regression ROS\": [],\n",
    "    \"Logistic Regression SMOTE\": [],\n",
    "    \"XGBoost\": [],\n",
    "    \"XGBoost ROS\": [],\n",
    "    \"XGBoost SMOTE\": [],\n",
    "    \"MLP\": [],\n",
    "    \"MLP with ROS\": [],\n",
    "    \"MLP with SMOTE\": [],\n",
    "    \"TabTransformer\": [],\n",
    "    \"TabTransformer with ROS\": [],\n",
    "    \"TabTransformer with SMOTE\": [],\n",
    "}\n",
    "\n",
    "# Replace this with actual values from user\n",
    "scores[\"Logistic Regression\"] = lg_original_f1\n",
    "scores[\"Logistic Regression ROS\"] = lg_ros_f1\n",
    "scores[\"Logistic Regression SMOTE\"] = lg_smote_f1\n",
    "scores[\"XGBoost\"] = xg_original_f1\n",
    "scores[\"XGBoost ROS\"] = xg_ros_f1\n",
    "scores[\"XGBoost SMOTE\"] = xg_smote_f1\n",
    "scores[\"MLP\"] = mlp_original_f1\n",
    "scores[\"MLP with ROS\"] = mlp_ros_f1\n",
    "scores[\"MLP with SMOTE\"] = mlp_smote_f1\n",
    "scores[\"TabTransformer\"] = tabtrans_original_f1\n",
    "scores[\"TabTransformer with ROS\"] = tabtrans_ros_f1\n",
    "scores[\"TabTransformer with SMOTE\"] = tabtrans_smote_f1\n",
    "\n",
    "# Define RQ1 comparisons (between models on original data)\n",
    "rq1_comparisons = [\n",
    "    (\"MLP\", \"Logistic Regression\"),\n",
    "    (\"MLP\", \"XGBoost\"),\n",
    "    (\"TabTransformer\", \"Logistic Regression\"),\n",
    "    (\"TabTransformer\", \"XGBoost\"),\n",
    "    (\"TabTransformer\", \"MLP\"),\n",
    "]\n",
    "\n",
    "# Define RQ2 comparisons (same model with different sampling)\n",
    "rq2_comparisons = [\n",
    "    (\"Logistic Regression\", \"Logistic Regression ROS\"),\n",
    "    (\"Logistic Regression\", \"Logistic Regression SMOTE\"),\n",
    "    (\"XGBoost\", \"XGBoost ROS\"),\n",
    "    (\"XGBoost\", \"XGBoost SMOTE\"),\n",
    "    (\"MLP\", \"MLP with ROS\"),\n",
    "    (\"MLP\", \"MLP with SMOTE\"),\n",
    "    (\"TabTransformer\", \"TabTransformer with ROS\"),\n",
    "    (\"TabTransformer\", \"TabTransformer with SMOTE\"),\n",
    "]\n",
    "\n",
    "# Run permutation tests\n",
    "def run_tests(pairs, label):\n",
    "    results = []\n",
    "    for a, b in pairs:\n",
    "        diff, p = permutation_test(scores[a], scores[b])\n",
    "        results.append({\n",
    "            \"Comparison\": f\"{a} vs {b}\",\n",
    "            \"Mean Difference\": round(diff, 4),\n",
    "            \"p-value\": round(p, 4)\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "rq1_results = run_tests(rq1_comparisons, \"RQ1\")\n",
    "rq2_results = run_tests(rq2_comparisons, \"RQ2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.782872988384616,\n",
       " 0.7652688371220492,\n",
       " 0.7649799252842872,\n",
       " 0.7633080228917368,\n",
       " 0.7687255576920458]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabtrans_original_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7612187122225885,\n",
       " 0.7610477202007528,\n",
       " 0.7653837374656515,\n",
       " 0.7616275090757973,\n",
       " 0.7609683057688739]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_original_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comparison</th>\n",
       "      <th>Mean Difference</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP vs Logistic Regression</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP vs XGBoost</td>\n",
       "      <td>-0.0217</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TabTransformer vs Logistic Regression</td>\n",
       "      <td>0.1396</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TabTransformer vs XGBoost</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.1257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TabTransformer vs MLP</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Comparison  Mean Difference  p-value\n",
       "0             MLP vs Logistic Regression           0.1109   0.0619\n",
       "1                         MLP vs XGBoost          -0.0217   0.0619\n",
       "2  TabTransformer vs Logistic Regression           0.1396   0.0619\n",
       "3              TabTransformer vs XGBoost           0.0070   0.1257\n",
       "4                  TabTransformer vs MLP           0.0287   0.0619"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rq1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comparison</th>\n",
       "      <th>Mean Difference</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression vs Logistic Regression ROS</td>\n",
       "      <td>-0.0668</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression vs Logistic Regression SMOTE</td>\n",
       "      <td>-0.0895</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost vs XGBoost ROS</td>\n",
       "      <td>-0.0659</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost vs XGBoost SMOTE</td>\n",
       "      <td>-0.0766</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP vs MLP with ROS</td>\n",
       "      <td>-0.0973</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP vs MLP with SMOTE</td>\n",
       "      <td>-0.0918</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TabTransformer vs TabTransformer with ROS</td>\n",
       "      <td>-0.0914</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TabTransformer vs TabTransformer with SMOTE</td>\n",
       "      <td>-0.0826</td>\n",
       "      <td>0.0619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Comparison  Mean Difference  p-value\n",
       "0    Logistic Regression vs Logistic Regression ROS          -0.0668   0.0619\n",
       "1  Logistic Regression vs Logistic Regression SMOTE          -0.0895   0.0619\n",
       "2                            XGBoost vs XGBoost ROS          -0.0659   0.0619\n",
       "3                          XGBoost vs XGBoost SMOTE          -0.0766   0.0619\n",
       "4                               MLP vs MLP with ROS          -0.0973   0.0619\n",
       "5                             MLP vs MLP with SMOTE          -0.0918   0.0619\n",
       "6         TabTransformer vs TabTransformer with ROS          -0.0914   0.0619\n",
       "7       TabTransformer vs TabTransformer with SMOTE          -0.0826   0.0619"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rq2_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
